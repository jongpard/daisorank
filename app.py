# -*- coding: utf-8 -*-
import os, re, csv, sys, time, traceback, pathlib, datetime as dt
from typing import List, Dict, Optional

import pytz
import requests
from bs4 import BeautifulSoup

# ================== ì„¤ì • ==================
BASE_URL = "https://www.daisomall.co.kr"
RANK_URL = f"{BASE_URL}/ds/rank/C105"   # ë·°í‹°/ìœ„ìƒ
DATA_DIR = pathlib.Path("data")
DEBUG_DIR = pathlib.Path("data/debug")
DATA_DIR.mkdir(parents=True, exist_ok=True)
DEBUG_DIR.mkdir(parents=True, exist_ok=True)
KST = pytz.timezone("Asia/Seoul")

SLACK_WEBHOOK_URL = os.getenv("SLACK_WEBHOOK_URL", "").strip()
GOOGLE_CLIENT_ID = os.getenv("GOOGLE_CLIENT_ID", "").strip()
GOOGLE_CLIENT_SECRET = os.getenv("GOOGLE_CLIENT_SECRET", "").strip()
GOOGLE_REFRESH_TOKEN = os.getenv("GOOGLE_REFRESH_TOKEN", "").strip()
GDRIVE_FOLDER_ID = os.getenv("GDRIVE_FOLDER_ID", "").strip()

# ================== ìœ í‹¸ ==================
def today_kst() -> str:
    return dt.datetime.now(KST).strftime("%Y-%m-%d")

def to_int(s: str) -> Optional[int]:
    try:
        return int(re.sub(r"[^\d]", "", s))
    except Exception:
        return None

def fmt_won(n: Optional[int]) -> str:
    if n is None:
        return "0ì›"
    return f"{n:,}ì›"

def slack(text: str):
    if not SLACK_WEBHOOK_URL:
        print("[Slack] ë¯¸ì„¤ì •")
        return
    try:
        requests.post(SLACK_WEBHOOK_URL, json={"text": text}, timeout=15)
        print("[Slack] ì „ì†¡ ì™„ë£Œ")
    except Exception as e:
        print("[Slack ì‹¤íŒ¨]", e)

def load_csv(path: Optional[pathlib.Path]) -> List[Dict]:
    if not path or not isinstance(path, pathlib.Path) or not path.exists() or not path.is_file():
        return []
    out = []
    with path.open("r", newline="", encoding="utf-8") as f:
        r = csv.DictReader(f)
        for row in r:
            out.append({
                "date": row.get("date",""),
                "rank": int(row.get("rank","0") or 0),
                "name": row.get("name",""),
                "price": int(row.get("price","0") or 0),
                "url": row.get("url",""),
            })
    return out

def save_csv(path: pathlib.Path, rows: List[Dict]):
    cols = ["date","rank","name","price","url"]
    with path.open("w", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=cols)
        w.writeheader()
        for r in rows:
            w.writerow({k:r.get(k,"") for k in cols})

# ============= Google Drive (scope ê°•ì œ X) =============
def gdrive_service():
    from google.oauth2.credentials import Credentials
    from googleapiclient.discovery import build
    if not (GOOGLE_CLIENT_ID and GOOGLE_CLIENT_SECRET and GOOGLE_REFRESH_TOKEN and GDRIVE_FOLDER_ID):
        raise RuntimeError("Drive ìê²©/í´ë” ID ë¯¸ì„¤ì •")
    creds = Credentials(
        None,
        refresh_token=GOOGLE_REFRESH_TOKEN,
        token_uri="https://oauth2.googleapis.com/token",
        client_id=GOOGLE_CLIENT_ID,
        client_secret=GOOGLE_CLIENT_SECRET,
        # scopes ì „ë‹¬í•˜ì§€ ì•ŠìŒ â†’ refresh tokenì— ìˆëŠ” ê¶Œí•œë§Œ ì‚¬ìš©
    )
    return build("drive","v3",credentials=creds, cache_discovery=False)

def gdrive_upload(path: pathlib.Path) -> str:
    from googleapiclient.http import MediaInMemoryUpload
    svc = gdrive_service()
    media = MediaInMemoryUpload(path.read_bytes(), mimetype="text/csv", resumable=False)
    meta = {"name": path.name, "parents":[GDRIVE_FOLDER_ID]}
    file = svc.files().create(body=meta, media_body=media, fields="id", supportsAllDrives=True).execute()
    return file["id"]

# ================== íŒŒì„œ ==================
def parse_html(html: str) -> List[Dict]:
    soup = BeautifulSoup(html, "lxml")
    items: List[Dict] = []
    for card in soup.select(".goods-unit"):
        num_el = card.select_one(".ranking-area .rank .num")
        if not num_el:  # ë°°ë„ˆ/ê´‘ê³ 
            continue
        rank = to_int(num_el.get_text(strip=True)) or 0

        # ì´ë¦„ (BEST ë¼ë²¨ ì œê±°)
        tit = card.select_one(".goods-detail .tit")
        name = ""
        if tit:
            for b in tit.select(".best"):
                b.extract()
            name = " ".join(tit.get_text(" ", strip=True).split())
            name = re.sub(r"^\s*BEST\s*", "", name, flags=re.I)

        price_el = card.select_one(".goods-detail .goods-price .value")
        price = to_int(price_el.get_text(strip=True)) if price_el else 0

        a = card.select_one(".goods-thumb a.goods-link")
        url = BASE_URL + a["href"] if a and a.has_attr("href") else RANK_URL

        items.append({"rank": rank, "name": name, "price": price or 0, "url": url})
    items.sort(key=lambda x: x["rank"])
    return items

# ============= ìˆ˜ì§‘ (Playwright ìš°ì„ ) =============
def fetch_playwright() -> List[Dict]:
    from playwright.sync_api import sync_playwright
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True, args=["--no-sandbox","--disable-dev-shm-usage"])
        page = browser.new_page(viewport={"width":1440,"height":2000},
                                user_agent=("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                                            "(KHTML, like Gecko) Chrome/126.0 Safari/537.36"))
        page.goto(RANK_URL, wait_until="networkidle", timeout=60_000)

        # --- ì¹´í…Œê³ ë¦¬: ë·°í‹°/ìœ„ìƒ ì„ íƒ ---
        def select_beauty():
            tries = [
                "role=button[name='ë·°í‹°/ìœ„ìƒ']",
                "role=link[name='ë·°í‹°/ìœ„ìƒ']",
                "button:has-text('ë·°í‹°/ìœ„ìƒ')",
                "a:has-text('ë·°í‹°/ìœ„ìƒ')",
            ]
            for sel in tries:
                try:
                    page.locator(sel).first.click(timeout=1200); break
                except Exception: pass
            # JS ê°•ì œ
            page.evaluate("""
                () => {
                  const nodes = Array.from(document.querySelectorAll('button,a,div,span'));
                  const el = nodes.find(n => (n.textContent||'').includes('ë·°í‹°') && (n.textContent||'').includes('ìœ„ìƒ'));
                  if (el) { el.scrollIntoView({block:'center'}); el.click(); }
                }
            """)
            page.wait_for_timeout(600)
        select_beauty()

        # --- ê¸°ê°„: ì¼ê°„ ì„ íƒ (is-active í™•ì¸ + ì¬ì‹œë„ ë£¨í”„) ---
        def is_daily_active() -> bool:
            return page.evaluate("""
                () => {
                  const btns = Array.from(document.querySelectorAll('button,a'));
                  const daily = btns.find(el => /ì¼ê°„/.test(el.textContent||''));
                  if (!daily) return false;
                  const cls = (daily.className||'') + ' ' + (daily.parentElement && daily.parentElement.className || '');
                  return /is-active/.test(cls);
                }
            """)
        def click_daily_once():
            try:
                page.locator("button:has-text('ì¼ê°„')").first.click(timeout=800); return True
            except Exception:
                try:
                    page.locator("a:has-text('ì¼ê°„')").first.click(timeout=800); return True
                except Exception:
                    page.evaluate("""
                        () => {
                          const nodes = Array.from(document.querySelectorAll('button,a,div,span'));
                          const el = nodes.find(n => /ì¼ê°„/.test(n.textContent||''));
                          if (el) { el.scrollIntoView({block:'center'}); el.click(); }
                        }
                    """); return True

        for _ in range(5):
            if is_daily_active(): break
            click_daily_once()
            page.wait_for_timeout(500)

        # --- ë¬´í•œ ìŠ¤í¬ë¡¤ (ì¹´ë“œ ì¦ê°€ ë©ˆì¶¤ ê¸°ì¤€) ---
        def count_cards():
            try:
                return page.evaluate("() => document.querySelectorAll('.goods-unit').length")
            except Exception:
                return 0

        prev_cnt, stall = 0, 0
        for _ in range(120):
            page.evaluate("() => window.scrollTo(0, document.body.scrollHeight)")
            page.wait_for_timeout(400)
            cnt = count_cards()
            if cnt <= prev_cnt:
                stall += 1
                if stall >= 5: break
            else:
                stall = 0
                prev_cnt = cnt

        # ë””ë²„ê·¸ ì €ì¥
        DEBUG_DIR.mkdir(parents=True, exist_ok=True)
        (DEBUG_DIR / "page_rank.png").write_bytes(page.screenshot(full_page=True))
        (DEBUG_DIR / "page_rank.html").write_text(page.content(), encoding="utf-8")

        html = page.content()
        browser.close()
    return parse_html(html)

def fetch_requests() -> List[Dict]:
    r = requests.get(RANK_URL, timeout=20, headers={
        "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0 Safari/537.36"
    })
    r.raise_for_status()
    html = r.text
    (DEBUG_DIR / "page_rank.html").write_text(html, encoding="utf-8")
    return parse_html(html)

# ============= ë³€ë™(Top30) =============
def normalize_name(s: str) -> str:
    return re.sub(r"\s+"," ",s or "").strip().lower()

def prev_csv_path(today_csv: pathlib.Path) -> Optional[pathlib.Path]:
    stem = "ë‹¤ì´ì†Œëª°_ë·°í‹°ìœ„ìƒ_ì¼ê°„_"
    files = sorted(DATA_DIR.glob(f"{stem}*.csv"))
    prevs = [p for p in files if p.name < today_csv.name]
    return prevs[-1] if prevs else None

def analyze(today: List[Dict], prev: List[Dict]) -> Dict[str, List[Dict]]:
    N = 30
    tmap = {normalize_name(x["name"]): x for x in today}
    pmap = {normalize_name(x["name"]): x for x in prev}
    rising, falling, new_in, out = [], [], [], []

    for k, t in tmap.items():
        if t["rank"]<=N:
            p = pmap.get(k)
            if p:
                d = p["rank"] - t["rank"]
                if d>0:
                    rising.append({"name":t["name"],"prev":p["rank"],"curr":t["rank"],"delta":d,"url":t["url"]})
            else:
                new_in.append({"name":t["name"],"prev":None,"curr":t["rank"],"url":t["url"]})

    for k, p in pmap.items():
        if p["rank"]<=N:
            t = tmap.get(k)
            if not t or t["rank"]>N:
                out.append({"name":p["name"],"prev":p["rank"],"curr":None})
            else:
                d = t["rank"] - p["rank"]
                if d>0:
                    falling.append({"name":t["name"],"prev":p["rank"],"curr":t["rank"],"delta":d,"url":t["url"]})

    rising.sort(key=lambda x:(-x["delta"], x["curr"], x["prev"], normalize_name(x["name"])))
    falling.sort(key=lambda x:(-x["delta"], x["prev"], x["curr"], normalize_name(x["name"])))
    new_in.sort(key=lambda x:(x["curr"], normalize_name(x["name"])))
    out.sort(key=lambda x:(x["prev"], normalize_name(x["name"])))

    return {
        "rising": rising[:3],
        "new_in": new_in[:3],
        "falling": falling[:5],
        "out": out,
        "inout_count": len(new_in)+len(out),
    }

# ============= Slack ë©”ì‹œì§€ =============
def slack_message(today_rows: List[Dict], change: Dict) -> str:
    lines = []
    lines.append(f"*ë‹¤ì´ì†Œëª° ë·°í‹°/ìœ„ìƒ ì¼ê°„ â€” {today_kst()}*")
    lines.append("")
    lines.append("*TOP 10*")
    for r in [x for x in today_rows if x["rank"]<=10]:
        nm = r["name"].replace("&","&amp;").replace("<","ã€ˆ").replace(">","ã€‰")
        lines.append(f"{r['rank']}. <{r['url']}|{nm}> â€” {fmt_won(r['price'])}")
    lines.append("")

    lines.append("ğŸ”¥ *ê¸‰ìƒìŠ¹*")
    if change["rising"]:
        for r in change["rising"]:
            lines.append(f"- {r['name']} {r['prev']}ìœ„ â†’ {r['curr']}ìœ„ (â†‘{r['delta']})")
    else:
        lines.append("- í•´ë‹¹ ì—†ìŒ")
    lines.append("")

    lines.append("ğŸ†• *ë‰´ë­ì»¤*")
    if change["new_in"]:
        for r in change["new_in"]:
            lines.append(f"- {r['name']} NEW â†’ {r['curr']}ìœ„")
    else:
        lines.append("- í•´ë‹¹ ì—†ìŒ")
    lines.append("")

    lines.append("ğŸ“‰ *ê¸‰í•˜ë½*")
    had = False
    for r in change["falling"]:
        lines.append(f"- {r['name']} {r['prev']}ìœ„ â†’ {r['curr']}ìœ„ (â†“{r['delta']})"); had=True
    outs = [o for o in change["out"] if o["prev"]<=30]
    for o in outs:
        lines.append(f"- {o['name']} {o['prev']}ìœ„ â†’ OUT"); had=True
    if not had: lines.append("- í•´ë‹¹ ì—†ìŒ")
    lines.append("")

    lines.append("ğŸ” *ë­í¬ ì¸&ì•„ì›ƒ*")
    lines.append(f"{change['inout_count']}ê°œì˜ ì œí’ˆì´ ì¸&ì•„ì›ƒ ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return "\n".join(lines)

# ============= MAIN =============
def main():
    t0 = time.time()
    print("ìˆ˜ì§‘ ì‹œì‘:", RANK_URL)

    # 1) ìˆ˜ì§‘
    try:
        items = fetch_playwright()
        print("[Playwright] ì •ìƒ ìˆ˜ì§‘")
    except Exception as e:
        print("[Playwright ì‹¤íŒ¨ â†’ Requests í´ë°±]", e)
        items = fetch_requests()

    cnt = len([i for i in items if i.get("rank")])
    print("ìˆ˜ì§‘ ì™„ë£Œ:", cnt)
    if cnt < 10:
        raise RuntimeError("ì œí’ˆ ì¹´ë“œê°€ ë„ˆë¬´ ì ê²Œ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì…€ë ‰í„°/ë Œë”ë§ ì ê²€ í•„ìš”")

    # 2) ì €ì¥
    csv_path = DATA_DIR / f"ë‹¤ì´ì†Œëª°_ë·°í‹°ìœ„ìƒ_ì¼ê°„_{today_kst()}.csv"
    rows = [{"date": today_kst(), **i} for i in items]
    save_csv(csv_path, rows)

    # 3) ì „ì¼ ë¹„êµ
    prev_path = prev_csv_path(csv_path)
    prev_rows = load_csv(prev_path)
    change = analyze(rows, prev_rows)

    # 4) ë“œë¼ì´ë¸Œ ì—…ë¡œë“œ (ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ë§í¬ëŠ” ë©”ì‹œì§€ì— ë¯¸í‘œê¸°)
    try:
        file_id = gdrive_upload(csv_path)
        print("Drive ì—…ë¡œë“œ ì™„ë£Œ:", file_id)
    except Exception as e:
        print("[Drive ì—…ë¡œë“œ ì‹¤íŒ¨]", e)

    # 5) ìŠ¬ë™ ë©”ì‹œì§€
    msg = slack_message(rows, change)
    slack(msg)

    print(f"ì´ {cnt}ê±´, ê²½ê³¼: {time.time()-t0:.1f}s")

if __name__ == "__main__":
    try:
        main()
    except Exception:
        print(traceback.format_exc())
        sys.exit(1)
