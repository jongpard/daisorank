# app.py â€” DaisoMall ë·°í‹°/ìœ„ìƒ 'ì¼ê°„' ë­í‚¹ ìˆ˜ì§‘ (2025-10-28 êµ¬ì¡° ëŒ€ì‘íŒ)
# - DOM í•´ì‹œ í´ë˜ìŠ¤(dDeMca ë“±) ë¬´ì‹œ: .product-list > div ì»¨í…Œì´ë„ˆì˜ í…ìŠ¤íŠ¸/ë§í¬ ê¸°ë°˜ íŒŒì‹±
# - ë¬´í•œ ìŠ¤í¬ë¡¤ ì•ˆì •í™”(ì•µì»¤ ê°œìˆ˜ ê¸°ë°˜), ìµœì†Œ ì¹´ë“œìˆ˜ ê°€ë“œ
# - Google Drive(OAuth) ì—…ë¡œë“œ/ì „ì¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ í›„ ë¹„êµ ë¶„ì„
# - Slack ìš”ì•½ ë¦¬í¬íŠ¸(Top10 ë³€ë™, ê¸‰ìƒìŠ¹/ë‰´ë­ì»¤/ê¸‰í•˜ë½/OUT, ì¸&ì•„ì›ƒ í•©)
# - ë””ë²„ê·¸ ì‚°ì¶œë¬¼ ì €ì¥(data/debug/)

import os, re, csv, io, time, json
from typing import List, Dict, Optional, Tuple
from datetime import datetime, timedelta, timezone

import requests
from playwright.sync_api import sync_playwright, TimeoutError as PWTimeout, Page

# ===== ê¸°ë³¸ ì„¤ì • =====
KST = timezone(timedelta(hours=9))
RANK_URL = os.getenv("DAISO_RANK_URL", "https://www.daisomall.co.kr/ds/rank/C105")
MAX_ITEMS = int(os.getenv("MAX_ITEMS", "200"))          # ëª©í‘œ ìˆ˜ì§‘ ê°œìˆ˜
MIN_VALID = int(os.getenv("MIN_VALID", "10"))           # ìµœì†Œ ìœ íš¨ ì¹´ë“œìˆ˜ (ë¯¸ë§Œì´ë©´ ê°€ë“œ)
SCROLL_MAX = int(os.getenv("SCROLL_MAX", "120"))        # ìŠ¤í¬ë¡¤ ë¼ìš´ë“œ ìƒí•œ
SCROLL_PAUSE = float(os.getenv("SCROLL_PAUSE", "0.5"))  # ë¼ìš´ë“œ ê°„ ëŒ€ê¸°(ì´ˆ)
SCROLL_STABLE = int(os.getenv("SCROLL_STABLE", "6"))    # ì •ì²´ ë¼ìš´ë“œ ìˆ˜

SLACK_WEBHOOK = os.getenv("SLACK_WEBHOOK_URL", "").strip()

# Google Drive OAuth (User)
GDRIVE_FOLDER_ID = os.getenv("GDRIVE_FOLDER_ID", "").strip()
GOOGLE_CLIENT_ID = os.getenv("GOOGLE_CLIENT_ID", "").strip()
GOOGLE_CLIENT_SECRET = os.getenv("GOOGLE_CLIENT_SECRET", "").strip()
GOOGLE_REFRESH_TOKEN = os.getenv("GOOGLE_REFRESH_TOKEN", "").strip()

# ===== ë‚ ì§œ ìœ í‹¸ =====
def today_str() -> str:
    return datetime.now(KST).strftime("%Y-%m-%d")

def yday_str() -> str:
    return (datetime.now(KST) - timedelta(days=1)).strftime("%Y-%m-%d")

# ===== íŒŒì¼/ë””ë²„ê·¸ =====
def ensure_dirs():
    os.makedirs("data", exist_ok=True)
    os.makedirs("data/debug", exist_ok=True)

def save_text(path: str, text: str):
    with open(path, "w", encoding="utf-8") as f:
        f.write(text)

# ===== Playwright ìœ í‹¸ =====
def wait_net_idle(page: Page, ms: int = 400):
    page.wait_for_load_state("networkidle")
    page.wait_for_timeout(ms)

def select_category_daily(page: Page):
    """
    í˜ì´ì§€ê°€ ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ë¡œ ì—´ë¦¬ê±°ë‚˜ ì´ˆê¸°ê°’ì´ 'ì£¼ê°„/ì›”ê°„'ì¼ ìˆ˜ ìˆì–´ ë°©ì–´ì  í´ë¦­.
    (ì‹¤íŒ¨í•´ë„ ì¹˜ëª…ì ì´ì§€ ì•Šë„ë¡ try ë¸”ë¡ êµ¬ì„±)
    """
    try:
        # ì¹´í…Œê³ ë¦¬(ë·°í‹°/ìœ„ìƒ)
        if page.locator('.prod-category .cate-btn[value="CTGR_00014"]').count() > 0:
            page.locator('.prod-category .cate-btn[value="CTGR_00014"]').first.click(timeout=1500)
        else:
            btn = page.get_by_role("button", name=re.compile("ë·°í‹°\\/?ìœ„ìƒ"))
            if btn.count() > 0:
                btn.first.click(timeout=1500)
    except Exception:
        pass
    wait_net_idle(page)

    try:
        # ì •ë ¬(ì¼ê°„)
        if page.locator('.ipt-sorting input[value="2"]').count() > 0:
            page.locator('.ipt-sorting input[value="2"]').first.click(timeout=1500)
        else:
            btn = page.get_by_role("button", name=re.compile("ì¼ê°„"))
            if btn.count() > 0:
                btn.first.click(timeout=1500)
    except Exception:
        pass
    wait_net_idle(page, 600)

def count_cards_by_anchor(page: Page) -> int:
    """ /pd/pdr/ ë§í¬ ê°œìˆ˜ë¥¼ ì¹´ë“œ ìˆ˜ë¡œ ê°„ì£¼ (í•´ì‹œ í´ë˜ìŠ¤ íšŒí”¼) """
    return page.locator('a[href*="/pd/pdr/"]').count()

def infinite_scroll(page: Page, target: int = MAX_ITEMS):
    prev = 0
    stable = 0
    for _ in range(SCROLL_MAX):
        page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        wait_net_idle(page, int(SCROLL_PAUSE * 1000))
        cur = count_cards_by_anchor(page)
        if cur >= target:
            break
        if cur == prev:
            stable += 1
            if stable >= SCROLL_STABLE:
                break
        else:
            stable = 0
            prev = cur

def extract_items_from_html(html: str) -> List[Dict]:
    """
    .product-list > div í•˜ìœ„ ë¸”ë¡ì˜ í…ìŠ¤íŠ¸ì—ì„œ '... 5,000 ì› ìƒí’ˆëª… ...' íŒ¨í„´ì„ íŒŒì‹±.
    ë§í¬ëŠ” /pd/pdr/ ì•µì»¤ ì‚¬ìš©.
    """
    items: List[Dict] = []

    # product-list ë¸”ë¡ ë‹¨ìœ„ë¡œ ìª¼ê°  ë’¤, ê° ë¸”ë¡ì—ì„œ í…ìŠ¤íŠ¸/ë§í¬ íŒŒì‹±
    # (ì •ê·œí‘œí˜„ì‹: price â†’ \d[,\d]* ì›, name â†’ 'ì› ' ì´í›„ ~ 'íƒë°°ë°°ì†¡|ë³„ì |ë¦¬ë·°|ë§¤ì¥í”½ì—…|ì˜¤ëŠ˜ë°°ì†¡' ì „ê¹Œì§€)
    # ë§í¬: /pd/pdr/ ?pdNo=xxxxx
    # ë‹¤ìˆ˜ì˜ í•´ì‹œ í´ë˜ìŠ¤(dDeMca ë“±)ë¡œ ë¶„ë¦¬ë˜ì–´ ìˆìœ¼ë¯€ë¡œ product-list ë‹¤ìŒ ê¹Šì´ì˜ <div>ë¥¼ ì¹´ë“œë¡œ ì·¨ê¸‰
    # ì„±ëŠ¥ì„ ìœ„í•´ ê°„ë‹¨í•œ ë¬¸ìì—´ ê¸°ë°˜ íŒŒì‹±ì„ ì‚¬ìš©
    from bs4 import BeautifulSoup
    soup = BeautifulSoup(html, "html.parser")
    plist = soup.select_one(".product-list")
    if not plist:
        return items

    cards = plist.select("> div")
    for rank, card in enumerate(cards, start=1):
        t = card.get_text(" ", strip=True)
        # ê°€ê²©
        m_price = re.search(r'([0-9][0-9,\.]*)\s*ì›', t)
        # ëª…ì¹­(ê°€ê²© ë‹¤ìŒ ë“±ì¥í•˜ëŠ” í‚¤ì›Œë“œ ì „ê¹Œì§€)
        m_name = re.search(r'ì›\s+(.+?)\s+(íƒë°°ë°°ì†¡|ë³„ì |ë¦¬ë·°|ë§¤ì¥í”½ì—…|ì˜¤ëŠ˜ë°°ì†¡)', t)
        # ë§í¬
        a = card.select_one('a[href*="/pd/pdr/"]')
        href = a["href"] if a and a.has_attr("href") else None

        if not (m_price and m_name and href):
            continue

        price_txt = m_price.group(1).replace(",", "")
        try:
            price = int(float(price_txt))
        except Exception:
            price = None

        name = m_name.group(1).strip()
        url = href if href.startswith("http") else f"https://www.daisomall.co.kr{href}"

        items.append(
            {
                "rank": rank,
                "name": name,
                "price": price,
                "url": url,
            }
        )

    return items

def fetch_products() -> List[Dict]:
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        ctx = browser.new_context(
            viewport={"width": 1360, "height": 900},
            user_agent=(
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/123.0.0.0 Safari/537.36"
            ),
        )
        page = ctx.new_page()
        page.goto(RANK_URL, wait_until="domcontentloaded", timeout=45_000)

        # ì´ˆê¸° ë¡œë”© ì•ˆì •í™” & ì¼ê°„/ì¹´í…Œê³ ë¦¬ ê³ ì •
        try:
            page.wait_for_selector(".product-list, .prod-category", timeout=15_000)
        except PWTimeout:
            pass
        select_category_daily(page)

        # ë¬´í•œ ìŠ¤í¬ë¡¤
        infinite_scroll(page, MAX_ITEMS)

        # ë””ë²„ê·¸(ì „/í›„ ì…€ë ‰í„° ì¹´ìš´íŠ¸ + ìŠ¤í¬ë¦°ìƒ· + ì›ë³¸HTML)
        try:
            before = page.locator('a[href*="/pd/pdr/"]').count()
            page.screenshot(path="data/debug/page_after.png", full_page=True)
            html = page.content()
            after = len(re.findall(r'href="[^"]*/pd/pdr/', html))
            save_text("data/debug/rank_raw_after.html", html)
            save_text("data/debug/selector_counts.txt", f"anchors_before={before}, anchors_after={after}\n")
            print("[DEBUG] ìµœì¢… ì¶”ì¶œ ì•µì»¤", after, "ê°œ")
        except Exception:
            pass

        html = page.content()
        items = extract_items_from_html(html)

        ctx.close()
        browser.close()
        return items

# ===== CSV =====
def save_csv(rows: List[Dict]) -> Tuple[str, str]:
    ensure_dirs()
    date_str = today_str()
    filename = f"ë‹¤ì´ì†Œëª°_ë·°í‹°ìœ„ìƒ_ì¼ê°„_{date_str}.csv"
    path = os.path.join("data", filename)
    with open(path, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["date", "rank", "name", "price", "url"])
        for r in rows:
            w.writerow([date_str, r.get("rank"), r.get("name"), r.get("price"), r.get("url")])
    return path, filename

# ===== Google Drive (OAuth) =====
def build_drive_service():
    if not (GOOGLE_CLIENT_ID and GOOGLE_CLIENT_SECRET and GOOGLE_REFRESH_TOKEN):
        print("[Drive] OAuth í™˜ê²½ë³€ìˆ˜ ë¶€ì¡± â†’ ì—…ë¡œë“œ ê±´ë„ˆëœ€")
        return None
    try:
        from googleapiclient.discovery import build
        from googleapiclient.http import MediaIoBaseUpload, MediaIoBaseDownload
        from google.oauth2.credentials import Credentials as UserCredentials
        from google.auth.transport.requests import Request as GoogleRequest

        creds = UserCredentials(
            None,
            refresh_token=GOOGLE_REFRESH_TOKEN,
            client_id=GOOGLE_CLIENT_ID,
            client_secret=GOOGLE_CLIENT_SECRET,
            token_uri="https://oauth2.googleapis.com/token",
            scopes=["https://www.googleapis.com/auth/drive.file"],
        )
        creds.refresh(GoogleRequest())
        svc = build("drive", "v3", credentials=creds, cache_discovery=False)
        # ë°˜í™˜ê³¼ í•¨ê»˜ í—¬í¼ ë¶™ì´ê¸°
        svc._MediaIoBaseUpload = MediaIoBaseUpload
        svc._MediaIoBaseDownload = MediaIoBaseDownload
        return svc
    except Exception as e:
        print("[Drive] ì„œë¹„ìŠ¤ ìƒì„± ì‹¤íŒ¨:", e)
        return None

def drive_upload_csv(svc, filepath: str, filename: str) -> Optional[str]:
    try:
        media = svc._MediaIoBaseUpload(io.FileIO(filepath, "rb"), mimetype="text/csv", resumable=True)
        meta = {"name": filename, "parents": [GDRIVE_FOLDER_ID]} if GDRIVE_FOLDER_ID else {"name": filename}
        f = svc.files().create(body=meta, media_body=media, fields="id,name").execute()
        print(f"[Drive] ì—…ë¡œë“œ ì„±ê³µ: {f.get('name')} (id={f.get('id')})")
        return f.get("id")
    except Exception as e:
        print("[Drive] ì—…ë¡œë“œ ì‹¤íŒ¨:", e)
        return None

def drive_find(svc, filename: str) -> Optional[Dict]:
    try:
        if GDRIVE_FOLDER_ID:
            q = f"name='{filename}' and '{GDRIVE_FOLDER_ID}' in parents and mimeType='text/csv' and trashed=false"
        else:
            q = f"name='{filename}' and mimeType='text/csv' and trashed=false"
        res = svc.files().list(q=q, pageSize=1, fields="files(id,name)").execute()
        files = res.get("files", [])
        return files[0] if files else None
    except Exception as e:
        print("[Drive] ê²€ìƒ‰ ì‹¤íŒ¨:", e)
        return None

def drive_download_text(svc, file_id: str) -> Optional[str]:
    try:
        req = svc.files().get_media(fileId=file_id)
        buf = io.BytesIO()
        dl = svc._MediaIoBaseDownload(buf, req)
        done = False
        while not done:
            _, done = dl.next_chunk()
        buf.seek(0)
        return buf.read().decode("utf-8", errors="replace")
    except Exception as e:
        print("[Drive] ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨:", e)
        return None

# ===== ì „ì¼ ë¹„êµ =====
def parse_prev_csv(csv_text: str) -> List[Dict]:
    arr: List[Dict] = []
    try:
        rd = csv.DictReader(io.StringIO(csv_text))
        for r in rd:
            try:
                rank = int(r.get("rank", "") or "0")
            except Exception:
                rank = None
            arr.append({"rank": rank, "name": r.get("name"), "url": r.get("url")})
    except Exception as e:
        print("[CSV Parse] ì‹¤íŒ¨:", e)
    return arr

def analyze_trends(today: List[Dict], prev: List[Dict], top_window: int = 200):
    prev_map = {p.get("url"): p.get("rank") for p in prev if p.get("url")}
    prev_top = {p.get("url") for p in prev if p.get("url") and (p.get("rank") or 9999) <= top_window}

    trends = []
    for t in today:
        u = t.get("url")
        pr = prev_map.get(u)
        change = (pr - t["rank"]) if pr else None
        trends.append({"name": t["name"], "url": u, "rank": t["rank"], "prev_rank": pr, "change": change})

    movers = [x for x in trends if x["prev_rank"] is not None]
    ups = sorted([x for x in movers if (x["change"] or 0) > 0], key=lambda x: x["change"], reverse=True)
    downs = sorted([x for x in movers if (x["change"] or 0) < 0], key=lambda x: x["change"])

    today_urls = {x["url"] for x in trends if x.get("url")}
    chart_ins = [x for x in trends if x["prev_rank"] is None and x["rank"] <= top_window]
    rank_out_urls = prev_top - today_urls
    rank_outs = [p for p in prev if p.get("url") in rank_out_urls]

    return ups, downs, chart_ins, rank_outs

# ===== Slack =====
def _link(name: str, url: Optional[str]) -> str:
    return f"<{url}|{name}>" if (url and name) else (name or (url or ""))

def post_slack(rows: List[Dict], analysis, prev: Optional[List[Dict]]):
    if not SLACK_WEBHOOK:
        return
    ups, downs, chart_ins, rank_outs = analysis

    # prev map for Top10 diff í‘œì‹œ
    prev_map = {}
    if prev:
        for p in prev:
            k = (p.get("url") or "").strip() or (p.get("name") or "").strip()
            if not k:
                continue
            try:
                prev_map[k] = int(p.get("rank") or 0)
            except Exception:
                pass

    def _key(d: Dict) -> str:
        return (d.get("url") or "").strip() or (d.get("name") or "").strip()

    lines = []
    lines.append(f"*ë‹¤ì´ì†Œëª° ë·°í‹°/ìœ„ìƒ ì¼ê°„ ë­í‚¹ 200* ({datetime.now(KST).strftime('%Y-%m-%d %H:%M KST')})")

    # Top10
    lines.append("\n*TOP 10*")
    for it in rows[:10]:
        cur = int(it["rank"])
        name = it["name"]
        url = it["url"]
        price = it.get("price")
        marker = "(new)"
        pk = _key(it)
        if pk in prev_map:
            diff = prev_map[pk] - cur
            marker = f"(â†‘{diff})" if diff > 0 else ("(â†“%d)" % (-diff) if diff < 0 else "(-)")
        lines.append(f"{cur}. {marker} {_link(name, url)} â€” {price:,}ì›" if price else f"{cur}. {marker} {_link(name, url)}")

    # ê¸‰ìƒìŠ¹
    lines.append("\n*ğŸ”¥ ê¸‰ìƒìŠ¹*")
    if ups:
        for m in ups[:5]:
            lines.append(f"- {_link(m['name'], m['url'])} {m['prev_rank']}ìœ„ â†’ {m['rank']}ìœ„ (â†‘{m['change']})")
    else:
        lines.append("- (í•´ë‹¹ ì—†ìŒ)")

    # ë‰´ë­ì»¤
    lines.append("\n*ğŸ†• ë‰´ë­ì»¤*")
    if chart_ins:
        for x in chart_ins[:5]:
            lines.append(f"- {_link(x['name'], x['url'])} NEW â†’ {x['rank']}ìœ„")
    else:
        lines.append("- (í•´ë‹¹ ì—†ìŒ)")

    # ê¸‰í•˜ë½ + OUT
    lines.append("\n*ğŸ“‰ ê¸‰í•˜ë½*")
    if downs:
        d_sorted = sorted(downs, key=lambda m: (-abs(int(m.get("change") or 0)), int(m.get("rank") or 9999)))
        for m in d_sorted[:5]:
            drop = abs(int(m.get("change") or 0))
            lines.append(f"- {_link(m['name'], m['url'])} {m['prev_rank']}ìœ„ â†’ {m['rank']}ìœ„ (â†“{drop})")
    else:
        lines.append("- (ê¸‰í•˜ë½ ì—†ìŒ)")

    if rank_outs:
        outs_sorted = sorted(rank_outs, key=lambda x: int(x.get("rank") or 9999))
        for ro in outs_sorted[:5]:
            lines.append(f"- {_link(ro.get('name'), ro.get('url'))} {int(ro.get('rank') or 0)}ìœ„ â†’ OUT")
    else:
        lines.append("- (OUT ì—†ìŒ)")

    # ì¸&ì•„ì›ƒ í•©(Top200 ê¸°ì¤€)
    if prev is not None:
        today_keys = {_key(it) for it in rows[:200] if _key(it)}
        prev_keys = {_key(p) for p in prev if _key(p) and 1 <= int(p.get("rank") or 0) <= 200}
        io_cnt = len(today_keys.symmetric_difference(prev_keys)) // 2
        lines.append("\n*â†” ë­í¬ ì¸&ì•„ì›ƒ*")
        lines.append(f"IN/OUT í•©: {io_cnt}")
    try:
        requests.post(SLACK_WEBHOOK, json={"text": "\n".join(lines)}, timeout=12).raise_for_status()
        print("[Slack] ì „ì†¡ ì„±ê³µ")
    except Exception as e:
        print("[Slack] ì „ì†¡ ì‹¤íŒ¨:", e)

# ===== main =====
def main():
    print("ìˆ˜ì§‘ ì‹œì‘:", RANK_URL)
    t0 = time.time()

    ensure_dirs()
    rows = fetch_products()
    print(f"[ìˆ˜ì§‘ ì™„ë£Œ] ê°œìˆ˜: {len(rows)}")

    # ìµœì†Œ ì¹´ë“œìˆ˜ ê°€ë“œ (ì‚¬ì´íŠ¸ ë³€ë™ ë“±)
    if len(rows) < MIN_VALID:
        print(f"[ê²½ê³ ] ìœ íš¨ ìƒí’ˆ ì¹´ë“œê°€ {len(rows)}ê°œ â€” ê¸°ì¤€({MIN_VALID}) ë¯¸ë‹¬ â†’ ê·¸ë˜ë„ CSV/ë“œë¼ì´ë¸Œ/ìŠ¬ë™ ì§„í–‰")
    # CSV ì €ì¥
    csv_path, csv_filename = save_csv(rows)
    print("ë¡œì»¬ ì €ì¥:", csv_path)

    # ë“œë¼ì´ë¸Œ ì—…ë¡œë“œ & ì „ì¼ ë¹„êµ
    prev_items: List[Dict] = []
    svc = build_drive_service()
    if svc:
        _id = drive_upload_csv(svc, csv_path, csv_filename)
        yname = f"ë‹¤ì´ì†Œëª°_ë·°í‹°ìœ„ìƒ_ì¼ê°„_{yday_str()}.csv"
        prev_file = drive_find(svc, yname)
        if prev_file:
            txt = drive_download_text(svc, prev_file["id"])
            if txt:
                prev_items = parse_prev_csv(txt)
                print(f"[ë¶„ì„] ì „ì¼ ë°ì´í„° {len(prev_items)}ê±´ ë¡œë“œ")
        else:
            print(f"[Drive] ì „ì¼ íŒŒì¼ ì—†ìŒ: {yname}")

    # ë¹„êµ/ìŠ¬ë™
    analysis = analyze_trends(rows, prev_items) if prev_items else ([], [], [], [])
    post_slack(rows, analysis, prev_items if prev_items else None)

    print(f"ì´ {len(rows)}ê±´, ê²½ê³¼ ì‹œê°„: {time.time() - t0:.1f}s")

if __name__ == "__main__":
    main()
