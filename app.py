# app.py â€” DaisoMall ë·°í‹°/ìœ„ìƒ 'ì¼ê°„' ë­í‚¹ ìˆ˜ì§‘ (ê°•í™”íŒ)
# - ìµœì‹  DOM ëŒ€ì‘: div.product-info + /pd/pdr/ ë§í¬ ê¸°ë°˜ ì¶”ì¶œ
# - ìŠ¤í¬ë¡¤ ë¡œë” ê°œì„ : 200ê°œ ë„ë‹¬ê¹Œì§€ ì•ˆì • ìŠ¤í¬ë¡¤
# - ì „ì¼ ë¹„êµ/Slack ë¦¬í¬íŠ¸(ìš”ì²­ í¬ë§·) + GDrive ì—…/ë‹¤ìš´

import os, re, csv, time, io
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Optional, Tuple

import requests
from playwright.sync_api import sync_playwright, TimeoutError as PWTimeout, Page, Locator

# Google Drive (OAuth)
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseUpload, MediaIoBaseDownload
from google.oauth2.credentials import Credentials as UserCredentials
from google.auth.transport.requests import Request as GoogleRequest

# ====== ì„¤ì • ======
RANK_URL = "https://www.daisomall.co.kr/ds/rank/C105"  # ë·°í‹°/ìœ„ìƒ Â· ì¼ê°„
MAX_ITEMS = int(os.getenv("MAX_ITEMS", "200"))
TOP_WINDOW = 150
SCROLL_PAUSE_MS = int(float(os.getenv("SCROLL_PAUSE", "650")))

SCROLL_STABLE_ROUNDS = int(os.getenv("SCROLL_STABLE_ROUNDS", "8"))
SCROLL_MAX_ROUNDS = int(os.getenv("SCROLL_MAX_ROUNDS", "160"))

SLACK_WEBHOOK = os.getenv("SLACK_WEBHOOK_URL", "")
GDRIVE_FOLDER_ID = os.getenv("GDRIVE_FOLDER_ID", "")
GOOGLE_CLIENT_ID = os.getenv("GOOGLE_CLIENT_ID", "")
GOOGLE_CLIENT_SECRET = os.getenv("GOOGLE_CLIENT_SECRET", "")
GOOGLE_REFRESH_TOKEN = os.getenv("GOOGLE_REFRESH_TOKEN", "")

KST = timezone(timedelta(hours=9))

# ====== ìœ í‹¸ ======
def today_str() -> str: return datetime.now(KST).strftime("%Y-%m-%d")
def yday_str() -> str:  return (datetime.now(KST) - timedelta(days=1)).strftime("%Y-%m-%d")

def strip_best(name: str) -> str:
    if not name: return ""
    name = re.sub(r"^\s*BEST\s*[\|\-:\u00A0]*", "", name, flags=re.I)
    name = re.sub(r"\s*\bBEST\b\s*", " ", name, flags=re.I)
    return re.sub(r"\s+", " ", name).strip()

def parse_name_price(text: str) -> Tuple[Optional[str], Optional[int]]:
    # "5,000 ì› [ìƒí’ˆëª…] íƒë°°ë°°ì†¡ ..." í˜•íƒœì—ì„œ ê°€ê²©/ì´ë¦„ ë½‘ê¸°
    text = re.sub(r"\s+", " ", (text or "")).strip()
    m = re.search(r"([0-9,]+)\s*ì›\s*(.+?)(?:\s*(?:íƒë°°ë°°ì†¡|ë§¤ì¥í”½ì—…|ì˜¤ëŠ˜ë°°ì†¡|ë³„ì |ë¦¬ë·°|êµ¬ë§¤|ì¿ í°|ì¥ë°”êµ¬ë‹ˆ|ì°œ|ìƒì„¸))", text)
    if not m: 
        return None, None
    try:
        price = int(m.group(1).replace(",", ""))
    except Exception:
        price = None
    name = strip_best(m.group(2).strip())
    return name or None, price

def _to_locator(page: Page, target) -> Locator:
    return target if isinstance(target, Locator) else page.locator(target)

def close_overlays(page: Page):
    sels = [
        ".layer-popup .btn-close", ".modal .btn-close", ".popup .btn-close",
        ".layer-popup .close", ".modal .close", ".popup .close",
        ".btn-x", ".btn-close, button[aria-label='ë‹«ê¸°']"
    ]
    for sel in sels:
        try:
            loc = page.locator(sel)
            if loc.count() > 0:
                loc.first.click(timeout=800)
                page.wait_for_timeout(150)
        except Exception: pass

# ====== í˜ì´ì§€ ê³ ì •(ë·°í‹°/ìœ„ìƒ Â· ì¼ê°„) ======
def ensure_tab(page: Page):
    close_overlays(page)
    # ì¹´í…Œê³ ë¦¬ 'ë·°í‹°/ìœ„ìƒ'
    try:
        if page.locator('.prod-category .cate-btn[value="CTGR_00014"]').count() > 0:
            page.locator('.prod-category .cate-btn[value="CTGR_00014"]').first.click(timeout=1200)
        else:
            page.get_by_role("button", name=re.compile("ë·°í‹°\\/?ìœ„ìƒ")).click(timeout=1200)
    except Exception:
        # ê°•ì œ í´ë¦­
        page.evaluate("""
            () => {
              const byVal = document.querySelector('.prod-category .cate-btn[value="CTGR_00014"]');
              if (byVal) byVal.click();
              else {
                const btns = [...document.querySelectorAll('.prod-category .cate-btn, .prod-category *')];
                const t = btns.find(b => /ë·°í‹°\\/?ìœ„ìƒ/.test((b.textContent||"").trim()));
                if (t) t.click();
              }
            }
        """)
    page.wait_for_timeout(300)

    # ì •ë ¬/ê¸°ê°„: ì¼ê°„
    try:
        page.locator('.ipt-sorting input[value="2"]').first.click(timeout=1200)  # ì¼ê°„
    except Exception:
        try:
            page.get_by_role("button", name=re.compile("ì¼ê°„")).click(timeout=1200)
        except Exception:
            pass
    page.wait_for_timeout(350)

# ====== ìŠ¤í¬ë¡¤ ë¡œë”(200ê°œê¹Œì§€) ======
def load_all(page: Page, target_min: int = MAX_ITEMS) -> int:
    prev = 0
    stable = 0
    for r in range(SCROLL_MAX_ROUNDS):
        page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        page.wait_for_timeout(SCROLL_PAUSE_MS)
        # í˜„ì¬ ë¡œë“œëœ ì¹´ë“œ ìˆ˜(ì‹ ê·œ DOM ê¸°ì¤€)
        try:
            cnt = page.evaluate("""() => {
              return document.querySelectorAll('div.product-info a[href*="/pd/pdr/"]').length
            }""")
        except Exception:
            cnt = 0
        if cnt >= target_min:
            return cnt
        if cnt == prev:
            stable += 1
            if stable >= SCROLL_STABLE_ROUNDS:
                break
        else:
            stable = 0
            prev = cnt
    return prev

# ====== ì¶”ì¶œ(ë¸Œë¼ìš°ì € JS ìš°ì„  + íŒŒì´ì¬ íŒŒì„œ ë³´ì •) ======
def extract_items_js(page: Page) -> List[Dict]:
    data = page.evaluate("""
      () => {
        const cards = [...document.querySelectorAll('div.product-info')];
        const items = [];
        const seen = new Set();
        for (const info of cards) {
          const a = info.querySelector('a[href*="/pd/pdr/"]');
          if (!a) continue;
          let href = a.href || a.getAttribute('href') || '';
          if (!href) continue;
          // ì ˆëŒ€ê²½ë¡œ ë³´ì •
          if (!/^https?:/i.test(href)) href = new URL(href, location.origin).href;

          const text = (info.textContent || '').replace(/\\s+/g,' ').trim();
          // ê°€ê²©/ì´ë¦„ ì¶”ì¶œì€ ì„œë²„ì—ì„œ ì¬ì²˜ë¦¬
          items.push({ raw: text, url: href });
        }
        return items;
      }
    """)
    # í›„ì²˜ë¦¬(ì´ë¦„/ê°€ê²© íŒŒì‹±)
    cleaned = []
    for it in data:
        name, price = parse_name_price(it.get("raw",""))
        if not (name and price and price > 0): 
            continue
        cleaned.append({"name": name, "price": price, "url": it["url"]})
    for i, it in enumerate(cleaned, 1):
        it["rank"] = i
    return cleaned

def extract_items_fallback(html_text: str) -> List[Dict]:
    # BeautifulSoup fallbackìš©(ì›Œí¬í”Œë¡œ ì•„í‹°íŒ©íŠ¸/ë””ë²„ê·¸ HTML í™œìš©)
    from bs4 import BeautifulSoup
    soup = BeautifulSoup(html_text, "html.parser")
    items = []
    for info in soup.select("div.product-info"):
        a = info.select_one('a[href*="/pd/pdr/"]')
        href = ""
        if a:
            href = a.get("href") or ""
            if href and not href.startswith("http"):
                href = "https://www.daisomall.co.kr" + href
        text = info.get_text(" ", strip=True)
        name, price = parse_name_price(text)
        if not (name and price and href): 
            continue
        items.append({"name": name, "price": price, "url": href})
    for i, it in enumerate(items, 1):
        it["rank"] = i
    return items

def fetch_products() -> List[Dict]:
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            viewport={"width": 1380, "height": 940},
            user_agent=("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                        "AppleWebKit/537.36 (KHTML, like Gecko) "
                        "Chrome/123.0.0.0 Safari/537.36"),
        )
        page = context.new_page()
        page.goto(RANK_URL, wait_until="domcontentloaded", timeout=45_000)
        ensure_tab(page)

        loaded = load_all(page, MAX_ITEMS)
        # ë””ë²„ê·¸ìš© HTML ì €ì¥(ì›Œí¬í”Œë¡œ ì•„í‹°íŒ©íŠ¸)
        os.makedirs("data/debug", exist_ok=True)
        page_content = page.content()
        with open(f"data/debug/rank_raw_{today_str()}.html", "w", encoding="utf-8") as f:
            f.write(page_content)

        items = extract_items_js(page)
        context.close(); browser.close()

        return items

# ====== CSV ì €ì¥ ======
def save_csv(rows: List[Dict]) -> Tuple[str,str]:
    date_str = today_str()
    os.makedirs("data", exist_ok=True)
    filename = f"ë‹¤ì´ì†Œëª°_ë·°í‹°ìœ„ìƒ_ì¼ê°„_{date_str}.csv"
    path = os.path.join("data", filename)
    with open(path, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["date", "rank", "name", "price", "url"])
        for r in rows:
            w.writerow([date_str, r["rank"], r["name"], r["price"], r["url"]])
    return path, filename

# ====== Google Drive ======
def build_drive_service():
    if not (GOOGLE_CLIENT_ID and GOOGLE_CLIENT_SECRET and GOOGLE_REFRESH_TOKEN):
        print("[Drive] OAuth í™˜ê²½ë³€ìˆ˜ ë¯¸ì„¤ì •")
        return None
    try:
        creds = UserCredentials(
            None,
            refresh_token=GOOGLE_REFRESH_TOKEN,
            client_id=GOOGLE_CLIENT_ID,
            client_secret=GOOGLE_CLIENT_SECRET,
            token_uri="https://oauth2.googleapis.com/token",
            scopes=["https://www.googleapis.com/auth/drive.file"],
        )
        creds.refresh(GoogleRequest())
        return build("drive", "v3", credentials=creds, cache_discovery=False)
    except Exception as e:
        print("[Drive] ì„œë¹„ìŠ¤ ìƒì„± ì‹¤íŒ¨:", e)
        return None

def upload_to_drive(service, filepath: str, filename: str):
    if not service or not GDRIVE_FOLDER_ID:
        print("[Drive] ì„œë¹„ìŠ¤ ë˜ëŠ” í´ë” ID ë¯¸ì„¤ì • â†’ ì—…ë¡œë“œ ìƒëµ")
        return None
    try:
        media = MediaIoBaseUpload(io.FileIO(filepath, 'rb'), mimetype="text/csv", resumable=True)
        body = {"name": filename, "parents": [GDRIVE_FOLDER_ID]}
        file = service.files().create(body=body, media_body=media, fields="id,name").execute()
        print(f"[Drive] ì—…ë¡œë“œ ì„±ê³µ: {file.get('name')} (ID: {file.get('id')})")
        return file.get("id")
    except Exception as e:
        print("[Drive] ì—…ë¡œë“œ ì‹¤íŒ¨:", e)
        return None

def find_file_in_drive(service, filename: str):
    if not service or not GDRIVE_FOLDER_ID:
        return None
    try:
        q = f"name='{filename}' and '{GDRIVE_FOLDER_ID}' in parents and mimeType='text/csv' and trashed=false"
        res = service.files().list(q=q, pageSize=1, fields="files(id,name)").execute()
        return res.get("files", [])[0] if res.get("files") else None
    except Exception as e:
        print(f"[Drive] íŒŒì¼ ê²€ìƒ‰ ì‹¤íŒ¨ ({filename}):", e)
        return None

def download_from_drive(service, file_id: str) -> Optional[str]:
    try:
        request = service.files().get_media(fileId=file_id)
        fh = io.BytesIO()
        downloader = MediaIoBaseDownload(fh, request)
        done = False
        while not done:
            _, done = downloader.next_chunk()
        fh.seek(0)
        return fh.read().decode("utf-8")
    except Exception as e:
        print(f"[Drive] íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (ID: {file_id}):", e)
        return None

# ====== ì „ì¼ íŒŒì‹±/ë¶„ì„ ======
def parse_prev_csv(csv_text: str) -> List[Dict]:
    import csv as _csv, io as _io
    items = []
    try:
        reader = _csv.DictReader(_io.StringIO(csv_text))
        for row in reader:
            try:
                items.append({"rank": int(row.get("rank")), "name": row.get("name"), "url": row.get("url")})
            except Exception: pass
    except Exception as e:
        print("[CSV Parse] ì „ì¼ ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨:", e)
    return items

def analyze_trends(today_items: List[Dict], prev_items: List[Dict]):
    prev_map = {p["url"]: p["rank"] for p in prev_items if p.get("url")}
    prev_top_urls = {p["url"] for p in prev_items if p.get("url") and p.get("rank", 999) <= TOP_WINDOW}

    trends = []
    for it in today_items:
        url = it.get("url")
        pr = prev_map.get(url)
        trends.append({"name": it["name"], "url": url, "rank": it["rank"], "prev_rank": pr,
                       "change": (pr - it["rank"]) if pr else None})

    movers = [t for t in trends if t["prev_rank"] is not None]
    ups   = sorted([t for t in movers if (t["change"] or 0) > 0], key=lambda x: x["change"], reverse=True)
    downs = sorted([t for t in movers if (t["change"] or 0) < 0], key=lambda x: x["change"])

    chart_ins = [t for t in trends if t["prev_rank"] is None and t["rank"] <= TOP_WINDOW]

    today_urls = {t["url"] for t in trends if t.get("url")}
    rank_out_urls = prev_top_urls - today_urls
    rank_outs = [p for p in prev_items if p.get("url") in rank_out_urls]

    # ì¸&ì•„ì›ƒ(ì§‘í•© ê¸°ì¤€) â€” í•­ìƒ INâ‰¡OUT
    return ups, downs, chart_ins, rank_outs, len(chart_ins)

# ====== Slack ë¦¬í¬íŠ¸ ======
def post_slack(rows: List[Dict], analysis_results, prev_items: Optional[List[Dict]] = None):
    if not SLACK_WEBHOOK: return
    ups, downs, chart_ins, rank_outs, _ = analysis_results

    def _link(name: str, url: Optional[str]) -> str:
        return f"<{url}|{name}>" if url else (name or "")

    # TOP10 ë³€ë™í‘œì‹œìš© ë§µ(í‚¤: url ìš°ì„ , ì—†ìœ¼ë©´ name)
    def _key(it: dict) -> str:
        return (it.get("url") or "").strip() or (it.get("name") or "").strip()

    prev_map: Dict[str, int] = {}
    if prev_items:
        for p in prev_items:
            try:
                r = int(p.get("rank") or 0)
            except Exception:
                continue
            k = _key(p)
            if k and r > 0:
                prev_map[k] = r

    now_kst = datetime.now(KST)
    title = f"*ë‹¤ì´ì†Œëª° ë·°í‹°/ìœ„ìƒ ì¼ê°„ ë­í‚¹ 200* ({now_kst.strftime('%Y-%m-%d %H:%M KST')})"
    lines = [title]

    # TOP 10
    lines.append("\n*TOP 10*")
    for it in (rows or [])[:10]:
        ptxt = f"{int(it.get('price') or 0):,}ì›"
        cur_r = int(it.get("rank") or 0)
        k = _key(it)
        marker = "(new)"
        if k in prev_map:
            prev_r = prev_map[k]
            diff = prev_r - cur_r
            marker = f"(â†‘{diff})" if diff > 0 else (f"(â†“{abs(diff)})" if diff < 0 else "(-)")
        lines.append(f"{cur_r}. {marker} {_link(it.get('name') or '', it.get('url'))} â€” {ptxt}")

    # ğŸ”¥ ê¸‰ìƒìŠ¹
    lines.append("\n*ğŸ”¥ ê¸‰ìƒìŠ¹*")
    if ups:
        for m in ups[:5]:
            lines.append(f"- {_link(m['name'], m['url'])} {m['prev_rank']}ìœ„ â†’ {m['rank']}ìœ„ (â†‘{m['change']})")
    else:
        lines.append("- (í•´ë‹¹ ì—†ìŒ)")

    # ğŸ†• ë‰´ë­ì»¤
    lines.append("\n*ğŸ†• ë‰´ë­ì»¤*")
    if chart_ins:
        for t in chart_ins[:5]:
            lines.append(f"- {_link(t['name'], t['url'])} NEW â†’ {t['rank']}ìœ„")
    else:
        lines.append("- (í•´ë‹¹ ì—†ìŒ)")

    # ğŸ“‰ ê¸‰í•˜ë½ + OUT
    lines.append("\n*ğŸ“‰ ê¸‰í•˜ë½*")
    if downs:
        downs_sorted = sorted(downs, key=lambda m: (-abs(int(m.get("change") or 0)),
                                                    int(m.get("rank") or 9999),
                                                    int(m.get("prev_rank") or 9999)))
        for m in downs_sorted[:5]:
            drop = abs(int(m.get("change") or 0))
            lines.append(f"- {_link(m['name'], m['url'])} {m['prev_rank']}ìœ„ â†’ {m['rank']}ìœ„ (â†“{drop})")
    else:
        lines.append("- (ê¸‰í•˜ë½ ì—†ìŒ)")

    if rank_outs:
        outs_sorted = sorted(rank_outs, key=lambda x: int(x.get("rank") or 9999))
        for ro in outs_sorted[:5]:
            prev_r = int(ro.get("rank") or 0)
            lines.append(f"- {_link(ro.get('name'), ro.get('url'))} {prev_r}ìœ„ â†’ OUT")
    else:
        lines.append("- (OUT ì—†ìŒ)")

    # â†” ë­í¬ ì¸&ì•„ì›ƒ (ì§‘í•© ê¸°ì¤€)
    io_cnt = 0
    if prev_items is not None:
        today_keys = {_key(it) for it in (rows or [])[:200] if _key(it)}
        prev_keys  = {_key(p) for p in (prev_items or []) if _key(p) and 1 <= int(p.get("rank") or 0) <= 200}
        io_cnt = len(today_keys.symmetric_difference(prev_keys)) // 2

    lines.append("\n*:+1::ì–‘ë°©í–¥_í™”ì‚´í‘œ: ë­í¬ ì¸&ì•„ì›ƒ*")
    lines.append(f"*{io_cnt}ê°œì˜ ì œí’ˆì´ ì¸&ì•„ì›ƒ ë˜ì—ˆìŠµë‹ˆë‹¤.*")

    try:
        requests.post(SLACK_WEBHOOK, json={"text": "\n".join(lines)}, timeout=10).raise_for_status()
        print("[Slack] ì „ì†¡ ì„±ê³µ")
    except Exception as e:
        print("[Slack] ì „ì†¡ ì‹¤íŒ¨:", e)

# ====== main ======
def main():
    print("ìˆ˜ì§‘ ì‹œì‘:", RANK_URL)
    t0 = time.time()
    rows = fetch_products()
    print(f"[ìˆ˜ì§‘ ì™„ë£Œ] ê°œìˆ˜: {len(rows)}")

    if len(rows) < MAX_ITEMS:
        print(f"[ê²½ê³ ] ëª©í‘œ {MAX_ITEMS} < ì‹¤ì œ {len(rows)} â†’ ì†ŒìŠ¤ êµ¬ì¡°/ë¡œë”© ì´ìŠˆ ê°€ëŠ¥")

    # CSV ë¡œì»¬ ì €ì¥
    csv_path, csv_filename = save_csv(rows)
    print("ë¡œì»¬ ì €ì¥:", csv_path)

    # êµ¬ê¸€ ë“œë¼ì´ë¸Œ
    drive_service = build_drive_service()
    prev_items: List[Dict] = []
    if drive_service:
        upload_to_drive(drive_service, csv_path, csv_filename)

        yday_filename = f"ë‹¤ì´ì†Œëª°_ë·°í‹°ìœ„ìƒ_ì¼ê°„_{yday_str()}.csv"
        prev_file = find_file_in_drive(drive_service, yday_filename)
        if prev_file:
            print(f"[Drive] ì „ì¼ íŒŒì¼ ë°œê²¬: {prev_file['name']} (ID: {prev_file['id']})")
            csv_content = download_from_drive(drive_service, prev_file['id'])
            if csv_content:
                prev_items = parse_prev_csv(csv_content)
                print(f"[ë¶„ì„] ì „ì¼ ë°ì´í„° {len(prev_items)}ê±´ ë¡œë“œ")
        else:
            print(f"[Drive] ì „ì¼ íŒŒì¼({yday_filename}) ë¯¸ë°œê²¬")

        analysis_results = analyze_trends(rows, prev_items)
    else:
        analysis_results = ([], [], [], [], 0)

    # Slack
    post_slack(rows, analysis_results, prev_items)

    print(f"ì´ ê²½ê³¼: {time.time()-t0:.1f}s")

if __name__ == "__main__":
    main()
