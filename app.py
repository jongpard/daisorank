# -*- coding: utf-8 -*-
"""
ë‹¤ì´ì†Œëª° ë·°í‹°/ìœ„ìƒ 'ì¼ê°„' ë­í‚¹ í¬ë¡¤ëŸ¬
- ìˆ˜ì§‘: Playwright(ìš°ì„ ) + Requests(í´ë°±)
- íƒ­ ê°•ì œ: 'ë·°í‹°/ìœ„ìƒ' ì¹´í…Œê³ ë¦¬, 'ì¼ê°„' íƒ­ ê³ ì • (ê²€ì¦/ì¬ì‹œë„)
- ë¡œë“œ: ë¬´í•œ ìŠ¤í¬ë¡¤ + 'ë”ë³´ê¸°' ë³‘í–‰, ìµœì†Œ TARGET_COUNTê¹Œì§€ ê°•ì œ
- ì €ì¥: data/ë‹¤ì´ì†Œëª°_ë·°í‹°ìœ„ìƒ_ì¼ê°„_YYYY-MM-DD.csv (KST)
- ìŠ¬ë™: ì˜¬ë¦¬ë¸Œì˜ í¬ë§· (TOP10 â†’ ê¸‰ìƒìŠ¹ â†’ ë‰´ë­ì»¤ â†’ ê¸‰í•˜ë½(5ê°œ) â†’ ë­í¬ ì¸&ì•„ì›ƒ)
- ë“œë¼ì´ë¸Œ: refresh token oauth-only ì—…ë¡œë“œ (IDëŠ” ë¡œê·¸ì—ë§Œ ë‚¨ê¸°ê³  ë©”ì‹œì§€ ë¯¸ë…¸ì¶œ)
"""
import os
import re
import csv
import sys
import time
import traceback
import pathlib
import datetime as dt
from typing import List, Dict, Optional

import pytz
import requests
from bs4 import BeautifulSoup

# -------------------- ê³ ì •ê°’/ê²½ë¡œ --------------------
BASE_URL = "https://www.daisomall.co.kr"
RANK_URL = f"{BASE_URL}/ds/rank/C105"  # ë·°í‹°/ìœ„ìƒ
DATA_DIR = pathlib.Path("data")
DEBUG_DIR = pathlib.Path("data/debug")
DATA_DIR.mkdir(parents=True, exist_ok=True)
DEBUG_DIR.mkdir(parents=True, exist_ok=True)
KST = pytz.timezone("Asia/Seoul")

# ìµœì†Œ ìˆ˜ì§‘ ëª©í‘œ ê°œìˆ˜ (ê¸°ë³¸ 200; í•„ìš” ì‹œ í™˜ê²½ë³€ìˆ˜ë¡œ ì¡°ì ˆ: 100/200 ë“±)
TARGET_COUNT = int(os.getenv("DAISO_TARGET_COUNT", "200"))

# -------------------- í™˜ê²½ë³€ìˆ˜ --------------------
SLACK_WEBHOOK_URL = os.getenv("SLACK_WEBHOOK_URL", "").strip()
GOOGLE_CLIENT_ID = os.getenv("GOOGLE_CLIENT_ID", "").strip()
GOOGLE_CLIENT_SECRET = os.getenv("GOOGLE_CLIENT_SECRET", "").strip()
GOOGLE_REFRESH_TOKEN = os.getenv("GOOGLE_REFRESH_TOKEN", "").strip()
GDRIVE_FOLDER_ID = os.getenv("GDRIVE_FOLDER_ID", "").strip()

# -------------------- ìœ í‹¸ --------------------
def today_kst() -> str:
    return dt.datetime.now(KST).strftime("%Y-%m-%d")

def to_int(s: str) -> Optional[int]:
    try:
        return int(re.sub(r"[^\d]", "", s))
    except Exception:
        return None

def fmt_won(n: Optional[int]) -> str:
    if n is None:
        return "0ì›"
    return f"{n:,}ì›"

def slack(text: str):
    if not SLACK_WEBHOOK_URL:
        print("[Slack] ë¯¸ì„¤ì •")
        return
    try:
        requests.post(SLACK_WEBHOOK_URL, json={"text": text}, timeout=15)
        print("[Slack] ì „ì†¡ ì™„ë£Œ")
    except Exception as e:
        print("[Slack ì‹¤íŒ¨]", e)

def load_csv(path: Optional[pathlib.Path]) -> List[Dict]:
    if not path or not isinstance(path, pathlib.Path) or not path.exists() or not path.is_file():
        return []
    out = []
    with path.open("r", newline="", encoding="utf-8") as f:
        r = csv.DictReader(f)
        for row in r:
            out.append({
                "date": row.get("date",""),
                "rank": int(row.get("rank","0") or 0),
                "name": row.get("name",""),
                "price": int(row.get("price","0") or 0),
                "url": row.get("url",""),
            })
    return out

def save_csv(path: pathlib.Path, rows: List[Dict]):
    cols = ["date","rank","name","price","url"]
    with path.open("w", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=cols)
        w.writeheader()
        for r in rows:
            w.writerow({k:r.get(k,"") for k in cols})

# -------------------- Google Drive --------------------
def gdrive_service():
    from google.oauth2.credentials import Credentials
    from googleapiclient.discovery import build
    if not (GOOGLE_CLIENT_ID and GOOGLE_CLIENT_SECRET and GOOGLE_REFRESH_TOKEN and GDRIVE_FOLDER_ID):
        raise RuntimeError("Drive ìê²©/í´ë” ID ë¯¸ì„¤ì •")
    creds = Credentials(
        None,
        refresh_token=GOOGLE_REFRESH_TOKEN,
        token_uri="https://oauth2.googleapis.com/token",
        client_id=GOOGLE_CLIENT_ID,
        client_secret=GOOGLE_CLIENT_SECRET,
        # scopes ë¯¸ì§€ì • â†’ refresh token ê¶Œí•œë§Œ ì‚¬ìš© (invalid_scope íšŒí”¼)
    )
    return build("drive","v3",credentials=creds, cache_discovery=False)

def gdrive_upload(path: pathlib.Path) -> str:
    from googleapiclient.http import MediaInMemoryUpload
    svc = gdrive_service()
    media = MediaInMemoryUpload(path.read_bytes(), mimetype="text/csv", resumable=False)
    meta = {"name": path.name, "parents":[GDRIVE_FOLDER_ID]}
    f = svc.files().create(body=meta, media_body=media, fields="id", supportsAllDrives=True).execute()
    return f["id"]

# -------------------- íŒŒì‹± --------------------
def _clean_name(txt: str) -> str:
    # 'BEST |' ì ‘ë‘ ì œê±° + ê³µë°± ì¶•ì†Œ
    t = re.sub(r"^\s*BEST\s*\|\s*", "", (txt or "").strip(), flags=re.I)
    return " ".join(t.split())

def parse_html(html: str) -> List[Dict]:
    soup = BeautifulSoup(html, "lxml")
    items: List[Dict] = []
    cards = soup.select(".goods-list.type-card .goods-unit")
    for idx, card in enumerate(cards, start=1):
        # ì´ë¦„
        name = ""
        for sel in [".goods-name", ".title", ".tit", ".name", ".goods-info .txt"]:
            el = card.select_one(sel)
            if el:
                name = el.get_text(" ", strip=True)
                break
        name = _clean_name(name)

        # ê°€ê²©
        price_txt = ""
        for sel in [".goods-price .value", ".price .value", ".goods-price .price", ".price"]:
            el = card.select_one(sel)
            if el:
                price_txt = el.get_text(strip=True)
                break
        price = to_int(price_txt) or 0

        # ë§í¬
        href = ""
        a = card.select_one("a")
        if a and a.has_attr("href"):
            href = a["href"]
        url = href if href.startswith("http") else (BASE_URL + href if href else RANK_URL)

        items.append({
            "rank": idx,
            "name": name,
            "price": price,
            "url": url,
        })
    return items

# -------------------- Playwright ìˆ˜ì§‘ --------------------
def fetch_playwright() -> List[Dict]:
    from playwright.sync_api import sync_playwright

    def is_day_active(page) -> bool:
        grp = page.locator(".el-radio-group.ipt-sorting")
        try:
            act = grp.locator("label.is-active, label.active")
            if act.count() and "ì¼ê°„" in act.first.inner_text():
                return True
        except Exception:
            pass
        try:
            day = grp.locator("label:has-text('ì¼ê°„')")
            if day.count():
                pressed = (day.first.get_attribute("aria-pressed") or "") == "true"
                checked = (day.first.locator("input").get_attribute("aria-checked") or "") == "true"
                if pressed or checked:
                    return True
        except Exception:
            pass
        return False

    def ensure_category_and_day(page):
        # 1) ë·°í‹°/ìœ„ìƒ on
        try:
            cat = page.locator("button.cate-btn:has-text('ë·°í‹°/ìœ„ìƒ')")
            if cat.count():
                cls = cat.first.get_attribute("class") or ""
                if "on" not in cls:
                    cat.first.click()
                    page.wait_for_timeout(500)
        except Exception:
            pass
        # 2) ì¼ê°„ íƒ­ ê°•ì œ(ì£¼ê°„â†’ì¼ê°„ ì—°ì† í´ë¦­ í¬í•¨, ê²€ì¦ ìµœëŒ€ 6íšŒ)
        for _ in range(6):
            if is_day_active(page):
                return
            try:
                week = page.locator(".el-radio-group.ipt-sorting label:has-text('ì£¼ê°„')")
                if week.count():
                    week.first.click()
            except Exception:
                pass
            try:
                day = page.locator(".el-radio-group.ipt-sorting label:has-text('ì¼ê°„')")
                if day.count():
                    day.first.click()
            except Exception:
                pass
            page.wait_for_timeout(700)
        print("[warn] ì¼ê°„ íƒ­ ê³ ì • í™•ì¸ ì‹¤íŒ¨(í˜„ì¬ íƒ­ìœ¼ë¡œ ì§„í–‰)")

    def force_load(page, target: int) -> int:
        def count_cards():
            return page.locator(".goods-list.type-card .goods-unit").count()

        last = -1
        stall = 0
        for _ in range(80):  # ì•ˆì „ í•œë„
            if count_cards() >= target:
                break
            # ë”ë³´ê¸° í´ë¦­
            try:
                more = page.locator("button:has-text('ë”ë³´ê¸°')")
                if more.count() and more.first.is_enabled():
                    more.first.click()
                    page.wait_for_timeout(800)
            except Exception:
                pass
            # ìŠ¤í¬ë¡¤
            page.mouse.wheel(0, 3600)
            page.wait_for_timeout(700)
            cur = count_cards()
            if cur == last:
                stall += 1
                if stall >= 6:  # ì¦ê°€ ì—†ìŒ 6íšŒ ì—°ì† â†’ ì¢…ë£Œ
                    break
            else:
                stall = 0
                last = cur
        return count_cards()

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True, args=["--no-sandbox","--disable-dev-shm-usage","--disable-gpu"])
        ctx = browser.new_context(locale="ko-KR", viewport={"width":1440,"height":2000})
        page = ctx.new_page()
        page.goto(RANK_URL, wait_until="domcontentloaded", timeout=60_000)

        ensure_category_and_day(page)
        total = force_load(page, TARGET_COUNT)

        # ë””ë²„ê·¸ ì €ì¥
        DEBUG_DIR.mkdir(parents=True, exist_ok=True)
        (DEBUG_DIR / "page_rank.png").write_bytes(page.screenshot(full_page=True))
        (DEBUG_DIR / "page_rank.html").write_text(page.content(), encoding="utf-8")

        html = page.content()
        ctx.close()
        browser.close()

    items = parse_html(html)
    # TARGET_COUNTë¡œ ì˜ë ¸ì„ ìˆ˜ ìˆìœ¼ë‹ˆ ìƒìœ„ë§Œ ì·¨í•¨
    return items[:max(TARGET_COUNT, 1)]

# -------------------- Requests í´ë°± --------------------
def fetch_requests() -> List[Dict]:
    r = requests.get(RANK_URL, timeout=20, headers={
        "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0 Safari/537.36"
    })
    r.raise_for_status()
    html = r.text
    (DEBUG_DIR / "page_rank.html").write_text(html, encoding="utf-8")
    return parse_html(html)

# -------------------- ì „ì¼ ë¹„êµ/ë³€ë™ --------------------
def normalize_name(s: str) -> str:
    return re.sub(r"\s+"," ",s or "").strip().lower()

def prev_csv_path(today_csv: pathlib.Path) -> Optional[pathlib.Path]:
    stem = "ë‹¤ì´ì†Œëª°_ë·°í‹°ìœ„ìƒ_ì¼ê°„_"
    files = sorted(DATA_DIR.glob(f"{stem}*.csv"))
    prevs = [p for p in files if p.name < today_csv.name]
    return prevs[-1] if prevs else None

def analyze(today: List[Dict], prev: List[Dict]) -> Dict[str, List[Dict]]:
    N = 30
    tmap = {normalize_name(x["name"]): x for x in today}
    pmap = {normalize_name(x["name"]): x for x in prev}
    rising, falling, new_in, out = [], [], [], []

    for k, t in tmap.items():
        if t["rank"]<=N:
            p = pmap.get(k)
            if p:
                d = p["rank"] - t["rank"]
                if d>0:
                    rising.append({"name":t["name"],"prev":p["rank"],"curr":t["rank"],"delta":d,"url":t["url"]})
            else:
                new_in.append({"name":t["name"],"prev":None,"curr":t["rank"],"url":t["url"]})

    for k, p in pmap.items():
        if p["rank"]<=N:
            t = tmap.get(k)
            if not t or t["rank"]>N:
                out.append({"name":p["name"],"prev":p["rank"],"curr":None})
            else:
                d = t["rank"] - p["rank"]
                if d>0:
                    falling.append({"name":t["name"],"prev":p["rank"],"curr":t["rank"],"delta":d,"url":t["url"]})

    rising.sort(key=lambda x:(-x["delta"], x["curr"], x["prev"], normalize_name(x["name"])))
    falling.sort(key=lambda x:(-x["delta"], x["prev"], x["curr"], normalize_name(x["name"])))
    new_in.sort(key=lambda x:(x["curr"], normalize_name(x["name"])))
    out.sort(key=lambda x:(x["prev"], normalize_name(x["name"])))

    return {
        "rising": rising[:3],
        "new_in": new_in[:3],
        "falling": falling[:5],
        "out": out,
        "inout_count": len(new_in)+len(out),
    }

# -------------------- ìŠ¬ë™ ë©”ì‹œì§€ --------------------
def slack_message(today_rows: List[Dict], change: Dict) -> str:
    lines = []
    lines.append(f"*ë‹¤ì´ì†Œëª° ë·°í‹°/ìœ„ìƒ ì¼ê°„ â€” {today_kst()}*")
    lines.append("")
    lines.append("*TOP 10*")
    for r in [x for x in today_rows if x["rank"]<=10]:
        nm = r["name"].replace("&","&amp;").replace("<","ã€ˆ").replace(">","ã€‰")
        lines.append(f"{r['rank']}. <{r['url']}|{nm}> â€” {fmt_won(r['price'])}")
    lines.append("")

    lines.append("ğŸ”¥ *ê¸‰ìƒìŠ¹*")
    if change["rising"]:
        for r in change["rising"]:
            lines.append(f"- {r['name']} {r['prev']}ìœ„ â†’ {r['curr']}ìœ„ (â†‘{r['delta']})")
    else:
        lines.append("- í•´ë‹¹ ì—†ìŒ")
    lines.append("")

    lines.append("ğŸ†• *ë‰´ë­ì»¤*")
    if change["new_in"]:
        for r in change["new_in"]:
            lines.append(f"- {r['name']} NEW â†’ {r['curr']}ìœ„")
    else:
        lines.append("- í•´ë‹¹ ì—†ìŒ")
    lines.append("")

    lines.append("ğŸ“‰ *ê¸‰í•˜ë½*")
    had = False
    for r in change["falling"]:
        lines.append(f"- {r['name']} {r['prev']}ìœ„ â†’ {r['curr']}ìœ„ (â†“{r['delta']})"); had=True
    outs = [o for o in change["out"] if o["prev"]<=30]
    for o in outs:
        lines.append(f"- {o['name']} {o['prev']}ìœ„ â†’ OUT"); had=True
    if not had: lines.append("- í•´ë‹¹ ì—†ìŒ")
    lines.append("")

    lines.append("ğŸ” *ë­í¬ ì¸&ì•„ì›ƒ*")
    lines.append(f"{change['inout_count']}ê°œì˜ ì œí’ˆì´ ì¸&ì•„ì›ƒ ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return "\n".join(lines)

# -------------------- MAIN --------------------
def main():
    t0 = time.time()
    print("ìˆ˜ì§‘ ì‹œì‘:", RANK_URL)

    # 1) ìˆ˜ì§‘
    try:
        items = fetch_playwright()
        print("[Playwright] ì •ìƒ ìˆ˜ì§‘")
    except Exception as e:
        print("[Playwright ì‹¤íŒ¨ â†’ Requests í´ë°±]", e)
        items = fetch_requests()

    cnt = len([i for i in items if i.get("name")])
    print("ìˆ˜ì§‘ ì™„ë£Œ:", cnt)
    if cnt < 20:
        raise RuntimeError("ì œí’ˆ ì¹´ë“œê°€ ë„ˆë¬´ ì ê²Œ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì…€ë ‰í„°/ë Œë”ë§ ì ê²€ í•„ìš”")

    # 2) ì €ì¥
    csv_path = DATA_DIR / f"ë‹¤ì´ì†Œëª°_ë·°í‹°ìœ„ìƒ_ì¼ê°„_{today_kst()}.csv"
    rows = [{"date": today_kst(), **{"rank":i["rank"], "name":i["name"], "price":i["price"], "url":i["url"]}} for i in items]
    save_csv(csv_path, rows)

    # 3) ì „ì¼ ë¹„êµ
    prev_path = prev_csv_path(csv_path)
    prev_rows = load_csv(prev_path)
    change = analyze(rows, prev_rows)

    # 4) ë“œë¼ì´ë¸Œ ì—…ë¡œë“œ (IDëŠ” ë¡œê·¸ë§Œ)
    try:
        file_id = gdrive_upload(csv_path)
        print("Drive ì—…ë¡œë“œ ì™„ë£Œ:", file_id)
    except Exception as e:
        print("[Drive ì—…ë¡œë“œ ì‹¤íŒ¨]", e)

    # 5) ìŠ¬ë™ ë©”ì‹œì§€
    msg = slack_message(rows, change)
    slack(msg)

    print(f"ì´ {cnt}ê±´, ê²½ê³¼: {time.time()-t0:.1f}s")

if __name__ == "__main__":
    try:
        main()
    except Exception:
        print(traceback.format_exc())
        sys.exit(1)
