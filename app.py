# app.py â€” DaisoMall ë·°í‹°/ìœ„ìƒ 'ì¼ê°„' Top200 ë­í‚¹ ìˆ˜ì§‘/ë¹„êµ/ë¦¬í¬íŠ¸ (2025-10 êµ¬ì¡° ëŒ€ì‘)
# - 200ìœ„ê¹Œì§€ ìˆ˜ì§‘: ?page=1..N í˜ì´ì§€ë„¤ì´ì…˜ ë£¨í”„
# - ë¹„êµ ì•ˆì •í‚¤: product_code(pdNo)  âœ NEW/ì¸&ì•„ì›ƒ ì˜¤íŒ ë°©ì§€
# - ì¸&ì•„ì›ƒ: ìŒ ì¼ì¹˜ ê°•ì œ(ë‹¨ì¼ ìˆ«ì), ë¶ˆì¼ì¹˜ ì‹œ ì „ì†¡ ì°¨ë‹¨ ê°€ë“œ
# - Google Drive(OAuth): ì˜¤ëŠ˜ ì—…ë¡œë“œ + ì „ì¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë¹„êµ
# - Slack: TOP10(ë³€ë™ â†‘â†“âˆ’/new), ê¸‰ìƒìŠ¹/ë‰´ë­ì»¤/ê¸‰í•˜ë½/OUT, ì¸&ì•„ì›ƒ ë‹¨ì¼ ë¬¸ì¥

import os, re, io, csv, time, json
from typing import List, Dict, Optional, Tuple
from datetime import datetime, timedelta, timezone

import requests
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright, Page, TimeoutError as PWTimeout

# ===== KST / ë‚ ì§œ =====
KST = timezone(timedelta(hours=9))
def kst_now() -> datetime: return datetime.now(KST)
def today_str() -> str:     return kst_now().strftime("%Y-%m-%d")
def yday_str() -> str:      return (kst_now() - timedelta(days=1)).strftime("%Y-%m-%d")

# ===== ì„¤ì • =====
RANK_URL = os.getenv("DAISO_RANK_URL", "https://www.daisomall.co.kr/ds/rank/C105")
MAX_ITEMS = 200
PER_PAGE_SAFE_MAX = 120     # í•œ í˜ì´ì§€ì—ì„œ ë³´ì´ëŠ” ìµœëŒ€ì¹˜(ì—¬ìœ  ìƒí•œ)
PAGE_LIMIT = 10             # ì•ˆì „ ìƒí•œ(200 ëª»ì±„ìš°ëŠ” ë¹„ì •ìƒ ë£¨í”„ ë°©ì§€)

SLACK_WEBHOOK = os.getenv("SLACK_WEBHOOK_URL", "").strip()

# Google Drive (OAuth)
GDRIVE_FOLDER_ID    = os.getenv("GDRIVE_FOLDER_ID", "").strip()
GOOGLE_CLIENT_ID    = os.getenv("GOOGLE_CLIENT_ID", "").strip()
GOOGLE_CLIENT_SECRET= os.getenv("GOOGLE_CLIENT_SECRET", "").strip()
GOOGLE_REFRESH_TOKEN= os.getenv("GOOGLE_REFRESH_TOKEN", "").strip()

# ===== ë””ë²„ê·¸ ì•„ì›ƒ =====
def ensure_dirs():
    os.makedirs("data", exist_ok=True)
    os.makedirs("data/debug", exist_ok=True)

def save_text(path: str, text: str):
    with open(path, "w", encoding="utf-8") as f:
        f.write(text)

# ===== ê³µí†µ íŒŒì„œ =====
PDNO_RE = re.compile(r"[?&]pdNo=(\d+)")

def parse_krw(s: str) -> Optional[int]:
    s = re.sub(r"[^\d]", "", s or "")
    return int(s) if s else None

def extract_rank(s: str) -> Optional[int]:
    m = re.search(r"\d+", s or "")
    return int(m.group()) if m else None

def parse_items_from_html(html: str) -> List[Dict]:
    """
    ì„œë²„ ë Œë”ëœ ë­í‚¹ ë¦¬ìŠ¤íŠ¸ë¥¼ íŒŒì‹±. (ì‹  UI í´ë˜ìŠ¤ ë³€ê²½ ëŒ€ì‘: ì˜ë¯¸ ìˆëŠ” í…ìŠ¤íŠ¸ ê¸°ë°˜)
    í•„ìˆ˜ ì»¬ëŸ¼: rank, raw_name, price, url, product_code(pdNo)
    """
    soup = BeautifulSoup(html, "html.parser")

    # ì¹´ë“œ ì»¨í…Œì´ë„ˆ íƒìƒ‰(ì—¬ëŸ¬ ë³€í˜• ëŒ€ì‘)
    cards = (
        soup.select("ul#goodsList li") or
        soup.select("ul.goods_list li") or
        soup.select(".product-list > li, .product-list > div")
    )

    items: List[Dict] = []
    for i, li in enumerate(cards, start=1):
        a = li.select_one("a[href]")
        href = a["href"].strip() if a and a.has_attr("href") else ""
        if href and href.startswith("/"):
            href = "https://www.daisomall.co.kr" + href

        pdno = None
        if href:
            m = PDNO_RE.search(href)
            pdno = m.group(1) if m else None

        name_el = li.select_one(".product_name, .goods_name, .name, .tit")
        raw_name = (name_el.get_text(" ", strip=True) if name_el else "").strip()

        brand_el = li.select_one(".brand, .brand_name")
        brand = (brand_el.get_text(" ", strip=True) if brand_el else "").strip()

        price_el = li.select_one(".product_price, .sale_price, .final, .price")
        price_txt = (price_el.get_text("", strip=True) if price_el else "")
        price = parse_krw(price_txt)

        rank_el = li.select_one(".rank, .num, .badge_rank")
        r = extract_rank(rank_el.get_text("", strip=True) if rank_el else "")
        rank = r if r else i

        if raw_name and price and href and pdno:
            items.append({
                "rank": int(rank),
                "brand": brand,
                "raw_name": raw_name,
                "price": int(price),
                "url": href,
                "product_code": pdno,
            })

    return items

# ===== Playwrightë¡œ í˜ì´ì§€ë„¤ì´ì…˜ ìˆ˜ì§‘ =====
def fetch_html_with_playwright(url: str) -> str:
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        ctx = browser.new_context(
            viewport={"width": 1360, "height": 900},
            user_agent=("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                        "AppleWebKit/537.36 (KHTML, like Gecko) "
                        "Chrome/125.0.0.0 Safari/537.36"),
        )
        page = ctx.new_page()
        page.goto(url, wait_until="domcontentloaded", timeout=45_000)
        # networkidle í•œë°•ì
        try: page.wait_for_load_state("networkidle", timeout=3000)
        except Exception: pass
        html = page.content()
        # ë””ë²„ê·¸: í˜ì´ì§€1 ì €ì¥
        if "page=1" in url or url.endswith("/C105") or url.endswith("/C105/"):
            save_text("data/debug/rank_raw_page1.html", html)
        ctx.close(); browser.close()
        return html

def collect_top200() -> List[Dict]:
    print(f"ìˆ˜ì§‘ ì‹œì‘: {RANK_URL}")
    results: List[Dict] = []
    seen = set()

    # page= íŒŒë¼ë¯¸í„° ì§€ì›: 1..N ë£¨í”„
    # ê¸°ë³¸ URLì´ /C105 í˜•íƒœë©´ page=1ë¶€í„° ë¶™ì—¬ ìˆœíšŒ
    base = RANK_URL
    if "?" in base and "page=" in base:
        base = re.sub(r"([?&])page=\d+", r"\g<1>page=%d", base)
    elif "?" in base:
        base = base + "&page=%d"
    else:
        base = base + "?page=%d"

    page_no = 1
    while len(results) < MAX_ITEMS and page_no <= PAGE_LIMIT:
        url = base % page_no
        html = fetch_html_with_playwright(url)
        items = parse_items_from_html(html)
        if not items:
            # í˜ì´ì§€ê°€ ì—†ìœ¼ë©´ break
            if page_no == 1:
                # í˜¹ì‹œ ê¸°ë³¸ URL ìì²´ì— pageê°€ í•„ìš” ì—†ì„ ë•Œ ë§ˆì§€ë§‰ ë°©ì–´ë¡œ í•œ ë²ˆ ë” íŒŒì‹±
                items = parse_items_from_html(fetch_html_with_playwright(RANK_URL))
                if not items:
                    break
            else:
                break

        # ìˆœìœ„ ë³´ì •(í˜ì´ì§€ ì˜¤í”„ì…‹ ê³ ë ¤: ì‚¬ì´íŠ¸ê°€ ì ˆëŒ€ìˆœìœ„ í‘œê¸° ì•ˆ í•´ë„ ì •ë ¬ ìœ ì§€)
        for it in items:
            code = it["product_code"]
            if code in seen:  # ì¤‘ë³µ ë°©ì§€
                continue
            seen.add(code)
            results.append(it)
            if len(results) >= MAX_ITEMS:
                break

        print(f"  - page {page_no}: ëˆ„ì  {len(results)}ê°œ")
        if len(items) < 1 or len(items) < PER_PAGE_SAFE_MAX // 2:
            # í¬ë°•í•˜ê²Œ ì ìœ¼ë©´ ë” ì´ìƒ í˜ì´ì§€ê°€ ì—†ë‹¤ê³  íŒë‹¨
            break
        page_no += 1

    # ì ˆëŒ€ìˆœìœ„ ì •ë ¬
    results = sorted(results, key=lambda x: int(x["rank"]))[:MAX_ITEMS]
    print(f"[ìˆ˜ì§‘ ì™„ë£Œ] {len(results)}ê°œ")
    return results

# ===== CSV =====
def save_csv(rows: List[Dict]) -> Tuple[str, str]:
    ensure_dirs()
    fn = f"ë‹¤ì´ì†Œëª°_ë·°í‹°ìœ„ìƒ_ì¼ê°„_{today_str()}.csv"
    path = os.path.join("data", fn)
    with open(path, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["date","rank","brand","raw_name","price","url","product_code"])
        for r in rows:
            w.writerow([today_str(), r["rank"], r["brand"], r["raw_name"], r["price"], r["url"], r["product_code"]])
    return path, fn

# ===== Google Drive (OAuth) =====
def build_drive_service():
    if not (GOOGLE_CLIENT_ID and GOOGLE_CLIENT_SECRET and GOOGLE_REFRESH_TOKEN):
        print("[Drive] OAuth í™˜ê²½ë³€ìˆ˜ ë¶€ì¡± â†’ ì—…ë¡œë“œ ìƒëµ")
        return None
    try:
        from googleapiclient.discovery import build
        from googleapiclient.http import MediaIoBaseUpload, MediaIoBaseDownload
        from google.oauth2.credentials import Credentials as UserCredentials
        from google.auth.transport.requests import Request as GoogleRequest

        creds = UserCredentials(
            None,
            refresh_token=GOOGLE_REFRESH_TOKEN,
            client_id=GOOGLE_CLIENT_ID,
            client_secret=GOOGLE_CLIENT_SECRET,
            token_uri="https://oauth2.googleapis.com/token",
            scopes=["https://www.googleapis.com/auth/drive.file"],
        )
        creds.refresh(GoogleRequest())
        svc = build("drive","v3",credentials=creds, cache_discovery=False)
        svc._Upload = MediaIoBaseUpload
        svc._Download = MediaIoBaseDownload
        return svc
    except Exception as e:
        print("[Drive] ì„œë¹„ìŠ¤ ìƒì„± ì‹¤íŒ¨:", e); return None

def drive_upload(svc, filepath: str, filename: str) -> Optional[str]:
    try:
        media = svc._Upload(io.FileIO(filepath,"rb"), mimetype="text/csv", resumable=True)
        meta = {"name": filename, "parents":[GDRIVE_FOLDER_ID]} if GDRIVE_FOLDER_ID else {"name": filename}
        file = svc.files().create(body=meta, media_body=media, fields="id,name").execute()
        print(f"[Drive] ì—…ë¡œë“œ ì„±ê³µ: {file.get('name')} (id={file.get('id')})")
        return file.get("id")
    except Exception as e:
        print("[Drive] ì—…ë¡œë“œ ì‹¤íŒ¨:", e); return None

def drive_find_csv(svc, filename: str) -> Optional[str]:
    try:
        q = f"name='{filename}' and trashed=false and mimeType='text/csv'"
        if GDRIVE_FOLDER_ID:
            q += f" and '{GDRIVE_FOLDER_ID}' in parents"
        res = svc.files().list(q=q, pageSize=1, fields="files(id,name)").execute()
        files = res.get("files",[])
        return files[0]["id"] if files else None
    except Exception as e:
        print("[Drive] ê²€ìƒ‰ ì‹¤íŒ¨:", e); return None

def drive_download_text(svc, file_id: str) -> Optional[str]:
    try:
        req = svc.files().get_media(fileId=file_id)
        buf = io.BytesIO()
        dl = svc._Download(buf, req)
        done = False
        while not done:
            _, done = dl.next_chunk()
        buf.seek(0); return buf.read().decode("utf-8")
    except Exception as e:
        print("[Drive] ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨:", e); return None

# ===== ì „ì¼ ë¹„êµ / ì¸&ì•„ì›ƒ =====
def parse_prev_csv(text: str) -> List[Dict]:
    out = []
    rd = csv.DictReader(io.StringIO(text))
    for r in rd:
        try:
            out.append({
                "rank": int(r.get("rank") or 0),
                "raw_name": r.get("raw_name") or "",
                "url": r.get("url") or "",
                "product_code": (r.get("product_code") or "").strip()
            })
        except Exception:
            continue
    return out

def analyze_trends(today: List[Dict], prev: Optional[List[Dict]], topN: int = 200):
    if not prev:
        # ë¹„êµ ë¶ˆê°€: NEWë„ í‘œì‹œí•˜ì§€ ì•ŠìŒ / ì¸&ì•„ì›ƒ 0
        for t in today: t["prev_rank"]=None; t["is_new"]=False
        return today, [], [], [], 0

    prev_top = {p["product_code"]: p["rank"] for p in prev if (p.get("product_code") and p.get("rank")<=topN)}
    prev_set = set(prev_top.keys())

    for t in today:
        code = t.get("product_code")
        pr = prev_top.get(code)
        t["prev_rank"] = pr
        t["is_new"] = pr is None

    movers = [t for t in today if t["prev_rank"] is not None]
    ups   = sorted([m for m in movers if m["prev_rank"] > m["rank"]],
                   key=lambda x: (x["prev_rank"]-x["rank"]), reverse=True)
    downs = sorted([m for m in movers if m["prev_rank"] < m["rank"]],
                   key=lambda x: (x["rank"]-x["prev_rank"]), reverse=True)

    today_set = {t["product_code"] for t in today[:topN] if t.get("product_code")}
    ins_set  = today_set - prev_set
    outs_set = prev_set - today_set
    # ë°˜ë“œì‹œ ìŒ ì¼ì¹˜ì—¬ì•¼ í•¨ â†’ ë¶ˆì¼ì¹˜ ì‹œ ì‘ì€ ìª½ ê¸°ì¤€ìœ¼ë¡œ ë§ì¶¤, ê°€ë“œ ë¡œê·¸
    io_cnt = min(len(ins_set), len(outs_set))

    chart_ins = [t for t in today if (t["product_code"] in ins_set and t["rank"]<=topN)]
    rank_outs = [p for p in prev if (p["product_code"] in outs_set and p["rank"]<=topN)]

    return today, ups, downs, chart_ins, rank_outs, io_cnt

# ===== Slack =====
def _link(name: str, url: Optional[str]) -> str:
    return f"<{url}|{name}>" if (name and url) else (name or (url or ""))

def post_slack(today: List[Dict], ups, downs, chart_ins, rank_outs, io_cnt: int):
    if not SLACK_WEBHOOK: return

    lines = []
    lines.append(f"*ë‹¤ì´ì†Œëª° ë·°í‹°/ìœ„ìƒ ì¼ê°„ ë­í‚¹ 200* ({kst_now().strftime('%Y-%m-%d %H:%M KST')})")
    lines.append("\n*TOP 10*")
    prev_map = {t["product_code"]: t["prev_rank"] for t in today if t.get("product_code") is not None}
    for it in today[:10]:
        cur = it["rank"]; name=it["raw_name"]; url=it["url"]; price=it["price"]
        pr  = it.get("prev_rank")
        if pr is None:
            marker = "(new)"
        else:
            diff = pr - cur
            marker = f"(â†‘{diff})" if diff>0 else (f"(â†“{abs(diff)})" if diff<0 else "(-)")
        lines.append(f"{cur}. {marker} {_link(name,url)} â€” {price:,}ì›")

    lines.append("\n*ğŸ”¥ ê¸‰ìƒìŠ¹*")
    if ups:
        for m in ups[:5]:
            lines.append(f"- {_link(m['raw_name'], m['url'])} {m['prev_rank']}ìœ„ â†’ {m['rank']}ìœ„ (â†‘{m['prev_rank']-m['rank']})")
    else:
        lines.append("- (í•´ë‹¹ ì—†ìŒ)")

    lines.append("\n*ğŸ†• ë‰´ë­ì»¤*")
    if chart_ins:
        for x in chart_ins[:5]:
            lines.append(f"- {_link(x['raw_name'], x['url'])} NEW â†’ {x['rank']}ìœ„")
    else:
        lines.append("- (í•´ë‹¹ ì—†ìŒ)")

    lines.append("\n*ğŸ“‰ ê¸‰í•˜ë½*")
    if downs:
        for m in downs[:5]:
            lines.append(f"- {_link(m['raw_name'], m['url'])} {m['prev_rank']}ìœ„ â†’ {m['rank']}ìœ„ (â†“{m['rank']-m['prev_rank']})")
    else:
        lines.append("- (ê¸‰í•˜ë½ ì—†ìŒ)")

    if rank_outs:
        rank_outs_sorted = sorted(rank_outs, key=lambda x: int(x.get("rank") or 9999))
        for ro in rank_outs_sorted[:5]:
            lines.append(f"- {_link(ro.get('raw_name') or '', ro.get('url'))} {ro.get('rank')}ìœ„ â†’ OUT")
    else:
        lines.append("- (OUT ì—†ìŒ)")

    # ì¸&ì•„ì›ƒ: ë‹¨ì¼ ìˆ«ì(ìŒ ì¼ì¹˜)
    lines.append("\n:left_right_arrow: ë­í¬ ì¸&ì•„ì›ƒ")
    if io_cnt==0:
        lines.append("ë³€ë™ ì—†ìŒ.")
    else:
        lines.append(f"{io_cnt}ê°œì˜ ì œí’ˆì´ ì¸&ì•„ì›ƒ ë˜ì—ˆìŠµë‹ˆë‹¤.")

    try:
        requests.post(SLACK_WEBHOOK, json={"text":"\n".join(lines)}, timeout=12).raise_for_status()
        print("[Slack] ì „ì†¡ ì„±ê³µ")
    except Exception as e:
        print("[Slack] ì „ì†¡ ì‹¤íŒ¨:", e)

# ===== main =====
def main():
    t0 = time.time()
    ensure_dirs()

    rows = collect_top200()
    path, fname = save_csv(rows)

    # Drive ì—…ë¡œë“œ + ì „ì¼ íŒŒì¼ ë¡œë“œ
    svc = build_drive_service()
    prev_list: Optional[List[Dict]] = None
    if svc:
        drive_upload(svc, path, fname)
        yname = f"ë‹¤ì´ì†Œëª°_ë·°í‹°ìœ„ìƒ_ì¼ê°„_{yday_str()}.csv"
        fid = drive_find_csv(svc, yname)
        if fid:
            txt = drive_download_text(svc, fid)
            if txt:
                prev_list = parse_prev_csv(txt)
                print(f"[ë¶„ì„] ì „ì¼ {len(prev_list)}ê±´")

    # ë¹„êµ/ë¦¬í¬íŠ¸
    today_aug, ups, downs, chart_ins, rank_outs, io_cnt = analyze_trends(rows, prev_list, topN=200)
    post_slack(today_aug, ups, downs, chart_ins, rank_outs, io_cnt)

    print(f"ì´ {len(rows)}ê±´, ê²½ê³¼ ì‹œê°„: {time.time()-t0:.1f}s")

if __name__ == "__main__":
    main()
