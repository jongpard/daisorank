# app.py â€” DaisoMall ë·°í‹°/ìœ„ìƒ 'ì¼ê°„' Top200 ìˆ˜ì§‘/ë¹„êµ/ë¦¬í¬íŠ¸ (2025-10 ì•ˆì •í™”)
# - 0ê°œ ìˆ˜ì§‘ ê°€ë“œ(ì¬ì‹œë„â†’ëŒ€ì²´íŒŒì„œâ†’í˜ì´ì§€ë„¤ì´ì…˜ í›„ì—ë„ 0ì´ë©´ ì¢…ë£Œ)
# - ì…€ë ‰í„° ë³€ê²½ ëŒ€ì‘: JS-Eval ê¸°ë°˜ ì¹´ë“œ ì¶”ì¶œ + BeautifulSoup ë³´ì¡°
# - ìŠ¤í¬ë¡¤Â·í˜ì´ì§€ë„¤ì´ì…˜ ë™ì‹œ ì§€ì›: í•œ í˜ì´ì§€ì—ì„œ ì¶©ë¶„íˆ ë¡œë”©ë˜ë©´ ìŠ¤í¬ë¡¤, ì•„ë‹ˆë©´ ?page=
# - ë¹„êµ ì•ˆì •í‚¤: pdNo(product_code) â†’ NEW/ì¸&ì•„ì›ƒ ì •í™•
# - ì¸&ì•„ì›ƒ ë‹¨ì¼ ìˆ«ì(ìŒ ì¼ì¹˜) + ë³¼ë“œ í¬ë§·
# - GDrive OAuth ì—…/ë‹¤ìš´ + ìŠ¬ë™ ë¦¬í¬íŠ¸

import os, re, io, csv, time
from typing import List, Dict, Optional, Tuple
from datetime import datetime, timedelta, timezone

import requests
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright, Page

# ===== KST / ë‚ ì§œ =====
KST = timezone(timedelta(hours=9))
def kst_now() -> datetime: return datetime.now(KST)
def today_str() -> str:     return kst_now().strftime("%Y-%m-%d")
def yday_str() -> str:      return (kst_now() - timedelta(days=1)).strftime("%Y-%m-%d")

# ===== ì„¤ì • =====
RANK_URL = os.getenv("DAISO_RANK_URL", "https://www.daisomall.co.kr/ds/rank/C105")
MAX_ITEMS = 200
SCROLL_ROUNDS = 60
SCROLL_STABLE = 4
PAGE_LIMIT = 8

SLACK_WEBHOOK = os.getenv("SLACK_WEBHOOK_URL", "").strip()

# Google Drive (OAuth)
GDRIVE_FOLDER_ID    = os.getenv("GDRIVE_FOLDER_ID", "").strip()
GOOGLE_CLIENT_ID    = os.getenv("GOOGLE_CLIENT_ID", "").strip()
GOOGLE_CLIENT_SECRET= os.getenv("GOOGLE_CLIENT_SECRET", "").strip()
GOOGLE_REFRESH_TOKEN= os.getenv("GOOGLE_REFRESH_TOKEN", "").strip()

# ===== ìœ í‹¸ =====
def ensure_dirs():
    os.makedirs("data", exist_ok=True)
    os.makedirs("data/debug", exist_ok=True)

def save_text(path: str, text: str):
    with open(path, "w", encoding="utf-8") as f:
        f.write(text)

def parse_krw(s: str) -> Optional[int]:
    s = re.sub(r"[^\d]", "", s or "")
    return int(s) if s else None

PDNO_RE = re.compile(r"[?&]pdNo=(\d+)")

# ===== Playwright ê³µí†µ =====
def open_page(p, url: str) -> Page:
    browser = p.chromium.launch(headless=True)
    ctx = browser.new_context(
        viewport={"width": 1360, "height": 900},
        user_agent=("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                    "AppleWebKit/537.36 (KHTML, like Gecko) "
                    "Chrome/125.0.0.0 Safari/537.36"),
    )
    page = ctx.new_page()
    page.goto(url, wait_until="domcontentloaded", timeout=45_000)
    try: page.wait_for_load_state("networkidle", timeout=3000)
    except Exception: pass
    return page

def close_page(page: Page):
    ctx = page.context
    br = ctx.browser
    ctx.close(); br.close()

# ===== JS ê¸°ë°˜ ì¹´ë“œ ì¶”ì¶œ(ì£¼ íŒŒì„œ) =====
JS_COLLECT = """
() => {
  const qs = (el, sel) => el.querySelector(sel);
  const qsa = (sel) => Array.from(document.querySelectorAll(sel));

  // ì¹´ë“œ í›„ë³´: li/div ì•ˆì— ìƒí’ˆìƒì„¸ ë§í¬ê°€ ìˆëŠ” ìš”ì†Œ
  let cards = qsa('li, div').filter(el => el.querySelector('a[href*="/pd/pdr/"]'));

  const seen = new Set();
  const items = [];

  for (const el of cards) {
    const a = qs(el, 'a[href*="/pd/pdr/"]');
    if (!a) continue;
    let href = a.getAttribute('href') || '';
    if (href.startsWith('/')) href = location.origin + href;

    // pdNo ì•ˆì •í‚¤
    const m = href.match(/[?&]pdNo=(\\d+)/);
    const code = m ? m[1] : null;
    if (!code || seen.has(code)) continue;
    seen.add(code);

    // ì´ë¦„
    const nameSel = ['.product_name','.goods_name','.name','.tit','[class*="name"]','[class*="tit"]'];
    let name = '';
    for (const s of nameSel) { const n = qs(el, s); if (n && n.textContent.trim()) { name = n.textContent.trim(); break; } }
    if (!name) continue;

    // ê°€ê²©
    const priceSel = ['.product_price','.sale_price','.final','.price','[class*="price"]'];
    let priceTxt = '';
    for (const s of priceSel) { const n = qs(el, s); if (n && n.textContent.trim()) { priceTxt = n.textContent.trim(); break; } }
    priceTxt = (priceTxt || '').replace(/[^0-9]/g, '');
    if (!priceTxt) continue;
    const price = parseInt(priceTxt,10)||0;
    if (!price) continue;

    // ë­í¬
    const rankSel = ['.rank','.num','.badge_rank','[class*="rank"]'];
    let rankTxt = '';
    for (const s of rankSel) { const n = qs(el, s); if (n && n.textContent.trim()) { rankTxt = n.textContent.trim(); break; } }
    let rank = null;
    if (rankTxt) { const m = rankTxt.match(/\\d+/); if (m) rank = parseInt(m[0],10); }

    items.push({ rank, raw_name: name, price, url: href, product_code: code });
  }
  // rank ì—†ìœ¼ë©´ ë¬¸ì„œ ìˆœì„œë¡œ ë³´ì •
  let num = 1;
  for (const it of items) { if (!it.rank) it.rank = num; num += 1; }
  // ì •ë ¬
  items.sort((a,b) => a.rank - b.rank);
  return items;
}
"""

def infinite_scroll(page: Page, target_min: int = 120):
    prev = 0; stable = 0
    for _ in range(SCROLL_ROUNDS):
        page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        try: page.wait_for_load_state("networkidle", timeout=1200)
        except Exception: pass
        page.wait_for_timeout(300)
        cnt = page.evaluate("""() => document.querySelectorAll('a[href*="/pd/pdr/"]').length""")
        if cnt >= target_min: break
        if cnt == prev:
            stable += 1
            if stable >= SCROLL_STABLE: break
        else:
            prev = cnt; stable = 0

def collect_cards_js(page: Page) -> List[Dict]:
    try:
        return page.evaluate(JS_COLLECT)
    except Exception:
        return []

# ===== BeautifulSoup ë³´ì¡° íŒŒì„œ =====
def parse_cards_bs(html: str) -> List[Dict]:
    soup = BeautifulSoup(html, "html.parser")
    # a[href*="/pd/pdr/"]ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¹´ë“œ íƒì§€
    items = []
    seen = set()
    for a in soup.select('a[href*="/pd/pdr/"]'):
        href = a.get("href","").strip()
        if href.startswith("/"): href = "https://www.daisomall.co.kr" + href
        m = PDNO_RE.search(href)
        code = m.group(1) if m else None
        if not code or code in seen: continue
        seen.add(code)
        card = a
        for _ in range(3):
            if card.name in ("li","div"): break
            if not card.parent: break
            card = card.parent

        # ì´ë¦„
        name_el = card.select_one(".product_name,.goods_name,.name,.tit,[class*='name'],[class*='tit']")
        name = name_el.get_text(" ", strip=True) if name_el else ""
        # ê°€ê²©
        price_el = card.select_one(".product_price,.sale_price,.final,.price,[class*='price']")
        price = parse_krw(price_el.get_text("", strip=True) if price_el else "")
        if not name or not price: continue

        # ë­í¬
        rank_el = card.select_one(".rank,.num,.badge_rank,[class*='rank']")
        rank = None
        if rank_el:
            m2 = re.search(r"\d+", rank_el.get_text("", strip=True))
            if m2: rank = int(m2.group())

        items.append({"rank": rank, "raw_name": name, "price": price, "url": href, "product_code": code})

    # rank ë³´ì •/ì •ë ¬
    n=1
    for it in items:
        if not it["rank"]: it["rank"]=n
        n+=1
    items.sort(key=lambda x: x["rank"])
    return items

# ===== Top200 ìˆ˜ì§‘(ìŠ¤í¬ë¡¤ â†’ 0ì´ë©´ ì¬ì‹œë„ â†’ ë³´ì¡°íŒŒì„œ â†’ í˜ì´ì§€ë„¤ì´ì…˜) =====
def collect_top200() -> List[Dict]:
    print(f"ìˆ˜ì§‘ ì‹œì‘: {RANK_URL}")
    results: List[Dict] = []
    seen = set()

    with sync_playwright() as p:
        # 1) ì²« í˜ì´ì§€ ìŠ¤í¬ë¡¤ + JS íŒŒì„œ
        page = open_page(p, RANK_URL)
        infinite_scroll(page, target_min=120)
        items = collect_cards_js(page)
        if items:
            save_text("data/debug/rank_raw_page1.html", page.content())
        close_page(page)

        # 2) 0ê°œë©´: í˜ì´ì§€ ë¦¬ë¡œë“œ ì¬ì‹œë„ + ë³´ì¡° íŒŒì„œ
        if not items:
            page = open_page(p, RANK_URL)
            page.reload(wait_until="domcontentloaded")
            try: page.wait_for_load_state("networkidle", timeout=2000)
            except Exception: pass
            infinite_scroll(page, target_min=60)
            items = collect_cards_js(page)
            if not items:
                html = page.content()
                save_text("data/debug/rank_raw_page1.html", html)
                items = parse_cards_bs(html)
            close_page(page)

        # 3) ìˆ˜ì§‘/ì¤‘ë³µì œê±°
        for it in items:
            code = it["product_code"]
            if code in seen: continue
            seen.add(code); results.append(it)
            if len(results) >= MAX_ITEMS: break

        # 4) 200 ë¯¸ë§Œì´ë©´ í˜ì´ì§€ë„¤ì´ì…˜ ?page=2..N
        if len(results) < MAX_ITEMS:
            # base ë§Œë“¤ê¸°
            base = RANK_URL
            if "?" in base and "page=" in base:
                base = re.sub(r"([?&])page=\d+", r"\\1page=%d", base)
            elif "?" in base:
                base += "&page=%d"
            else:
                base += "?page=%d"

            for pg in range(2, PAGE_LIMIT+1):
                if len(results) >= MAX_ITEMS: break
                url = base % pg
                page = open_page(p, url)
                infinite_scroll(page, target_min=60)
                more = collect_cards_js(page)
                if not more:
                    html = page.content()
                    save_text(f"data/debug/rank_raw_page{pg}.html", html)
                    more = parse_cards_bs(html)
                close_page(page)
                if not more: break

                for it in more:
                    code = it["product_code"]
                    if code in seen: continue
                    seen.add(code); results.append(it)
                    if len(results) >= MAX_ITEMS: break

    # rank ì •ë ¬, ìƒìœ„ 200 ì»·
    results.sort(key=lambda x: int(x["rank"]))
    results = results[:MAX_ITEMS]
    print(f"[ìˆ˜ì§‘ ì™„ë£Œ] {len(results)}ê°œ")

    # === ê°•ë ¥ ê°€ë“œ: 0ê°œë©´ ì¦‰ì‹œ ì¢…ë£Œ(ì—…ë¡œë“œ/ìŠ¬ë™ ê¸ˆì§€) ===
    if len(results) == 0:
        raise RuntimeError("ë­í‚¹ ì¹´ë“œ 0ê°œ ìˆ˜ì§‘ â€” ì…€ë ‰í„°/ë¡œë”© ì´ìŠˆ. data/debug/rank_raw_page1.html í™•ì¸ í•„ìš”.")

    return results

# ===== CSV ì €ì¥ =====
def save_csv(rows: List[Dict]) -> Tuple[str, str]:
    ensure_dirs()
    fn = f"ë‹¤ì´ì†Œëª°_ë·°í‹°ìœ„ìƒ_ì¼ê°„_{today_str()}.csv"
    path = os.path.join("data", fn)
    with open(path, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["date","rank","raw_name","price","url","product_code"])
        for r in rows:
            w.writerow([today_str(), r["rank"], r["raw_name"], r["price"], r["url"], r["product_code"]])
    return path, fn

# ===== Google Drive (OAuth) =====
def build_drive_service():
    if not (GOOGLE_CLIENT_ID and GOOGLE_CLIENT_SECRET and GOOGLE_REFRESH_TOKEN):
        print("[Drive] OAuth í™˜ê²½ë³€ìˆ˜ ë¶€ì¡± â†’ ì—…ë¡œë“œ ìƒëµ"); return None
    try:
        from googleapiclient.discovery import build
        from googleapiclient.http import MediaIoBaseUpload, MediaIoBaseDownload
        from google.oauth2.credentials import Credentials as UserCredentials
        from google.auth.transport.requests import Request as GoogleRequest
        creds = UserCredentials(
            None,
            refresh_token=GOOGLE_REFRESH_TOKEN,
            client_id=GOOGLE_CLIENT_ID,
            client_secret=GOOGLE_CLIENT_SECRET,
            token_uri="https://oauth2.googleapis.com/token",
            scopes=["https://www.googleapis.com/auth/drive.file"],
        )
        creds.refresh(GoogleRequest())
        svc = build("drive","v3",credentials=creds, cache_discovery=False)
        svc._Upload = MediaIoBaseUpload
        svc._Download = MediaIoBaseDownload
        return svc
    except Exception as e:
        print("[Drive] ì„œë¹„ìŠ¤ ìƒì„± ì‹¤íŒ¨:", e); return None

def drive_upload(svc, filepath: str, filename: str) -> Optional[str]:
    try:
        media = svc._Upload(io.FileIO(filepath,"rb"), mimetype="text/csv", resumable=True)
        meta = {"name": filename, "parents":[GDRIVE_FOLDER_ID]} if GDRIVE_FOLDER_ID else {"name": filename}
        file = svc.files().create(body=meta, media_body=media, fields="id,name").execute()
        print(f"[Drive] ì—…ë¡œë“œ ì„±ê³µ: {file.get('name')} (id={file.get('id')})")
        return file.get("id")
    except Exception as e:
        print("[Drive] ì—…ë¡œë“œ ì‹¤íŒ¨:", e); return None

def drive_find_csv(svc, filename: str) -> Optional[str]:
    try:
        q = f"name='{filename}' and trashed=false and mimeType='text/csv'"
        if GDRIVE_FOLDER_ID:
            q += f" and '{GDRIVE_FOLDER_ID}' in parents"
        res = svc.files().list(q=q, pageSize=1, fields="files(id,name)").execute()
        files = res.get("files",[])
        return files[0]["id"] if files else None
    except Exception as e:
        print("[Drive] ê²€ìƒ‰ ì‹¤íŒ¨:", e); return None

def drive_download_text(svc, file_id: str) -> Optional[str]:
    try:
        req = svc.files().get_media(fileId=file_id)
        buf = io.BytesIO()
        dl = svc._Download(buf, req)
        done = False
        while not done:
            _, done = dl.next_chunk()
        buf.seek(0); return buf.read().decode("utf-8")
    except Exception as e:
        print("[Drive] ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨:", e); return None

# ===== ì „ì¼ ë¹„êµ / ì¸&ì•„ì›ƒ =====
def parse_prev_csv(text: str) -> List[Dict]:
    out = []
    rd = csv.DictReader(io.StringIO(text))
    for r in rd:
        code = (r.get("product_code") or "").strip()
        if not code: continue
        try:
            out.append({
                "rank": int(r.get("rank") or 0),
                "raw_name": r.get("raw_name") or "",
                "url": r.get("url") or "",
                "product_code": code
            })
        except Exception:
            continue
    return out

def analyze_trends(today: List[Dict], prev: Optional[List[Dict]], topN: int = 200):
    if not prev:
        for t in today: t["prev_rank"]=None; t["is_new"]=False
        return today, [], [], [], 0

    prev_top = {p["product_code"]: p["rank"] for p in prev if p.get("rank",9999) <= topN}
    prev_set = set(prev_top.keys())

    for t in today:
        code = t.get("product_code")
        pr = prev_top.get(code)
        t["prev_rank"] = pr
        t["is_new"] = pr is None

    movers = [t for t in today if t["prev_rank"] is not None]
    ups   = sorted([m for m in movers if m["prev_rank"] > m["rank"]],
                   key=lambda x: (x["prev_rank"]-x["rank"]), reverse=True)
    downs = sorted([m for m in movers if m["prev_rank"] < m["rank"]],
                   key=lambda x: (x["rank"]-x["prev_rank"]), reverse=True)

    today_set = {t["product_code"] for t in today[:topN] if t.get("product_code")}
    ins_set  = today_set - prev_set
    outs_set = prev_set - today_set
    io_cnt = min(len(ins_set), len(outs_set))  # ìŒ ì¼ì¹˜ ê°•ì œ

    chart_ins = [t for t in today if (t["product_code"] in ins_set and t["rank"]<=topN)]
    rank_outs = [p for p in prev if (p["product_code"] in outs_set and p["rank"]<=topN)]

    return today, ups, downs, chart_ins, rank_outs, io_cnt

# ===== Slack =====
def _link(name: str, url: Optional[str]) -> str:
    return f"<{url}|{name}>" if (name and url) else (name or (url or ""))

def post_slack(today: List[Dict], ups, downs, chart_ins, rank_outs, io_cnt: int):
    if not SLACK_WEBHOOK: return
    lines = []
    lines.append(f"*ë‹¤ì´ì†Œëª° ë·°í‹°/ìœ„ìƒ ì¼ê°„ ë­í‚¹ 200* ({kst_now().strftime('%Y-%m-%d %H:%M KST')})")

    lines.append("\n*TOP 10*")
    for it in today[:10]:
        cur = it["rank"]; name=it["raw_name"]; url=it["url"]; price=it["price"]
        pr  = it.get("prev_rank")
        if pr is None: marker = "(new)"
        else:
            diff = pr - cur
            marker = f"(â†‘{diff})" if diff>0 else (f"(â†“{abs(diff)})" if diff<0 else "(-)")
        lines.append(f"{cur}. {marker} {_link(name,url)} â€” {price:,}ì›")

    lines.append("\n*ğŸ”¥ ê¸‰ìƒìŠ¹*")
    if ups:
        for m in ups[:5]:
            lines.append(f"- {_link(m['raw_name'], m['url'])} {m['prev_rank']}ìœ„ â†’ {m['rank']}ìœ„ (â†‘{m['prev_rank']-m['rank']})")
    else:
        lines.append("- (í•´ë‹¹ ì—†ìŒ)")

    lines.append("\n*ğŸ†• ë‰´ë­ì»¤*")
    if chart_ins:
        for x in chart_ins[:5]:
            lines.append(f"- {_link(x['raw_name'], x['url'])} NEW â†’ {x['rank']}ìœ„")
    else:
        lines.append("- (í•´ë‹¹ ì—†ìŒ)")

    lines.append("\n*ğŸ“‰ ê¸‰í•˜ë½*")
    if downs:
        for m in downs[:5]:
            lines.append(f"- {_link(m['raw_name'], m['url'])} {m['prev_rank']}ìœ„ â†’ {m['rank']}ìœ„ (â†“{m['rank']-m['prev_rank']})")
    else:
        lines.append("- (ê¸‰í•˜ë½ ì—†ìŒ)")

    if rank_outs:
        rank_outs_sorted = sorted(rank_outs, key=lambda x: int(x.get("rank") or 9999))
        for ro in rank_outs_sorted[:5]:
            lines.append(f"- {_link(ro.get('raw_name') or '', ro.get('url'))} {ro.get('rank')}ìœ„ â†’ OUT")
    else:
        lines.append("- (OUT ì—†ìŒ)")

    lines.append("\n:left_right_arrow: **ë­í¬ ì¸&ì•„ì›ƒ**")
    if io_cnt==0:
        lines.append("ë³€ë™ ì—†ìŒ.")
    else:
        lines.append(f"**{io_cnt}ê°œì˜ ì œí’ˆì´ ì¸&ì•„ì›ƒ ë˜ì—ˆìŠµë‹ˆë‹¤.**")

    try:
        requests.post(SLACK_WEBHOOK, json={"text":"\n".join(lines)}, timeout=12).raise_for_status()
        print("[Slack] ì „ì†¡ ì„±ê³µ")
    except Exception as e:
        print("[Slack] ì „ì†¡ ì‹¤íŒ¨:", e)

# ===== main =====
def main():
    t0 = time.time()
    ensure_dirs()

    rows = collect_top200()  # 0ê°œë©´ ì—¬ê¸°ì„œ ì˜ˆì™¸ë¡œ ì¤‘ë‹¨
    path, fname = save_csv(rows)

    # Drive ì—…ë¡œë“œ + ì „ì¼ íŒŒì¼ ë¡œë“œ
    svc = build_drive_service()
    prev_list: Optional[List[Dict]] = None
    if svc:
        drive_upload(svc, path, fname)
        yname = f"ë‹¤ì´ì†Œëª°_ë·°í‹°ìœ„ìƒ_ì¼ê°„_{yday_str()}.csv"
        fid = drive_find_csv(svc, yname)
        if fid:
            txt = drive_download_text(svc, fid)
            if txt:
                prev_list = parse_prev_csv(txt)
                print(f"[ë¶„ì„] ì „ì¼ {len(prev_list)}ê±´")

    # ë¹„êµ/ë¦¬í¬íŠ¸
    today_aug, ups, downs, chart_ins, rank_outs, io_cnt = analyze_trends(rows, prev_list, topN=200)
    post_slack(today_aug, ups, downs, chart_ins, rank_outs, io_cnt)

    print(f"ì´ {len(rows)}ê±´, ê²½ê³¼ ì‹œê°„: {time.time()-t0:.1f}s")

if __name__ == "__main__":
    main()
