# app.py  â€” DaisoMall ë­í‚¹(ë·°í‹°/ìœ„ìƒ Â· ì¼ê°„) ìˆ˜ì§‘ê¸°
# - Playwrightë¡œ 'ë·°í‹°/ìœ„ìƒ' ì¹´í…Œê³ ë¦¬ì™€ 'ì¼ê°„' íƒ­ì„ ê°•ì œ ì„ íƒ
# - ìŠ¤í¬ë¡¤ë¡œ ìµœëŒ€ 200ìœ„ê¹Œì§€ ë¡œë“œ(ë¶€í•˜ì‹œ ìë™ ì¡°ê¸°ì¢…ë£Œ)
# - ìƒí’ˆëª… ì• 'BEST' ì œê±°, ë¹ˆ ì¹´ë“œ/ê´‘ê³  ì œê±°, ì¤‘ë³µ URL ì œê±°
# - ê²°ê³¼ CSV ì €ì¥ + (ì„ íƒ) êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì—…ë¡œë“œ + ìŠ¬ë™ ì•Œë¦¼
# --------------------------------------------------------------

import os, re, csv, time, json, math
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Optional
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright, TimeoutError as PWTimeout

# ========= ì„¤ì • =========
RANK_URL = "https://www.daisomall.co.kr/ds/rank/C105"
MAX_ITEMS = int(os.getenv("MAX_ITEMS", "200"))  # 100~200 ê¶Œì¥
SCROLL_PAUSE = 0.6
SCROLL_STABLE_ROUNDS = 3        # ìƒˆ ì¹´ë“œê°€ ë” ì´ìƒ ì•ˆ ëŠ˜ë©´ ë©ˆì¶¤
SCROLL_MAX_ROUNDS = 60

SLACK_WEBHOOK = os.getenv("SLACK_WEBHOOK_URL", "")
# êµ¬ê¸€ ë“œë¼ì´ë¸Œ(ì„ íƒ)
GDRIVE_FOLDER_ID = os.getenv("GDRIVE_FOLDER_ID", "")
GOOGLE_CLIENT_ID = os.getenv("GOOGLE_CLIENT_ID", "")
GOOGLE_CLIENT_SECRET = os.getenv("GOOGLE_CLIENT_SECRET", "")
GOOGLE_REFRESH_TOKEN = os.getenv("GOOGLE_REFRESH_TOKEN", "")

KST = timezone(timedelta(hours=9))

# ========= ê³µí†µ ìœ í‹¸ =========
def today_str() -> str:
    return datetime.now(KST).strftime("%Y-%m-%d")

def yday_str() -> str:
    return (datetime.now(KST) - timedelta(days=1)).strftime("%Y-%m-%d")

def num(s: str) -> Optional[int]:
    s = re.sub(r"[^\d]", "", s or "")
    return int(s) if s else None

def clean_name(name: str) -> str:
    if not name: return ""
    # 'BEST' ì ‘ë‘/ì¤‘ê°„ í‘œê¸° ì œê±°
    name = re.sub(r"^\s*BEST\s*[|:\-\u00A0]*", "", name, flags=re.I)
    name = re.sub(r"\s*\bBEST\b\s*", " ", name, flags=re.I)
    # ë‹¤ì¤‘ ê³µë°± ì •ë¦¬
    name = re.sub(r"\s+", " ", name).strip()
    return name

# ========= Playwright ìˆ˜ì§‘ =========
def fetch_html() -> str:
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        ctx = browser.new_context(
            user_agent=("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                        "AppleWebKit/537.36 (KHTML, like Gecko) "
                        "Chrome/123.0.0.0 Safari/537.36")
        )
        page = ctx.new_page()
        page.goto(RANK_URL, wait_until="domcontentloaded", timeout=45_000)

        # ì¹´í…Œê³ ë¦¬ ë²„íŠ¼: value="CTGR_00014" (ë·°í‹°/ìœ„ìƒ)
        try:
            page.wait_for_selector(".prod-category .cate-btn", timeout=15_000)
        except PWTimeout:
            pass

        # ë·°í‹°/ìœ„ìƒ ë²„íŠ¼ í™•ë³´ (value ìš°ì„ , ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ë¡œ)
        cate = page.locator('.prod-category .cate-btn[value="CTGR_00014"]')
        if cate.count() == 0:
            cate = page.get_by_role("button", name=re.compile("ë·°í‹°/?ìœ„ìƒ"))

        try:
            cate.first.scroll_into_view_if_needed()
            cate.first.click()
        except Exception:
            # ê°€ë” ìŠ¤ì™€ì´í¼ ë·° ë°–ì´ë©´ ê°•ì œ í´ë¦­
            page.evaluate("""
                (sel) => {
                  const el = document.querySelector(sel);
                  if (el) el.click();
                }
            """, '.prod-category .cate-btn[value="CTGR_00014"]')

        page.wait_for_load_state("networkidle")

        # ì •ë ¬ íƒ­: 'ì¼ê°„' (ë¼ë””ì˜¤ value="2")
        # ë²„íŠ¼ ë ˆì´ë¸”ì´ê±°ë‚˜ ë¼ë””ì˜¤ ëª¨ë‘ ëŒ€ì‘
        daily = page.locator('.ipt-sorting input[value="2"]')
        if daily.count() == 0:
            daily = page.get_by_role("button", name=re.compile("ì¼ê°„"))
        try:
            daily.first.scroll_into_view_if_needed()
            daily.first.click()
        except Exception:
            page.evaluate("""
                () => {
                  const byVal = document.querySelector('.ipt-sorting input[value="2"]');
                  if (byVal) { byVal.click(); return; }
                  const btns = [...document.querySelectorAll('.ipt-sorting *')];
                  const t = btns.find(b => /ì¼ê°„/.test(b.textContent||""));
                  if (t) t.click();
                }
            """)
        page.wait_for_load_state("networkidle")

        # ì²« ì¹´ë“œê°€ ë³´ì¼ ë•Œê¹Œì§€ ëŒ€ê¸°
        page.wait_for_selector(".goods-list .goods-unit", timeout=20_000)

        # ë¬´í•œ ìŠ¤í¬ë¡¤
        prev = 0
        stable = 0
        for _ in range(SCROLL_MAX_ROUNDS):
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            page.wait_for_load_state("networkidle")
            time.sleep(SCROLL_PAUSE)
            cnt = page.locator(".goods-list .goods-unit").count()
            if cnt >= MAX_ITEMS:
                break
            if cnt == prev:
                stable += 1
                if stable >= SCROLL_STABLE_ROUNDS:
                    break
            else:
                stable = 0
                prev = cnt

        html = page.content()
        ctx.close()
        browser.close()
        return html

# ========= íŒŒì„œ =========
def parse(html: str) -> List[Dict]:
    soup = BeautifulSoup(html, "html.parser")
    units = soup.select(".goods-list .goods-unit")
    items: List[Dict] = []
    seen_urls = set()

    for u in units:
        # ìƒí’ˆëª…
        name_el = u.select_one(".goods-detail .tit a") or u.select_one(".goods-detail .tit")
        if not name_el: 
            continue
        name = clean_name(name_el.get_text(strip=True))
        if not name:
            continue

        # ê°€ê²©
        price_el = u.select_one(".goods-detail .goods-price .value")
        price = num(price_el.get_text()) if price_el else None
        if not price:
            # ê°€ê²© ì—†ëŠ” ì¹´ë“œ(ê´‘ê³ /í’ˆì ˆ)ëŠ” ì œì™¸
            continue

        # URL
        link_el = u.select_one(".goods-detail .tit a") or u.select_one(".goods-detail .goods-link")
        url = None
        if link_el and link_el.has_attr("href"):
            href = link_el["href"]
            url = urljoin(RANK_URL, href)
        if not url or url in seen_urls:
            # URL ì—†ê±°ë‚˜ ì¤‘ë³µ(ê°™ì€ ìƒí’ˆ í˜ì´ì§€) ì œì™¸
            continue

        seen_urls.add(url)
        items.append({
            "name": name,
            "price": price,
            "url": url,
        })

        if len(items) >= MAX_ITEMS:
            break

    # ìˆœìœ„ ì¬ë¶€ì—¬(ë¹ˆì¹´ë“œ/ê´‘ê³  ì œê±°ë¡œ ìƒê¸´ êµ¬ë© ë°©ì§€)
    for i, it in enumerate(items, start=1):
        it["rank"] = i
    return items

# ========= CSV =========
def save_csv(rows: List[Dict]) -> str:
    date_str = today_str()
    fname = f"ë‹¤ì´ì†Œëª°_ë·°í‹°ìœ„ìƒ_ì¼ê°„_{date_str}.csv"
    os.makedirs("data", exist_ok=True)
    path = os.path.join("data", fname)
    with open(path, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["date", "rank", "name", "price", "url"])
        for r in rows:
            w.writerow([date_str, r["rank"], r["name"], r["price"], r["url"]])
    return path

# ========= ë³€í™” ê°ì§€(ê¸‰ìƒìŠ¹/ë‰´ë­ì»¤/ê¸‰í•˜ë½/ì¸&ì•„ì›ƒ) =========
def load_prev() -> Dict[str, int]:
    """ì–´ì œ CSVë¥¼ ì—´ì–´ url->rank ë§¤í•‘(ì—†ìœ¼ë©´ ë¹ˆê°’)."""
    y = yday_str()
    p = os.path.join("data", f"ë‹¤ì´ì†Œëª°_ë·°í‹°ìœ„ìƒ_ì¼ê°„_{y}.csv")
    if not os.path.exists(p):
        return {}
    ranks = {}
    with open(p, newline="", encoding="utf-8") as f:
        for i, row in enumerate(csv.DictReader(f)):
            ranks[row["url"]] = int(row["rank"])
    return ranks

def diff_sections(cur: List[Dict], prev_map: Dict[str, int]):
    ups, downs, new, out = [], [], [], []
    cur_urls = [x["url"] for x in cur]
    for it in cur:
        url, r = it["url"], it["rank"]
        pr = prev_map.get(url)
        if pr is None:
            if r <= 20:  # ìƒìœ„ê¶Œ ì‹ ê·œë§Œ ë…¸ì¶œ
                new.append((r, it["name"]))
        else:
            delta = pr - r
            if delta >= 20:
                ups.append((r, it["name"], f"{pr}â†’{r}"))
            elif delta <= -20:
                downs.append((r, it["name"], f"{pr}â†’{r}"))
    for url, pr in prev_map.items():
        if url not in cur_urls and pr <= 20:
            out.append(pr)
    return ups[:5], new[:5], downs[:5], out

# ========= Slack =========
def post_slack(cur: List[Dict]):
    if not SLACK_WEBHOOK:
        return
    top10 = cur[:10]
    prev_map = load_prev()
    ups, new, downs, out = diff_sections(cur, prev_map)

    lines = []
    lines.append(f"*ë‹¤ì´ì†Œëª° ë·°í‹°/ìœ„ìƒ ì¼ê°„ ë­í‚¹ â€” {today_str()}*")
    lines.append("")
    lines.append("*TOP 10*")
    for it in top10:
        lines.append(f"{it['rank']}. <{it['url']}|{it['name']}> â€” {it['price']:,}ì›")
    lines.append("")
    # ê¸‰ìƒìŠ¹
    lines.append("ğŸ”¥ *ê¸‰ìƒìŠ¹*")
    if ups:
        for r, name, movement in ups:
            lines.append(f"- {name}  ({movement})")
    else:
        lines.append("- í•´ë‹¹ ì—†ìŒ")
    # ë‰´ë­ì»¤
    lines.append("\nğŸ†• *ë‰´ë­ì»¤*")
    if new:
        for r, name in new:
            lines.append(f"- {name}  NEW â†’ {r}ìœ„")
    else:
        lines.append("- í•´ë‹¹ ì—†ìŒ")
    # ê¸‰í•˜ë½
    lines.append("\nğŸ“‰ *ê¸‰í•˜ë½*")
    if downs:
        for r, name, movement in downs:
            lines.append(f"- {name}  ({movement})")
    else:
        lines.append("- í•´ë‹¹ ì—†ìŒ")
    # ë§í¬ ì¸&ì•„ì›ƒ
    lines.append("\nğŸ”— *ë§í¬ ì¸&ì•„ì›ƒ*")
    if out:
        lines.append(f"{len(out)}ê°œì˜ ì œí’ˆì´ ì´íƒˆ/ìœ ì…ì´ ìˆì—ˆìŠµë‹ˆë‹¤.")
    else:
        lines.append("0ê°œì˜ ì œí’ˆì´ ì¸&ì•„ì›ƒ ë˜ì—ˆìŠµë‹ˆë‹¤.")

    msg = {"text": "\n".join(lines)}
    try:
        requests.post(SLACK_WEBHOOK, json=msg, timeout=10).raise_for_status()
    except Exception as e:
        print("[Slack] ì „ì†¡ ì‹¤íŒ¨:", e)

# ========= Google Drive(ì„ íƒ) =========
def upload_to_drive(path: str) -> Optional[str]:
    if not (GDRIVE_FOLDER_ID and GOOGLE_CLIENT_ID and GOOGLE_CLIENT_SECRET and GOOGLE_REFRESH_TOKEN):
        return None
    try:
        # í† í°
        token_res = requests.post(
            "https://oauth2.googleapis.com/token",
            data={
                "client_id": GOOGLE_CLIENT_ID,
                "client_secret": GOOGLE_CLIENT_SECRET,
                "refresh_token": GOOGLE_REFRESH_TOKEN,
                "grant_type": "refresh_token",
            },
            timeout=15,
        )
        token_res.raise_for_status()
        access_token = token_res.json()["access_token"]

        meta = {"name": os.path.basename(path), "parents": [GDRIVE_FOLDER_ID]}
        files = {
            "metadata": ("metadata", json.dumps(meta), "application/json"),
            "file": (os.path.basename(path), open(path, "rb"), "text/csv"),
        }
        up = requests.post(
            "https://www.googleapis.com/upload/drive/v3/files?uploadType=multipart",
            headers={"Authorization": f"Bearer {access_token}"},
            files=files,
            timeout=60,
        )
        up.raise_for_status()
        file_id = up.json().get("id")
        return file_id
    except Exception as e:
        print("[Drive] ì—…ë¡œë“œ ì‹¤íŒ¨:", e)
        return None

# ========= ë©”ì¸ =========
def main():
    print("ìˆ˜ì§‘ ì‹œì‘:", RANK_URL)
    t0 = time.time()
    html = fetch_html()
    cards = parse(html)
    print("[ìˆ˜ì§‘ ì™„ë£Œ] ê°œìˆ˜:", len(cards))

    if len(cards) < 20:
        raise RuntimeError("ìœ íš¨ ìƒí’ˆ ì¹´ë“œê°€ ë„ˆë¬´ ì ê²Œ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì…€ë ‰í„°/ë Œë”ë§ ì ê²€ í•„ìš”")

    csv_path = save_csv(cards)
    file_id = upload_to_drive(csv_path)
    post_slack(cards)

    print("ë¡œì»¬ ì €ì¥:", csv_path)
    if file_id:
        print("Drive ì—…ë¡œë“œ ì™„ë£Œ:", file_id)
    print("ì´", len(cards), "ê±´, ê²½ê³¼:", f"{time.time()-t0:.1f}s")

if __name__ == "__main__":
    main()
