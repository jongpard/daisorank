# app.py â€” DaisoMall ë·°í‹°/ìœ„ìƒ 'ì¼ê°„' ë­í‚¹ ìˆ˜ì§‘ (ê¸°ëŠ¥ ê°•í™”íŒ)
# - GDrive ì—°ë™: ì „ì¼ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ë¶„ì„ ê¸°ëŠ¥ ì¶”ê°€
# - ìˆœìœ„ ë³€ë™ ë¶„ì„: ê¸‰ìƒìŠ¹, ë‰´ë­ì»¤, ê¸‰í•˜ë½, ë­í¬ì•„ì›ƒ
# - Slack í¬ë§· ê°œì„ : ì˜¬ë¦¬ë¸Œì˜ ë²„ì „ê³¼ ë™ì¼í•œ ë¦¬í¬íŠ¸ í˜•ì‹ ì ìš©
# - ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€: Playwright ê¸°ë°˜ í¬ë¡¤ë§, CSV ì €ì¥ ë° ì—…ë¡œë“œ

import os, re, csv, time, json, io
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Optional, Union

import requests
from playwright.sync_api import sync_playwright, TimeoutError as PWTimeout, Page, Locator

# Google Drive (OAuth)
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseUpload, MediaIoBaseDownload
from google.oauth2.credentials import Credentials as UserCredentials
from google.auth.transport.requests import Request as GoogleRequest

# ====== ì„¤ì • ======
SLACK_WEBHOOK = os.getenv("SLACK_WEBHOOK_URL", "").strip()
GOOGLE_CLIENT_ID = os.getenv("GOOGLE_CLIENT_ID", "")
GOOGLE_CLIENT_SECRET = os.getenv("GOOGLE_CLIENT_SECRET", "")
GOOGLE_REFRESH_TOKEN = os.getenv("GOOGLE_REFRESH_TOKEN", "")
GDRIVE_FOLDER_ID = os.getenv("GDRIVE_FOLDER_ID", "")

RANK_URL = "https://www.daisomall.co.kr/ds/rank/C105"
MAX_ITEMS = int(os.getenv("MAX_ITEMS", "200"))
TOP_WINDOW = 30  # ë‰´ë­ì»¤, ë­í¬ì•„ì›ƒ ë“±ì„ íŒë‹¨í•˜ëŠ” ê¸°ì¤€ ìˆœìœ„

# ---- Slack ë¶„ì„/í‘œì‹œ ê·œì¹™(ë‹¤ì´ì†Œ ì „ìš©) ----
DS_TOTAL_RANGE = 200             # ë¹„êµ ë²”ìœ„: Top200
DS_OUT_LIMIT = 150               # OUT ê¸°ì¤€: ì „ì¼ 150ìœ„ ì´ë‚´ë§Œ
DS_RISING_FALLING_THRESHOLD = 10 # ê¸‰ìƒìŠ¹/ê¸‰í•˜ë½ ìµœì†Œ ë³€ë™ ê³„ë‹¨
DS_TOP_MOVERS_MAX = 5            # ê¸‰ìƒìŠ¹/ê¸‰í•˜ë½ ìµœëŒ€ ë…¸ì¶œ ê°œìˆ˜
DS_NEWCOMERS_TOP = 30            # ë‰´ë­ì»¤ ì§„ì… í•œê³„(Top30)

KST = timezone(timedelta(hours=9))


# ====== ìœ í‹¸ ======
def today_str() -> str:
    return datetime.now(KST).strftime("%Y-%m-%d")

def yday_str() -> str:
    return (datetime.now(KST) - timedelta(days=1)).strftime("%Y-%m-%d")

def ensure_int(v) -> Optional[int]:
    try:
        return int(v)
    except Exception:
        return None

def get_text(node) -> str:
    if node is None:
        return ""
    return (node.text_content() if hasattr(node, "text_content") else node.inner_text()).strip()

def clean_spaces(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())

def parse_price_kr(s: str) -> Optional[int]:
    if not s: return None
    m = re.search(r"(\d[\d,]*)", s)
    if not m: return None
    try:
        return int(m.group(1).replace(",", ""))
    except Exception:
        return None

def csv_safe(s: Optional[str]) -> str:
    if s is None: return ""
    s = str(s)
    if any(c in s for c in [",", "\n", '"']):
        s = '"' + s.replace('"', '""') + '"'
    return s


# ====== Playwright ë³´ì¡° ======
def _to_locator(page: Page, target: Union[str, Locator]) -> Locator:
    return target if isinstance(target, Locator) else page.locator(target)


def close_overlays(page: Page):
    candidates = [
        ".layer-popup .btn-close", ".modal .btn-close", ".popup .btn-close",
        ".layer-popup .close", ".modal .close", ".popup .close",
        ".btn-x", ".btn-close, button[aria-label='ë‹«ê¸°']"
    ]
    for sel in candidates:
        try:
            if page.locator(sel).first.is_visible():
                page.locator(sel).first.click(timeout=500)
        except Exception:
            pass


def click_hard(page: Page, target: Union[str, Locator], name=""):
    try:
        l = _to_locator(page, target)
        l.click(timeout=3000)
        time.sleep(0.2)
        return True
    except Exception:
        try:
            _ = page.evaluate("""
                (sel) => {
                    const el = (typeof sel === 'string') ? document.querySelector(sel) : sel;
                    if (el) { el.click(); return true; }
                    return false;
                }
            """, target if isinstance(target, str) else None)
            time.sleep(0.2)
            return True
        except Exception:
            pass
    return False


# ====== í¬ë¡¤ë§ ======
def fetch_products() -> List[Dict]:
    out: List[Dict] = []
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True, args=["--no-sandbox", "--disable-dev-shm-usage"])
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36",
            locale="ko-KR"
        )
        page = context.new_page()
        page.set_default_timeout(30000)

        # ì§„ì…
        page.goto(RANK_URL, wait_until="domcontentloaded")
        try:
            page.wait_for_load_state("networkidle", timeout=15000)
        except Exception:
            pass

        close_overlays(page)

        # ì¹´í…Œê³ ë¦¬ ì„ íƒ: ë·°í‹°/ìœ„ìƒ
        try:
            if page.locator('.prod-category .cate-btn[value="CTGR_00014"]').first.is_visible():
                click_hard(page, '.prod-category .cate-btn[value="CTGR_00014"]', "ë·°í‹°/ìœ„ìƒ(value)")
            else:
                click_hard(page, page.get_by_role("button", name=re.compile("ë·°í‹°\\/?ìœ„ìƒ")), "ë·°í‹°/ìœ„ìƒ(text)")
        except Exception:
            page.evaluate("""
                () => {
                  const byVal = document.querySelector('.prod-category .cate-btn[value="CTGR_00014"]');
                  if (byVal) byVal.click();
                  else {
                    const btns = [...document.querySelectorAll('.prod-category .cate-btn, .prod-category *')];
                    const t = btns.find(b => /ë·°í‹°\\/?ìœ„ìƒ/.test((b.textContent||"").trim()));
                    if (t) t.click();
                  }
                }
            """)

        # ì¼ê°„ íƒ­
        try:
            click_hard(page, page.get_by_role("tab", name=re.compile("ì¼ê°„|Day")), "ì¼ê°„ íƒ­")
        except Exception:
            pass

        # ë¬´í•œìŠ¤í¬ë¡¤/í˜ì´ì§€ ì „í™˜: Top200 í™•ë³´
        last_count = 0
        for _ in range(40):
            page.mouse.wheel(0, 4000)
            time.sleep(0.4)
            cur = page.locator(".prod-list .prod-item, .rank-list .rank-item, .prod-area .prod-item").count()
            if cur >= 200:
                break
            if cur == last_count:
                try:
                    click_hard(page, page.get_by_role("button", name=re.compile("ë”ë³´ê¸°|ë” ë³´ê¸°|More")), "ë”ë³´ê¸°")
                except Exception:
                    page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    time.sleep(0.2)
            last_count = cur

        cards = page.locator(".prod-list .prod-item, .rank-list .rank-item, .prod-area .prod-item")
        n = min(cards.count(), 200)
        for i in range(n):
            el = cards.nth(i)

            rank = ensure_int(get_text(el.locator(".rank, .num, .rank-num").first)) or (i + 1)
            a = el.locator("a").first
            url = a.get_attribute("href") if a else None
            if url and url.startswith("/"):
                url = "https://www.daisomall.co.kr" + url

            name = clean_spaces(get_text(el.locator(".tit, .name, .prod-name, .product-name").first))
            if not name:
                name = clean_spaces(get_text(a)) or f"ìƒí’ˆ {rank}"

            price = None
            pnode = el.locator(".price, .sale, .cur, .prod-price .num, .price .num").first
            price = parse_price_kr(get_text(pnode))

            out.append({
                "rank": rank,
                "name": name,
                "url": url,
                "price": price
            })

        browser.close()
    # ë­í¬ ì±„ìš°ê¸°/ì •ë ¬
    out = sorted(out, key=lambda x: int(x["rank"]))
    if len(out) > MAX_ITEMS:
        out = out[:MAX_ITEMS]
    return out


# ====== CSV ì €ì¥ ======
def save_csv(rows: List[Dict], path: str):
    header = ["rank", "name", "url", "price"]
    with open(path, "w", encoding="utf-8", newline="") as f:
        w = csv.writer(f)
        w.writerow(header)
        for r in rows:
            w.writerow([r.get("rank"), r.get("name"), r.get("url"), r.get("price")])


# ====== GDrive OAuth ======
def drive_service_oauth():
    if not (GOOGLE_CLIENT_ID and GOOGLE_CLIENT_SECRET and GOOGLE_REFRESH_TOKEN):
        print("[Drive] OAuth í™˜ê²½ë³€ìˆ˜ ë¯¸ì„¤ì •")
        return None
    try:
        creds = UserCredentials(
            None,
            refresh_token=GOOGLE_REFRESH_TOKEN,
            client_id=GOOGLE_CLIENT_ID,
            client_secret=GOOGLE_CLIENT_SECRET,
            token_uri="https://oauth2.googleapis.com/token",
            scopes=["https://www.googleapis.com/auth/drive.file"],
        )
        creds.refresh(GoogleRequest())
        return build("drive", "v3", credentials=creds, cache_discovery=False)
    except Exception as e:
        print("[Drive] OAuth ì„¸ì…˜ ìƒì„± ì‹¤íŒ¨:", e)
        return None

def upload_to_drive(service, filepath: str, filename: str) -> Optional[str]:
    if not service or not GDRIVE_FOLDER_ID:
        return None
    try:
        media = MediaIoBaseUpload(io.FileIO(filepath, 'rb'), mimetype="text/csv", resumable=True)
        body = {"name": filename, "parents": [GDRIVE_FOLDER_ID]}
        file = service.files().create(body=body, media_body=media, fields="id,name").execute()
        print(f"[Drive] ì—…ë¡œë“œ ì„±ê³µ: {file.get('name')} (ID: {file.get('id')})")
        return file.get("id")
    except Exception as e:
        print("[Drive] ì—…ë¡œë“œ ì‹¤íŒ¨:", e)
        return None

def find_file_in_drive(service, filename: str):
    if not service or not GDRIVE_FOLDER_ID:
        return None
    try:
        q = f"name='{filename}' and '{GDRIVE_FOLDER_ID}' in parents and mimeType='text/csv' and trashed=false"
        res = service.files().list(q=q, pageSize=1, fields="files(id,name)").execute()
        return res.get("files", [])[0] if res.get("files") else None
    except Exception as e:
        print(f"[Drive] íŒŒì¼ ê²€ìƒ‰ ì‹¤íŒ¨ ({filename}):", e)
        return None

def download_from_drive(service, file_id: str) -> Optional[str]:
    try:
        request = service.files().get_media(fileId=file_id)
        fh = io.BytesIO()
        downloader = MediaIoBaseDownload(fh, request)
        done = False
        while not done:
            _, done = downloader.next_chunk()
        fh.seek(0)
        return fh.read().decode("utf-8")
    except Exception as e:
        print(f"[Drive] íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (ID: {file_id}):", e)
        return None


# ====== ì „ì¼ CSV íŒŒì‹± ======
def parse_prev_csv(csv_text: str) -> List[Dict]:
    items: List[Dict] = []
    try:
        rdr = csv.DictReader(io.StringIO(csv_text))
        for row in rdr:
            try:
                items.append({
                    "rank": int(row.get("rank") or 0),
                    "name": row.get("name"),
                    "url": row.get("url"),
                    "price": int(row.get("price") or 0) if row.get("price") else None,
                })
            except (ValueError, TypeError):
                continue
    except Exception as e:
        print("[CSV Parse] ì „ì¼ ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨:", e)
    return items


# ====== ë¶„ì„(ë‹¤ì´ì†Œ ì „ìš© ê·œì¹™ ì ìš©) ======
def analyze_trends(today_items: List[Dict], prev_items: List[Dict]):
    """URL ê¸°ì¤€ ë§¤ì¹­.
    - ê¸‰ìƒìŠ¹/ê¸‰í•˜ë½: Top200 ì „ì²´ ëŒ€ìƒ, ë³€ë™ ì ˆëŒ€ê°’ >= DS_RISING_FALLING_THRESHOLD
    - ë‰´ë­ì»¤: Top30 ì‹ ê·œ ì§„ì…
    - OUT: ì „ì¼ 150ìœ„ ì´ë‚´ì˜€ê³  ì˜¤ëŠ˜ ëª©ë¡ì— ì—†ìŒ
    """
    # prev: url -> rank
    prev_map: Dict[str, int] = {}
    for p in (prev_items or []):
        u = p.get("url")
        r = p.get("rank")
        if not u:
            continue
        try:
            r = int(r)
        except Exception:
            continue
        if 1 <= r <= DS_TOTAL_RANGE:
            prev_map[u] = r

    # ì „ì¼ OUT ê¸°ì¤€ ì§‘í•©(<=150ìœ„)
    prev_out_set = {u for u, r in prev_map.items() if r <= DS_OUT_LIMIT}

    # ì˜¤ëŠ˜ íŠ¸ë Œë“œ ëª©ë¡
    trends = []
    for it in (today_items or []):
        u = it.get("url")
        if not u:
            continue
        try:
            cr = int(it.get("rank") or 0)
        except Exception:
            continue
        pr = prev_map.get(u)
        trends.append({
            "name": it.get("name"),
            "url": u,
            "rank": cr,
            "prev_rank": pr,
            "change": (pr - cr) if pr else None,  # +: ìƒìŠ¹, -: í•˜ë½
        })

    # ìƒìŠ¹/í•˜ë½ í•„í„° ë° ì •ë ¬
    movers = [t for t in trends if t.get("prev_rank") is not None]
    ups = [t for t in movers if (t.get("change") or 0) >= DS_RISING_FALLING_THRESHOLD]
    downs = [t for t in movers if (t.get("change") or 0) <= -DS_RISING_FALLING_THRESHOLD]
    ups.sort(key=lambda x: (-(x.get("change") or 0), x.get("rank") or 9999, x.get("prev_rank") or 9999, x.get("name") or ""))
    downs.sort(key=lambda x: (abs(x.get("change") or 0), x.get("rank") or 9999, x.get("prev_rank") or 9999, x.get("name") or ""))

    # ë‰´ë­ì»¤: Top30 ì‹ ê·œ ì§„ì…
    chart_ins = [t for t in trends if t.get("prev_rank") is None and (t.get("rank") or 9999) <= DS_NEWCOMERS_TOP]
    chart_ins.sort(key=lambda x: x.get("rank") or 9999)

    # OUT: ì „ì¼ <=150ìœ„ì˜€ê³  ì˜¤ëŠ˜ ì—†ìŒ
    today_urls = {t.get("url") for t in trends}
    rank_out_urls = prev_out_set - today_urls
    rank_outs = [{ "name": p.get("name"), "url": p.get("url"), "rank": p.get("rank") } 
                 for p in (prev_items or []) if p.get("url") in rank_out_urls]
    rank_outs.sort(key=lambda x: int(x.get("rank") or 9999))

    in_out_count = len(chart_ins) + len(rank_outs)

    return ups, downs, chart_ins, rank_outs, in_out_count


# ====== Slack (ë‹¤ì´ì†Œ ì „ìš© í¬ë§·) ======
def post_slack(rows: List[Dict], analysis_results, prev_items: List[Dict]):
    if not SLACK_WEBHOOK:
        return

    ups, downs, chart_ins, rank_outs, in_out_count = analysis_results

    # prev map for TOP10 badge
    prev_rank_map: Dict[str, int] = {}
    for p in (prev_items or []):
        u = p.get("url")
        try:
            r = int(p.get("rank") or 0)
        except Exception:
            continue
        if u and 1 <= r <= DS_TOTAL_RANGE:
            prev_rank_map[u] = r

    now_kst = datetime.now(KST)
    title = f"*ë‹¤ì´ì†Œ ë°ì¼ë¦¬ ì „ì²´ ë­í‚¹ {DS_TOTAL_RANGE}* ({now_kst.strftime('%Y-%m-%d %H:%M KST')})"
    lines = [title, "\n*TOP 10*"]

    # TOP10 with badge
    for it in (rows or [])[:10]:
        cur = int(it.get("rank") or 0)
        url = it.get("url")
        prev = prev_rank_map.get(url)
        if prev is None:
            badge = "(new)"
        elif prev > cur:
            badge = f"(â†‘{prev - cur})"
        elif prev < cur:
            badge = f"(â†“{cur - prev})"
        else:
            badge = "(-)"
        price = it.get("price")
        price_txt = f"{int(price):,}ì›" if isinstance(price, (int, float)) else (f"{price:,}ì›" if isinstance(price, str) and price.isdigit() else str(price or ""))
        name_link = f"<{url}|{it.get('name') or ''}>" if url else (it.get('name') or '')
        lines.append(f"{cur}. {badge} {name_link} â€” {price_txt}")

    # ğŸ”¥ ê¸‰ìƒìŠ¹
    lines.append("\n*ğŸ”¥ ê¸‰ìƒìŠ¹*")
    if ups:
        for m in ups[:DS_TOP_MOVERS_MAX]:
            lines.append(f"- <{m['url']}|{m['name']}> {m['prev_rank']}ìœ„ â†’ {m['rank']}ìœ„ (â†‘{m['change']})")
    else:
        lines.append("- (í•´ë‹¹ ì—†ìŒ)")

    # ğŸ†• ë‰´ë­ì»¤
    lines.append("\n*ğŸ†• ë‰´ë­ì»¤*")
    if chart_ins:
        for t in chart_ins[:3]:
            lines.append(f"- <{t['url']}|{t['name']}> NEW â†’ {t['rank']}ìœ„")
    else:
        lines.append("- (í•´ë‹¹ ì—†ìŒ)")

    # ğŸ“‰ ê¸‰í•˜ë½ (ìš°ì„  ê¸‰í•˜ë½, ë¶€ì¡±í•˜ë©´ OUTë¡œ ë³´ê°•)
    lines.append("\n*ğŸ“‰ ê¸‰í•˜ë½*")
    shown = 0
    if downs:
        for m in downs[:DS_TOP_MOVERS_MAX]:
            lines.append(f"- <{m['url']}|{m['name']}> {m['prev_rank']}ìœ„ â†’ {m['rank']}ìœ„ (â†“{abs(m['change'])})")
            shown += 1
    if shown < DS_TOP_MOVERS_MAX and rank_outs:
        for ro in rank_outs[:DS_TOP_MOVERS_MAX - shown]:
            lines.append(f"- {ro['name']} {int(ro['rank'])}ìœ„ â†’ OUT")

    # â†” ì¸&ì•„ì›ƒ
    lines.append(f"\n*â†” ë­í¬ ì¸&ì•„ì›ƒ*")
    lines.append(f"{in_out_count}ê°œì˜ ì œí’ˆì´ ì¸&ì•„ì›ƒ ë˜ì—ˆìŠµë‹ˆë‹¤.")

    try:
        requests.post(SLACK_WEBHOOK, json={"text": "\n".join(lines)}, timeout=10).raise_for_status()
        print("[Slack] ì „ì†¡ ì„±ê³µ")
    except Exception as e:
        print("[Slack] ì „ì†¡ ì‹¤íŒ¨:", e)


# ====== main ======
def main():
    print("ìˆ˜ì§‘ ì‹œì‘:", RANK_URL)
    t0 = time.time()
    rows = fetch_products()
    print(f"[ìˆ˜ì§‘ ì™„ë£Œ] ê°œìˆ˜: {len(rows)}")

    if len(rows) < 1:
        print("[ì˜¤ë¥˜] ìˆ˜ì§‘ ì‹¤íŒ¨")
        return

    # CSV ì €ì¥
    csv_dir = "rankings"
    os.makedirs(csv_dir, exist_ok=True)
    csv_filename = f"ë‹¤ì´ì†Œëª°_ë·°í‹°ìœ„ìƒ_ì¼ê°„_{today_str()}.csv"
    csv_path = os.path.join(csv_dir, csv_filename)
    save_csv(rows, csv_path)
    print("[CSV] ì €ì¥ ì™„ë£Œ:", csv_path)

    # ë“œë¼ì´ë¸Œ ì—…ë¡œë“œ/ì „ì¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    prev_items: List[Dict] = []
    drive_service = drive_service_oauth()
    if drive_service:
        # ì˜¤ëŠ˜ ë°ì´í„° ì—…ë¡œë“œ
        upload_to_drive(drive_service, csv_path, csv_filename)

        # ì–´ì œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ë¶„ì„
        yday_filename = f"ë‹¤ì´ì†Œëª°_ë·°í‹°ìœ„ìƒ_ì¼ê°„_{yday_str()}.csv"
        prev_file = find_file_in_drive(drive_service, yday_filename)
        prev_items = []
        if prev_file:
            print(f"[Drive] ì „ì¼ íŒŒì¼ ë°œê²¬: {prev_file['name']} (ID: {prev_file['id']})")
            csv_content = download_from_drive(drive_service, prev_file['id'])
            if csv_content:
                prev_items = parse_prev_csv(csv_content)
                print(f"[ë¶„ì„] ì „ì¼ ë°ì´í„° {len(prev_items)}ê±´ ë¡œë“œ ì™„ë£Œ")
        else:
            print(f"[Drive] ì „ì¼ íŒŒì¼({yday_filename})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        analysis_results = analyze_trends(rows, prev_items)
    else:
        # ë“œë¼ì´ë¸Œ ì—°ë™ ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¶„ì„ ê²°ê³¼ë¡œ ì „ë‹¬
        analysis_results = ([], [], [], [], 0)

    # ìŠ¬ë™ ì•Œë¦¼
    post_slack(rows, analysis_results, prev_items)

    print(f"ì´ {len(rows)}ê±´, ê²½ê³¼ ì‹œê°„: {time.time()-t0:.1f}s")


if __name__ == "__main__":
    main()
