# app.py â€” ë‹¤ì´ì†Œëª° ë·°í‹°/ìœ„ìƒ 'ì¼ê°„' ëž­í‚¹ ìˆ˜ì§‘/ë¶„ì„
# - ì¹´í…Œê³ ë¦¬/ì¼ê°„ ê°•ì œ ê²€ì¦ (ë¼ë””ì˜¤/ë¼ë²¨/ë²„íŠ¼/JS ìµœí›„ìˆ˜ë‹¨)
# - Top200 ìŠ¤í¬ë¡¤ í™•ë¡œë“œ(End/scrollTo/wheel/ë”ë³´ê¸°)
# - ê²¬ê³ í•œ ì¶”ì¶œ(ì œí’ˆëª…/ê°€ê²©/ë§í¬), pdNo ê¸°ì¤€ ë¹„êµ
# - Slack í¬ë§·: ì¸&ì•„ì›ƒ 1ì¤„ ê³ ì •

import os, re, csv, io, time
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Optional, Tuple

import requests
from playwright.sync_api import sync_playwright, Page

# Google Drive (OAuth)
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseUpload, MediaIoBaseDownload
from google.oauth2.credentials import Credentials as UserCredentials
from google.auth.transport.requests import Request as GoogleRequest

# ===== ì„¤ì • =====
RANK_URL = os.getenv("RANK_URL", "https://www.daisomall.co.kr/ds/rank/C105")
TOPN = int(os.getenv("TOPN", "200"))
SCROLL_MAX_ROUNDS = int(os.getenv("SCROLL_MAX_ROUNDS", "260"))
SCROLL_PAUSE_MS = int(os.getenv("SCROLL_PAUSE_MS", "750"))

SLACK_WEBHOOK = os.getenv("SLACK_WEBHOOK_URL", "")

# Drive OAuth
GDRIVE_FOLDER_ID = os.getenv("GDRIVE_FOLDER_ID", "")
GOOGLE_CLIENT_ID = os.getenv("GOOGLE_CLIENT_ID", "")
GOOGLE_CLIENT_SECRET = os.getenv("GOOGLE_CLIENT_SECRET", "")
GOOGLE_REFRESH_TOKEN = os.getenv("GOOGLE_REFRESH_TOKEN", "")

KST = timezone(timedelta(hours=9))
def today_str(): return datetime.now(KST).strftime("%Y-%m-%d")
def yday_str():  return (datetime.now(KST) - timedelta(days=1)).strftime("%Y-%m-%d")

def strip_best(s: str) -> str:
    if not s: return ""
    s = re.sub(r"^\s*BEST\s*[\|\-:\u00A0]*", "", s, flags=re.I)
    s = re.sub(r"\s*\bBEST\b\s*", " ", s, flags=re.I)
    return re.sub(r"\s+", " ", s).strip()

def extract_pdno(url: str) -> Optional[str]:
    if not url: return None
    m = re.search(r"[?&]pdNo=(\d+)", url)
    if m: return m.group(1)
    m = re.search(r"/pd/(?:pdr|detail)/(\d+)", url)
    if m: return m.group(1)
    return None

# ===== ê³µí†µ =====
def _count_cards(page: Page) -> int:
    try:
        return page.evaluate("""
          () => document.querySelectorAll(
            'a[href*="pdNo="], a[href*="/pd/pdr/"], a[href*="/product/detail/"]'
          ).length
        """)
    except Exception:
        return 0

def _scroll_to_chipbar(page: Page):
    try:
        page.evaluate("""
          () => {
            const bars = [...document.querySelectorAll('.prod-category, .chips, .tab, .category')];
            if (bars.length) bars[0].scrollIntoView({block:'center'});
          }
        """)
        page.wait_for_timeout(200)
    except Exception:
        pass

def _click_via_js(page: Page, text: str):
    page.evaluate("""
      (txt) => {
        const nodes = [...document.querySelectorAll('button, a, .cate-btn, .chip, .tab *, .category *')];
        const t = nodes.find(n => (n.textContent||'').trim().includes(txt));
        if (t) {
          t.scrollIntoView({block:'center'});
          t.dispatchEvent(new MouseEvent('mouseover', {bubbles:true}));
          t.dispatchEvent(new MouseEvent('mousedown', {bubbles:true}));
          t.click();
          t.dispatchEvent(new MouseEvent('mouseup', {bubbles:true}));
        }
      }
    """, text)

def _is_active_like(page: Page, el) -> bool:
    # pythonì—ì„œ ì•ˆì”€. js ì•ˆìœ¼ë¡œ ë„£ì–´ í˜¸ì¶œ
    return False

# ===== ì¹´í…Œê³ ë¦¬/ì¼ê°„ =====
def _beauty_chip_active(page: Page) -> bool:
    try:
        return page.evaluate("""
          () => {
            const all = [...document.querySelectorAll('.prod-category * , .category, .cate, .chips, .tab, .filter *')];
            const isActive = (el) => {
              const c = (el.className||'') + ' ' + (el.parentElement?.className||'');
              return /\\bis-active\\b|\\bon\\b|\\bactive\\b|\\bselected\\b/i.test(c)
                  || el.getAttribute('aria-selected')==='true'
                  || el.getAttribute('aria-pressed')==='true';
            };
            const t = all.find(n => /ë·°í‹°\\/?ìœ„ìƒ/.test((n.textContent||'').trim()));
            return !!(t && isActive(t));
          }
        """)
    except Exception:
        return False

def _period_is_daily(page: Page) -> bool:
    try:
        return page.evaluate("""
          () => {
            const dailyInput = document.querySelector('.ipt-sorting input[value="2"]');
            if (dailyInput && (dailyInput.checked || dailyInput.getAttribute('checked')==='true')) return true;

            const nodes = [...document.querySelectorAll('*')];
            const isActive = (el) => {
              const c = (el.className||'') + ' ' + (el.parentElement?.className||'');
              return /\\bis-active\\b|\\bon\\b|\\bactive\\b|\\bselected\\b/i.test(c)
                  || el.getAttribute('aria-selected')==='true'
                  || el.getAttribute('aria-pressed')==='true';
            };
            const candid = nodes.filter(n => /ì¼ê°„/.test((n.textContent||'').trim()));
            return candid.some(isActive);
          }
        """)
    except Exception:
        return False

def _click_beauty_chip(page: Page) -> bool:
    _scroll_to_chipbar(page)
    before = _count_cards(page)
    candidates = [
        '.prod-category .cate-btn[value="CTGR_00014"]',
        "button:has-text('ë·°í‹°/ìœ„ìƒ')",
        "a:has-text('ë·°í‹°/ìœ„ìƒ')",
        "text=ë·°í‹°/ìœ„ìƒ",
    ]
    for attempt in range(8):
        clicked = False
        for sel in candidates:
            try:
                if sel == "text=ë·°í‹°/ìœ„ìƒ":
                    page.get_by_text("ë·°í‹°/ìœ„ìƒ", exact=False).first.scroll_into_view_if_needed()
                    page.get_by_text("ë·°í‹°/ìœ„ìƒ", exact=False).first.click(timeout=900)
                    clicked = True; break
                loc = page.locator(sel)
                if loc.count() > 0:
                    loc.first.scroll_into_view_if_needed()
                    page.wait_for_timeout(120)
                    loc.first.hover(timeout=600)
                    loc.first.click(timeout=900)
                    clicked = True; break
            except Exception:
                continue
        if not clicked:
            _click_via_js(page, "ë·°í‹°/ìœ„ìƒ")
        page.wait_for_timeout(450)
        if _beauty_chip_active(page):
            return True
        if _count_cards(page) != before:
            return True
    return _beauty_chip_active(page)

def _click_daily(page: Page) -> bool:
    # ìµœëŒ€ë¡œ ì§‘ìš”í•˜ê²Œ 'ì¼ê°„'ì„ í™œì„±í™”
    for attempt in range(10):
        try:
            page.evaluate("window.scrollTo(0,0)")
            page.wait_for_timeout(200)
        except Exception:
            pass

        try:
            # 1) ë¼ë””ì˜¤ ì§ì ‘ í´ë¦­
            inp = page.locator('.ipt-sorting input[value="2"]')
            if inp.count() > 0:
                inp.first.click(timeout=800)
        except Exception:
            pass

        try:
            # 2) ë¼ë²¨ í´ë¦­
            page.evaluate("""
              () => {
                const inp = document.querySelector('.ipt-sorting input[value="2"]');
                if (inp && inp.id) {
                  const lb = document.querySelector('label[for="'+inp.id+'"]');
                  if (lb) lb.click();
                }
              }
            """)
        except Exception:
            pass

        # 3) ë²„íŠ¼/íƒ­ í…ìŠ¤íŠ¸
        try:
            page.get_by_role("button", name=re.compile("ì¼ê°„")).first.click(timeout=800)
        except Exception:
            try:
                _click_via_js(page, "ì¼ê°„")
            except Exception:
                pass

        # 4) ìµœí›„ìˆ˜ë‹¨: JSë¡œ checked + ì´ë²¤íŠ¸
        page.evaluate("""
          () => {
            const inp = document.querySelector('.ipt-sorting input[value="2"]');
            if (inp) {
              inp.checked = true;
              inp.setAttribute('checked','true');
              ['input','change','click'].forEach(ev => inp.dispatchEvent(new Event(ev, {bubbles:true})));
              const form = inp.closest('form');
              if (form) form.dispatchEvent(new Event('change', {bubbles:true}));
            }
          }
        """)

        page.wait_for_timeout(400)
        # ë„¤íŠ¸ì›Œí¬ ì•ˆì • ëŒ€ê¸°
        try:
            page.wait_for_load_state("networkidle", timeout=1200)
        except Exception:
            pass

        if _period_is_daily(page):
            return True

    return _period_is_daily(page)

# ===== ìŠ¤í¬ë¡¤ ë¡œë”© =====
def _try_more_button(page: Page) -> bool:
    try:
        btn = page.locator("button:has-text('ë”ë³´ê¸°'), a:has-text('ë”ë³´ê¸°')")
        if btn.count() > 0:
            btn.first.scroll_into_view_if_needed()
            btn.first.click(timeout=800)
            page.wait_for_timeout(400)
            return True
    except Exception:
        pass
    return False

def _load_all(page: Page, want: int):
    prev = 0; stable = 0
    for _ in range(SCROLL_MAX_ROUNDS):
        acted = _try_more_button(page)
        if not acted:
            page.keyboard.press("End")
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            try: page.mouse.wheel(0, 16000)
            except Exception: pass
        page.wait_for_timeout(SCROLL_PAUSE_MS)
        cnt = _count_cards(page)
        if cnt >= want: break
        if cnt == prev:
            stable += 1
            if stable >= 10: break
        else:
            stable = 0; prev = cnt
    # ë¶€ì¡±í•˜ë©´ 2ë¼ìš´ë“œ ë”
    for _ in range(2):
        if _count_cards(page) >= want: break
        for __ in range(6):
            page.keyboard.press("End")
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            page.wait_for_timeout(int(SCROLL_PAUSE_MS*0.8))
        _try_more_button(page)
        page.wait_for_timeout(600)

# ===== ì¶”ì¶œ =====
def _extract_items(page: Page) -> List[Dict]:
    data = page.evaluate("""
      () => {
        const anchors = [...document.querySelectorAll('a[href*="pdNo="], a[href*="/pd/pdr/"], a[href*="/product/detail/"]')];
        const seenCard = new Set();
        const rows = [];

        const cleanName = (s) => {
          if (!s) return '';
          s = s.trim();
          s = s.replace(/(íƒë°°ë°°ì†¡|ì˜¤ëŠ˜ë°°ì†¡|ë§¤ìž¥í”½ì—…|ë³„ì \\s*\\d+[.,\\d]*ì |\\d+[.,\\d]*\\s*ê±´\\s*ìž‘ì„±).*$/g, '').trim();
          return s;
        };

        const cardOf = (a) => a.closest('.product-info, .goods-unit, .goods-item, .goods-unit-v2, li') || a.parentElement;

        for (const a of anchors) {
          const card = cardOf(a);
          if (!card || seenCard.has(card)) continue;
          seenCard.add(card);

          const nameEl = card.querySelector('.goods-detail .tit a, .goods-detail .tit, .tit a, .tit, .goods-name, .name') || a;
          let name = cleanName(nameEl?.textContent || '');
          if (!name) continue;

          const priceEl = card.querySelector('.goods-detail .goods-price .value, .price .num, .sale-price .num, .sale .price, .goods-price .num, .price');
          let priceText = (priceEl?.textContent || '').replace(/[^0-9]/g, '');
          let price = parseInt(priceText || '0', 10);
          if (!price || price <= 0) {
            const t = (card.textContent || '').replace(/\\s+/g, ' ');
            const matches = [...t.matchAll(/([0-9][0-9,]{2,})\\s*ì›/g)];
            if (matches.length) {
              const last = matches[matches.length - 1][1];
              price = parseInt(last.replace(/,/g, ''), 10);
            }
          }
          if (!price || price <= 0) continue;

          let href = a.href || a.getAttribute('href') || '';
          if (!/^https?:/i.test(href)) href = new URL(href, location.origin).href;

          rows.push({ name, price, url: href });
        }
        return rows;
      }
    """)

    out = []
    seen_pd = set()
    for it in data:
        pd = extract_pdno(it.get("url","") or "")
        if not pd: continue
        if pd in seen_pd: continue
        seen_pd.add(pd)

        nm = strip_best(it["name"])
        if not nm: continue

        out.append({
            "pdNo": pd,
            "name": nm,
            "price": int(it["price"]),
            "url": it["url"],
        })

    for i, r in enumerate(out, 1):
        r["rank"] = i
    return out[:TOPN]

# ===== ìˆ˜ì§‘ =====
def fetch_products() -> List[Dict]:
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True, args=["--no-sandbox","--disable-dev-shm-usage"])
        ctx = browser.new_context(
            viewport={"width": 1380, "height": 940},
            user_agent=("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"),
        )
        page = ctx.new_page()
        page.goto(RANK_URL, wait_until="domcontentloaded", timeout=60_000)

        if not _click_beauty_chip(page):
            print("[ê²½ê³ ] ë·°í‹°/ìœ„ìƒ ì¹© í™œì„±í™” ê²€ì¦ ì‹¤íŒ¨")
        if not _click_daily(page):
            print("[ê²½ê³ ] 'ì¼ê°„' í™œì„±í™” ê²€ì¦ ì‹¤íŒ¨")

        _load_all(page, TOPN)

        os.makedirs("data/debug", exist_ok=True)
        with open(f"data/debug/rank_raw_{today_str()}.html", "w", encoding="utf-8") as f:
            f.write(page.content())

        rows = _extract_items(page)

        ctx.close(); browser.close()
        return rows

# ===== CSV =====
def save_csv(rows: List[Dict]):
    date = today_str()
    os.makedirs("data", exist_ok=True)
    name = f"ë‹¤ì´ì†Œëª°_ë·°í‹°ìœ„ìƒ_ì¼ê°„_{date}.csv"
    path = os.path.join("data", name)
    with open(path, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f); w.writerow(["date","pdNo","rank","name","price","url"])
        for r in rows[:TOPN]:
            w.writerow([date, r["pdNo"], r["rank"], r["name"], r["price"], r["url"]])
    return path, name

# ===== Drive =====
def build_drive_service():
    if not (GOOGLE_CLIENT_ID and GOOGLE_CLIENT_SECRET and GOOGLE_REFRESH_TOKEN):
        print("[Drive] OAuth env ë¯¸ì„¤ì • â†’ ë¹„í™œì„±í™”"); return None
    try:
        creds = UserCredentials(
            None, refresh_token=GOOGLE_REFRESH_TOKEN,
            client_id=GOOGLE_CLIENT_ID, client_secret=GOOGLE_CLIENT_SECRET,
            token_uri="https://oauth2.googleapis.com/token",
            scopes=["https://www.googleapis.com/auth/drive.file"]
        )
        creds.refresh(GoogleRequest())
        return build("drive","v3",credentials=creds,cache_discovery=False)
    except Exception as e:
        print("[Drive] ì„œë¹„ìŠ¤ ìƒì„± ì‹¤íŒ¨:", e); return None

def upload_to_drive(svc, path: str, name: str):
    if not svc or not GDRIVE_FOLDER_ID: return None
    try:
        media = MediaIoBaseUpload(io.FileIO(path,'rb'), mimetype="text/csv", resumable=True)
        meta  = {"name": name, "parents":[GDRIVE_FOLDER_ID]}
        f = svc.files().create(body=meta, media_body=media, fields="id,name").execute()
        print(f"[Drive] ì—…ë¡œë“œ ì„±ê³µ: {f.get('name')} (ID: {f.get('id')})"); return f.get("id")
    except Exception as e:
        print("[Drive] ì—…ë¡œë“œ ì‹¤íŒ¨:", e); return None

def find_file_in_drive(svc, name: str):
    if not svc or not GDRIVE_FOLDER_ID: return None
    try:
        q = f"name='{name}' and '{GDRIVE_FOLDER_ID}' in parents and mimeType='text/csv' and trashed=false"
        r = svc.files().list(q=q, pageSize=1, fields="files(id,name)").execute()
        return r.get("files", [])[0] if r.get("files") else None
    except Exception as e:
        print(f"[Drive] íŒŒì¼ ê²€ìƒ‰ ì‹¤íŒ¨({name}):", e); return None

def download_from_drive(svc, file_id: str) -> Optional[str]:
    try:
        req = svc.files().get_media(fileId=file_id)
        buf = io.BytesIO(); dl = MediaIoBaseDownload(buf, req); done=False
        while not done: _, done = dl.next_chunk()
        buf.seek(0); return buf.read().decode("utf-8")
    except Exception as e:
        print(f"[Drive] íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨(ID:{file_id}):", e); return None

# ===== ë¹„êµ/ë¶„ì„ (pdNo) =====
def parse_prev_csv(txt: str) -> List[Dict]:
    items=[]; rdr=csv.DictReader(io.StringIO(txt))
    for row in rdr:
        try:
            pdno = row.get("pdNo") or extract_pdno(row.get("url","") or "")
            if not pdno: continue
            items.append({"pdNo": pdno, "rank": int(row.get("rank")), "name": row.get("name"), "url": row.get("url")})
        except Exception: continue
    return items

def analyze_trends(today: List[Dict], prev: List[Dict]):
    prev_map = {p["pdNo"]: p["rank"] for p in prev}
    ups, downs = [], []
    for t in today[:TOPN]:
        pd = t["pdNo"]; tr=t["rank"]; pr=prev_map.get(pd)
        if pr is None: continue
        ch = pr - tr
        d = {"pdNo":pd,"name":t["name"],"url":t["url"],"rank":tr,"prev_rank":pr,"change":ch}
        if ch>0: ups.append(d)
        elif ch<0: downs.append(d)
    ups.sort(key=lambda x:(-x["change"], x["rank"]))
    downs.sort(key=lambda x:(x["change"], x["rank"]))

    today_keys = {t["pdNo"] for t in today[:TOPN]}
    prev_keys  = {p["pdNo"] for p in prev if 1 <= p["rank"] <= TOPN}
    chart_ins = [t for t in today if t["pdNo"] in (today_keys - prev_keys)]
    rank_outs = [p for p in prev  if p["pdNo"] in (prev_keys - today_keys)]
    io_cnt = len(today_keys.symmetric_difference(prev_keys)) // 2
    chart_ins.sort(key=lambda r:r["rank"])
    rank_outs.sort(key=lambda r:r["rank"])
    return ups, downs, chart_ins, rank_outs, io_cnt

# ===== Slack =====
def post_slack(rows: List[Dict], analysis, prev_items: Optional[List[Dict]] = None):
    if not SLACK_WEBHOOK: return
    ups, downs, chart_ins, rank_outs, io_cnt = analysis
    prev_map = {p["pdNo"]: p["rank"] for p in (prev_items or [])}
    def _link(n,u): return f"<{u}|{n}>" if u else (n or "")

    now = datetime.now(KST)
    lines = [f"*ë‹¤ì´ì†Œëª° ë·°í‹°/ìœ„ìƒ ì¼ê°„ ëž­í‚¹ {TOPN}* ({now.strftime('%Y-%m-%d %H:%M KST')})"]

    lines.append("\n*TOP 10*")
    for it in rows[:10]:
        cur=it["rank"]; price=f"{int(it['price']):,}ì›"
        pr=prev_map.get(it["pdNo"])
        marker="(new)" if pr is None else (f"(â†‘{pr-cur})" if pr>cur else (f"(â†“{cur-pr})" if pr<cur else "(-)"))
        lines.append(f"{cur}. {marker} {_link(it['name'], it['url'])} â€” {price}")

    lines.append("\n*ðŸ”¥ ê¸‰ìƒìŠ¹*")
    if ups:
        for m in ups[:5]:
            lines.append(f"- {_link(m['name'], m['url'])} {m['prev_rank']}ìœ„ â†’ {m['rank']}ìœ„ (â†‘{m['change']})")
    else: lines.append("- (í•´ë‹¹ ì—†ìŒ)")

    lines.append("\n*ðŸ†• ë‰´ëž­ì»¤*")
    if chart_ins:
        for t in chart_ins[:5]:
            lines.append(f"- {_link(t['name'], t['url'])} NEW â†’ {t['rank']}ìœ„")
    else: lines.append("- (í•´ë‹¹ ì—†ìŒ)")

    lines.append("\n*ðŸ“‰ ê¸‰í•˜ë½*")
    if downs:
        ds = sorted(downs, key=lambda x:(-abs(x["change"]), x["rank"]))
        for m in ds[:5]:
            lines.append(f"- {_link(m['name'], m['url'])} {m['prev_rank']}ìœ„ â†’ {m['rank']}ìœ„ (â†“{abs(m['change'])})")
    else: lines.append("- (ê¸‰í•˜ë½ ì—†ìŒ)")
    if rank_outs:
        os_ = sorted(rank_outs, key=lambda x:x["rank"])
        for ro in os_[:5]:
            lines.append(f"- {_link(ro.get('name'), ro.get('url'))} {int(ro.get('rank') or 0)}ìœ„ â†’ OUT")
    else: lines.append("- (OUT ì—†ìŒ)")

    lines.append("\n*â†” ëž­í¬ ì¸&ì•„ì›ƒ*")
    lines.append(f"{io_cnt}ê°œì˜ ì œí’ˆì´ ì¸&ì•„ì›ƒ ë˜ì—ˆìŠµë‹ˆë‹¤.")

    try:
        requests.post(SLACK_WEBHOOK, json={"text":"\n".join(lines)}, timeout=12).raise_for_status()
        print("[Slack] ì „ì†¡ ì„±ê³µ")
    except Exception as e:
        print("[Slack] ì „ì†¡ ì‹¤íŒ¨:", e)

# ===== main =====
def main():
    print("ìˆ˜ì§‘ ì‹œìž‘:", RANK_URL)
    rows = fetch_products()
    print(f"[ìˆ˜ì§‘ ì™„ë£Œ] {len(rows)}ê°œ â†’ Top{TOPN}ë¡œ ì‚¬ìš©")
    rows = rows[:TOPN]

    csv_path, csv_name = save_csv(rows)
    print("ë¡œì»¬ ì €ìž¥:", csv_path)

    prev_items: List[Dict] = []
    svc = build_drive_service()
    if svc:
        upload_to_drive(svc, csv_path, csv_name)
        yname = f"ë‹¤ì´ì†Œëª°_ë·°í‹°ìœ„ìƒ_ì¼ê°„_{yday_str()}.csv"
        prev = find_file_in_drive(svc, yname)
        if prev:
            txt = download_from_drive(svc, prev["id"])
            if txt: prev_items = parse_prev_csv(txt)

    analysis = analyze_trends(rows, prev_items)
    post_slack(rows, analysis, prev_items)

if __name__ == "__main__":
    main()
