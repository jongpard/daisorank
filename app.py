# app.py ‚Äî DaisoMall Î∑∞Ìã∞/ÏúÑÏÉù 'ÏùºÍ∞Ñ' Îû≠ÌÇπ ÏàòÏßë (Í∏∞Îä• Í∞ïÌôîÌåê)
# - GDrive Ïó∞Îèô: Ï†ÑÏùº Îç∞Ïù¥ÌÑ∞ Îã§Ïö¥Î°úÎìú Î∞è Î∂ÑÏÑù Í∏∞Îä• Ï∂îÍ∞Ä
# - ÏàúÏúÑ Î≥ÄÎèô Î∂ÑÏÑù: Í∏âÏÉÅÏäπ, Îâ¥Îû≠Ïª§, Í∏âÌïòÎùΩ, Îû≠ÌÅ¨ÏïÑÏõÉ
# - Slack Ìè¨Îß∑ Í∞úÏÑ†: Ïò¨Î¶¨Î∏åÏòÅ Î≤ÑÏ†ÑÍ≥º ÎèôÏùºÌïú Î¶¨Ìè¨Ìä∏ ÌòïÏãù Ï†ÅÏö©
# - Í∏∞Ï°¥ Í∏∞Îä• Ïú†ÏßÄ: Playwright Í∏∞Î∞ò ÌÅ¨Î°§ÎßÅ, CSV Ï†ÄÏû• Î∞è ÏóÖÎ°úÎìú

import os, re, csv, time, json, io
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Optional, Union

import requests
from playwright.sync_api import sync_playwright, TimeoutError as PWTimeout, Page, Locator

# Google Drive (OAuth)
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseUpload, MediaIoBaseDownload
from google.oauth2.credentials import Credentials as UserCredentials
from google.auth.transport.requests import Request as GoogleRequest

# ====== ÏÑ§Ï†ï ======
RANK_URL = "https://www.daisomall.co.kr/ds/rank/C105"
MAX_ITEMS = int(os.getenv("MAX_ITEMS", "200"))
TOP_WINDOW = 150  # Îâ¥Îû≠Ïª§, Îû≠ÌÅ¨ÏïÑÏõÉ Îì±ÏùÑ ÌåêÎã®ÌïòÎäî Í∏∞Ï§Ä ÏàúÏúÑ
DS_RISING_FALLING_THRESHOLD = 10
DS_TOP_MOVERS_MAX = 5
DS_NEWCOMERS_TOP = 30

SCROLL_PAUSE = 0.6
SCROLL_STABLE_ROUNDS = 4
SCROLL_MAX_ROUNDS = 80

SLACK_WEBHOOK = os.getenv("SLACK_WEBHOOK_URL", "")
# Google Drive (OAuth ÏÇ¨Ïö©Ïûê Í≥ÑÏ†ï Ï†ïÎ≥¥)
GDRIVE_FOLDER_ID = os.getenv("GDRIVE_FOLDER_ID", "")
GOOGLE_CLIENT_ID = os.getenv("GOOGLE_CLIENT_ID", "")
GOOGLE_CLIENT_SECRET = os.getenv("GOOGLE_CLIENT_SECRET", "")
GOOGLE_REFRESH_TOKEN = os.getenv("GOOGLE_REFRESH_TOKEN", "")

KST = timezone(timedelta(hours=9))


# ====== Ïú†Ìã∏ ======
def today_str() -> str:
    return datetime.now(KST).strftime("%Y-%m-%d")

def yday_str() -> str:
    return (datetime.now(KST) - timedelta(days=1)).strftime("%Y-%m-%d")

def strip_best(name: str) -> str:
    if not name:
        return ""
    name = re.sub(r"^\s*BEST\s*[\|\-:\u00A0]*", "", name, flags=re.I)
    name = re.sub(r"\s*\bBEST\b\s*", " ", name, flags=re.I)
    return re.sub(r"\s+", " ", name).strip()


def _to_locator(page: Page, target: Union[str, Locator]) -> Locator:
    return target if isinstance(target, Locator) else page.locator(target)


def close_overlays(page: Page):
    candidates = [
        ".layer-popup .btn-close", ".modal .btn-close", ".popup .btn-close",
        ".layer-popup .close", ".modal .close", ".popup .close",
        ".btn-x", ".btn-close, button[aria-label='Îã´Í∏∞']"
    ]
    for sel in candidates:
        try:
            if page.locator(sel).count() > 0:
                page.locator(sel).first.click(timeout=1000)
                page.wait_for_timeout(200)
        except Exception:
            pass


def click_hard(page: Page, target: Union[str, Locator], name_for_log: str = ""):
    loc = _to_locator(page, target)
    try:
        loc.first.wait_for(state="attached", timeout=3000)
    except Exception:
        raise RuntimeError(f"[click_hard] ÎåÄÏÉÅ ÎØ∏Ï°¥Ïû¨: {name_for_log}")
    try:
        loc.first.click(timeout=1200)
        return
    except Exception: pass
    try:
        loc.first.scroll_into_view_if_needed(timeout=1000)
        page.wait_for_timeout(150)
        loc.first.click(timeout=1200)
        return
    except Exception: pass
    try:
        loc.first.evaluate("(el) => { el.click(); }")
        return
    except Exception: pass
    raise RuntimeError(f"[click_hard] ÌÅ¥Î¶≠ Ïã§Ìå®: {name_for_log}")


# ====== Playwright (Ïπ¥ÌÖåÍ≥†Î¶¨/Ï†ïÎ†¨ Í≥†Ï†ï + Ïä§ÌÅ¨Î°§ + Ï∂îÏ∂ú) ======
def select_beauty_daily(page: Page):
    close_overlays(page)
    try:
        if page.locator('.prod-category .cate-btn[value="CTGR_00014"]').count() > 0:
            click_hard(page, '.prod-category .cate-btn[value="CTGR_00014"]', "Î∑∞Ìã∞/ÏúÑÏÉù(value)")
        else:
            click_hard(page, page.get_by_role("button", name=re.compile("Î∑∞Ìã∞\\/?ÏúÑÏÉù")), "Î∑∞Ìã∞/ÏúÑÏÉù(text)")
    except Exception:
        page.evaluate("""
            () => {
              const byVal = document.querySelector('.prod-category .cate-btn[value="CTGR_00014"]');
              if (byVal) byVal.click();
              else {
                const btns = [...document.querySelectorAll('.prod-category .cate-btn, .prod-category *')];
                const t = btns.find(b => /Î∑∞Ìã∞\\/?ÏúÑÏÉù/.test((b.textContent||"").trim()));
                if (t) t.click();
              }
            }
        """)
    page.wait_for_load_state("networkidle")
    page.wait_for_timeout(300)

    try:
        click_hard(page, '.ipt-sorting input[value="2"]', "ÏùºÍ∞Ñ(value)")
    except Exception:
        click_hard(page, page.get_by_role("button", name=re.compile("ÏùºÍ∞Ñ")), "ÏùºÍ∞Ñ(text)")
    page.wait_for_load_state("networkidle")
    page.wait_for_timeout(400)


def infinite_scroll(page: Page):
    prev = 0
    stable = 0
    for _ in range(SCROLL_MAX_ROUNDS):
        page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        page.wait_for_load_state("networkidle")
        page.wait_for_timeout(int(SCROLL_PAUSE * 1000))
        cnt = page.evaluate("""
            () => document.querySelectorAll('.goods-list .goods-unit, .goods-list .goods-item, .goods-list li.goods, .goods-unit-v2').length
        """)
        if cnt >= MAX_ITEMS:
            break
        if cnt == prev:
            stable += 1
            if stable >= SCROLL_STABLE_ROUNDS:
                break
        else:
            stable = 0
            prev = cnt


def collect_items(page: Page) -> List[Dict]:
    data = page.evaluate(
        """
        () => {
          const qs = sel => [...document.querySelectorAll(sel)];
          const units = qs('.goods-list .goods-unit, .goods-list .goods-item, .goods-list li.goods, .goods-unit-v2');
          const seen = new Set();
          const items = [];
          for (const el of units) {
            const nameEl = el.querySelector('.goods-detail .tit a, .goods-detail .tit, .tit a, .tit, .name, .goods-name');
            let name = (nameEl?.textContent || '').trim();
            if (!name) continue;
            const priceEl = el.querySelector('.goods-detail .goods-price .value, .price .num, .sale-price .num, .sale .price, .goods-price .num');
            let priceTxt = (priceEl?.textContent || '').replace(/[^0-9]/g, '');
            if (!priceTxt) continue;
            const price = parseInt(priceTxt, 10);
            if (!price || price <= 0) continue;
            let href = null;
            const a = el.querySelector('a[href*="/pd/pdr/"]');
            if (a && a.href) href = a.href;
            if (!href) continue;
            if (seen.has(href)) continue;
            seen.add(href);
            items.push({ name, price, url: href });
          }
          return items;
        }
        """
    )
    cleaned = []
    for it in data:
        nm = strip_best(it["name"])
        if not nm:
            continue
        cleaned.append({"name": nm, "price": it["price"], "url": it["url"]})
    for i, it in enumerate(cleaned, 1):
        it["rank"] = i
    return cleaned


def fetch_products() -> List[Dict]:
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            viewport={"width": 1360, "height": 900},
            user_agent=("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                        "AppleWebKit/537.36 (KHTML, like Gecko) "
                        "Chrome/123.0.0.0 Safari/537.36"),
        )
        page = context.new_page()
        page.goto(RANK_URL, wait_until="domcontentloaded", timeout=45_000)
        try:
            page.wait_for_selector(".prod-category", timeout=15_000)
        except PWTimeout:
            pass
        select_beauty_daily(page)
        try:
            page.wait_for_selector(".goods-list .goods-unit, .goods-list .goods-item, .goods-list li.goods, .goods-unit-v2", timeout=20_000)
        except PWTimeout:
            pass
        infinite_scroll(page)
        items = collect_items(page)
        context.close()
        browser.close()
        return items


# ====== CSV Ï†ÄÏû• ======
def save_csv(rows: List[Dict]) -> str:
    date_str = today_str()
    os.makedirs("data", exist_ok=True)
    filename = f"Îã§Ïù¥ÏÜåÎ™∞_Î∑∞Ìã∞ÏúÑÏÉù_ÏùºÍ∞Ñ_{date_str}.csv"
    path = os.path.join("data", filename)
    with open(path, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["date", "rank", "name", "price", "url"])
        for r in rows:
            w.writerow([date_str, r["rank"], r["name"], r["price"], r["url"]])
    return path, filename

# ====== Google Drive (Ïã†Í∑ú Ï∂îÍ∞Ä Î∞è ÏàòÏ†ï) ======
def build_drive_service():
    if not (GOOGLE_CLIENT_ID and GOOGLE_CLIENT_SECRET and GOOGLE_REFRESH_TOKEN):
        print("[Drive] OAuth ÌôòÍ≤ΩÎ≥ÄÏàò ÎØ∏ÏÑ§Ï†ï")
        return None
    try:
        creds = UserCredentials(
            None,
            refresh_token=GOOGLE_REFRESH_TOKEN,
            client_id=GOOGLE_CLIENT_ID,
            client_secret=GOOGLE_CLIENT_SECRET,
            token_uri="https://oauth2.googleapis.com/token",
            scopes=["https://www.googleapis.com/auth/drive.file"],
        )
        creds.refresh(GoogleRequest())
        return build("drive", "v3", credentials=creds, cache_discovery=False)
    except Exception as e:
        print("[Drive] ÏÑúÎπÑÏä§ ÏÉùÏÑ± Ïã§Ìå®:", e)
        return None

def upload_to_drive(service, filepath: str, filename: str):
    if not service or not GDRIVE_FOLDER_ID:
        print("[Drive] ÏÑúÎπÑÏä§ ÎòêÎäî Ìè¥Îçî IDÍ∞Ä ÏóÜÏñ¥ ÏóÖÎ°úÎìúÎ•º Í±¥ÎÑàÎúÅÎãàÎã§.")
        return None
    try:
        media = MediaIoBaseUpload(io.FileIO(filepath, 'rb'), mimetype="text/csv", resumable=True)
        body = {"name": filename, "parents": [GDRIVE_FOLDER_ID]}
        file = service.files().create(body=body, media_body=media, fields="id,name").execute()
        print(f"[Drive] ÏóÖÎ°úÎìú ÏÑ±Í≥µ: {file.get('name')} (ID: {file.get('id')})")
        return file.get("id")
    except Exception as e:
        print("[Drive] ÏóÖÎ°úÎìú Ïã§Ìå®:", e)
        return None

def find_file_in_drive(service, filename: str):
    if not service or not GDRIVE_FOLDER_ID:
        return None
    try:
        q = f"name='{filename}' and '{GDRIVE_FOLDER_ID}' in parents and mimeType='text/csv' and trashed=false"
        res = service.files().list(q=q, pageSize=1, fields="files(id,name)").execute()
        return res.get("files", [])[0] if res.get("files") else None
    except Exception as e:
        print(f"[Drive] ÌååÏùº Í≤ÄÏÉâ Ïã§Ìå® ({filename}):", e)
        return None

def download_from_drive(service, file_id: str) -> Optional[str]:
    try:
        request = service.files().get_media(fileId=file_id)
        fh = io.BytesIO()
        downloader = MediaIoBaseDownload(fh, request)
        done = False
        while not done:
            _, done = downloader.next_chunk()
        fh.seek(0)
        return fh.read().decode("utf-8")
    except Exception as e:
        print(f"[Drive] ÌååÏùº Îã§Ïö¥Î°úÎìú Ïã§Ìå® (ID: {file_id}):", e)
        return None


# ====== Î≥ÄÌôî Í∞êÏßÄ Î∞è Î∂ÑÏÑù (Ïã†Í∑ú) ======
def parse_prev_csv(csv_text: str) -> List[Dict]:
    items = []
    try:
        reader = csv.DictReader(io.StringIO(csv_text))
        for row in reader:
            try:
                items.append({
                    "rank": int(row.get("rank")),
                    "name": row.get("name"),
                    "url": row.get("url"),
                })
            except (ValueError, TypeError):
                continue
    except Exception as e:
        print("[CSV Parse] Ï†ÑÏùº Îç∞Ïù¥ÌÑ∞ ÌååÏã± Ïã§Ìå®:", e)
    return items

def analyze_trends(today_items: List[Dict], prev_items: List[Dict]):
    prev_map = {p["url"]: p["rank"] for p in prev_items if p.get("url")}
    prev_top_urls = {p["url"] for p in prev_items if p.get("url") and p.get("rank", 999) <= TOP_WINDOW}

    trends = []
    for it in today_items:
        url = it.get("url")
        if not url: continue
        prev_rank = prev_map.get(url)
        trends.append({
            "name": it["name"],
            "url": url,
            "rank": it["rank"],
            "prev_rank": prev_rank,
            "change": (prev_rank - it["rank"]) if prev_rank else None
        })

    movers = [t for t in trends if t["prev_rank"] is not None]
    ups = sorted([t for t in movers if t["change"] > 0], key=lambda x: x["change"], reverse=True)
    downs = sorted([t for t in movers if t["change"] < 0], key=lambda x: x["change"])

    chart_ins = [t for t in trends if t["prev_rank"] is None and t["rank"] <= TOP_WINDOW]
    
    today_urls = {t["url"] for t in trends}
    rank_out_urls = prev_top_urls - today_urls
    rank_outs = [p for p in prev_items if p.get("url") in rank_out_urls]

    in_out_count = len(chart_ins) + len(rank_outs)

    return ups, downs, chart_ins, rank_outs, in_out_count


def post_slack(rows: List[Dict], analysis_results, prev_items: List[Dict]):
    if not SLACK_WEBHOOK:
        return

    # Î∂ÑÏÑù Í≤∞Í≥º(Í∏∞Ï°¥ analyze_trends Ï∂úÎ†• Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©)
    ups, downs, chart_ins, rank_outs, in_out_count = analysis_results

    # Ï†ÑÏùº rank Îßµ (url Ïö∞ÏÑ†, ÏóÜÏúºÎ©¥ name), ÎπÑÍµê Î≤îÏúÑÎäî Top200
    TOTAL_RANGE = 200
    prev_rank_map: Dict[str, int] = {}
    for p in (prev_items or []):
        key = (p.get("url") or "").strip() or (p.get("name") or "").strip()
        if not key:
            continue
        try:
            r = int(p.get("rank") or 0)
            if 1 <= r <= TOTAL_RANGE:
                prev_rank_map[key] = r
        except Exception:
            pass

    def _key(it: dict) -> str:
        return (it.get("url") or "").strip() or (it.get("name") or "").strip()

    def _fmt_price(v) -> str:
        try:
            return f"{int(v):,}Ïõê"
        except Exception:
            return str(v or "")

    def _link(name: str, url: str | None) -> str:
        return f"<{url}|{name}>" if url else (name or "")

    # Î©îÏãúÏßÄ ÌÉÄÏù¥ÌãÄ
    now_kst = datetime.now(KST)
    title = f"*Îã§Ïù¥ÏÜåÎ™∞ Î∑∞Ìã∞/ÏúÑÏÉù ÏùºÍ∞Ñ Îû≠ÌÇπ 200* ({now_kst.strftime('%Y-%m-%d %H:%M KST')})"
    lines = [title]

    # TOP10 (Ï†ÑÏùº ÎåÄÎπÑ Î∞∞ÏßÄ)
    lines.append("\n*TOP 10*")
    for it in (rows or [])[:10]:
        cur = int(it.get("rank") or 0)
        k   = _key(it)
        prev= prev_rank_map.get(k)
        if prev is None:
            badge = "(new)"
        elif prev > cur:
            badge = f"(‚Üë{prev - cur})"
        elif prev < cur:
            badge = f"(‚Üì{cur - prev})"
        else:
            badge = "(-)"
        lines.append(f"{cur}. {badge} {_link(it.get('name') or '', it.get('url'))} ‚Äî {_fmt_price(it.get('price'))}")

    # üî• Í∏âÏÉÅÏäπ (¬±10 Ïù¥ÏÉÅ, 5Í∞ú)
    lines.append("\n*üî• Í∏âÏÉÅÏäπ*")
    if ups:
        for m in ups[:5]:
            lines.append(f"- {_link(m['name'], m.get('url'))} {m['prev_rank']}ÏúÑ ‚Üí {m['rank']}ÏúÑ (‚Üë{m['change']})")
    else:
        lines.append("- (Ìï¥Îãπ ÏóÜÏùå)")

    # üÜï Îâ¥Îû≠Ïª§ (ÏÉÅÏúÑ ÏßÑÏûÖ 5Í∞ú ÎÖ∏Ï∂ú)
    lines.append("\n*üÜï Îâ¥Îû≠Ïª§*")
    if chart_ins:
        for t in chart_ins[:5]:
            lines.append(f"- {_link(t['name'], t.get('url'))} NEW ‚Üí {t['rank']}ÏúÑ")
    else:
        lines.append("- (Ìï¥Îãπ ÏóÜÏùå)")

    # üìâ Í∏âÌïòÎùΩ (¬±10 Ïù¥ÏÉÅ 5Í∞ú, Î∂ÄÏ°±Î∂ÑÏùÄ OUTÎ°ú Î≥¥Í∞ï)
    lines.append("\n*üìâ Í∏âÌïòÎùΩ*")
    shown = 0
    if downs:
        for m in downs[:5]:
            drop = abs(int(m.get("change") or 0))
            lines.append(f"- {_link(m['name'], m.get('url'))} {m['prev_rank']}ÏúÑ ‚Üí {m['rank']}ÏúÑ (‚Üì{drop})")
            shown += 1
    if shown < 5 and rank_outs:
        for ro in rank_outs[: 5 - shown]:
            lines.append(f"- {ro['name']} {int(ro['rank'])}ÏúÑ ‚Üí OUT")
            shown += 1
    if shown == 0:
        lines.append("- (Ìï¥Îãπ ÏóÜÏùå)")

    # ‚Üî Ïù∏&ÏïÑÏõÉ Ïπ¥Ïö¥Ìä∏
    lines.append("\n*‚Üî Îû≠ÌÅ¨ Ïù∏&ÏïÑÏõÉ*")
    lines.append(f"{in_out_count}Í∞úÏùò Ï†úÌíàÏù¥ Ïù∏&ÏïÑÏõÉ ÎêòÏóàÏäµÎãàÎã§.")

    try:
        requests.post(SLACK_WEBHOOK, json={"text": "\n".join(lines)}, timeout=10).raise_for_status()
        print("[Slack] Ï†ÑÏÜ° ÏÑ±Í≥µ")
    except Exception as e:
        print("[Slack] Ï†ÑÏÜ° Ïã§Ìå®:", e)
