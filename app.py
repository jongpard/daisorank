# -*- coding: utf-8 -*-
"""
ë‹¤ì´ì†Œëª° ë·°í‹°/ìœ„ìƒ 'ì¼ê°„' ë­í‚¹ í¬ë¡¤ëŸ¬ (ì¤‘ë³µ/ë’¤ì£½ë°•ì£½ ë°©ì§€Â·ì¹´í…Œê³ ë¦¬/ì¼ê°„ ê°•ì œ)
- ì¹´í…Œê³ ë¦¬: ë·°í‹°/ìœ„ìƒ ê°•ì œ ì„ íƒ
- ì •ë ¬(ê¸°ê°„): ì¼ê°„ ê°•ì œ ì„ íƒ + í™œì„± ê²€ì¦ ë£¨í”„
- ë¡œë”©: ë¬´í•œ ìŠ¤í¬ë¡¤ + 'ë”ë³´ê¸°' ë³‘í–‰, ëª©í‘œ ê°œìˆ˜(ê¸°ë³¸ 200) ë„ë‹¬ ì‹œê¹Œì§€
- ì•ˆì •í™”: ì¹´ë“œ ìˆ˜ ì •ì²´ ê°ì§€ + ë§ˆì§€ë§‰ ì¹´ë“œ ë…¸ì¶œ ëŒ€ê¸°
- íŒŒì‹±: ìŠ¤ì¼ˆë ˆí†¤/ë°°ë„ˆ ì œì™¸(ì œëª©Â·ê°€ê²© ë‘˜ ë‹¤ ìˆì–´ì•¼ ìƒí’ˆìœ¼ë¡œ ê°„ì£¼), 'BEST' ì ‘ë‘ ì œê±°
- ì¤‘ë³µ ë°©ì§€: URL ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±° â†’ ìµœì¢…ì ìœ¼ë¡œ 1..N ìˆœìœ„ ì¬ë¶€ì—¬(ë’¤ì£½ë°•ì£½ ë°©ì§€)
- ì €ì¥: data/ë‹¤ì´ì†Œëª°_ë·°í‹°ìœ„ìƒ_ì¼ê°„_YYYY-MM-DD.csv (KST)
- Slack: ì˜¬ë¦¬ë¸Œì˜ í¬ë§· (TOP10 â†’ ê¸‰ìƒìŠ¹ â†’ ë‰´ë­ì»¤ â†’ ê¸‰í•˜ë½(5) â†’ ë­í¬ ì¸&ì•„ì›ƒ)
- Google Drive: refresh token(OAuth)ë¡œ ì—…ë¡œë“œ (IDëŠ” ë¡œê·¸ë§Œ, ë©”ì‹œì§€ ë¯¸ë…¸ì¶œ)
í™˜ê²½ë³€ìˆ˜:
  SLACK_WEBHOOK_URL, GOOGLE_CLIENT_ID, GOOGLE_CLIENT_SECRET, GOOGLE_REFRESH_TOKEN, GDRIVE_FOLDER_ID
  DAISO_TARGET_COUNT(ì„ íƒ, ê¸°ë³¸ 200)
"""
import os, re, csv, sys, time, traceback, pathlib, datetime as dt
from typing import List, Dict, Optional

import pytz
import requests
from bs4 import BeautifulSoup

# -------------------- ìƒìˆ˜/ê²½ë¡œ --------------------
BASE_URL = "https://www.daisomall.co.kr"
RANK_URL  = f"{BASE_URL}/ds/rank/C105"  # ë·°í‹°/ìœ„ìƒ
DATA_DIR  = pathlib.Path("data")
DEBUG_DIR = pathlib.Path("data/debug")
DATA_DIR.mkdir(parents=True, exist_ok=True)
DEBUG_DIR.mkdir(parents=True, exist_ok=True)
KST = pytz.timezone("Asia/Seoul")

TARGET_COUNT = int(os.getenv("DAISO_TARGET_COUNT", "200"))  # 100/200 ë“± ì¡°ì ˆ

SLACK_WEBHOOK_URL   = os.getenv("SLACK_WEBHOOK_URL", "").strip()
GOOGLE_CLIENT_ID    = os.getenv("GOOGLE_CLIENT_ID", "").strip()
GOOGLE_CLIENT_SECRET= os.getenv("GOOGLE_CLIENT_SECRET", "").strip()
GOOGLE_REFRESH_TOKEN= os.getenv("GOOGLE_REFRESH_TOKEN", "").strip()
GDRIVE_FOLDER_ID    = os.getenv("GDRIVE_FOLDER_ID", "").strip()

# -------------------- ìœ í‹¸ --------------------
def today_kst() -> str:
    return dt.datetime.now(KST).strftime("%Y-%m-%d")

def to_int(s: str) -> Optional[int]:
    try:
        return int(re.sub(r"[^\d]", "", s or ""))
    except Exception:
        return None

def fmt_won(n: Optional[int]) -> str:
    if not n:
        return "0ì›"
    return f"{n:,}ì›"

def slack(text: str):
    if not SLACK_WEBHOOK_URL:
        print("[Slack] ë¯¸ì„¤ì •")
        return
    try:
        requests.post(SLACK_WEBHOOK_URL, json={"text": text}, timeout=15)
        print("[Slack] ì „ì†¡ ì™„ë£Œ")
    except Exception as e:
        print("[Slack ì‹¤íŒ¨]", e)

def load_csv(path: Optional[pathlib.Path]) -> List[Dict]:
    if not path or not path.exists() or not path.is_file():
        return []
    out = []
    with path.open("r", newline="", encoding="utf-8") as f:
        r = csv.DictReader(f)
        for row in r:
            out.append({
                "date": row.get("date",""),
                "rank": int(row.get("rank","0") or 0),
                "name": row.get("name",""),
                "price": int(row.get("price","0") or 0),
                "url": row.get("url",""),
            })
    return out

def save_csv(path: pathlib.Path, rows: List[Dict]):
    cols = ["date","rank","name","price","url"]
    with path.open("w", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=cols)
        w.writeheader()
        for r in rows:
            w.writerow({k:r.get(k,"") for k in cols})

# -------------------- Google Drive --------------------
def gdrive_service():
    from google.oauth2.credentials import Credentials
    from googleapiclient.discovery import build
    if not (GOOGLE_CLIENT_ID and GOOGLE_CLIENT_SECRET and GOOGLE_REFRESH_TOKEN and GDRIVE_FOLDER_ID):
        raise RuntimeError("Drive ìê²©/í´ë” ID ë¯¸ì„¤ì •")
    creds = Credentials(
        None,
        refresh_token=GOOGLE_REFRESH_TOKEN,
        token_uri="https://oauth2.googleapis.com/token",
        client_id=GOOGLE_CLIENT_ID,
        client_secret=GOOGLE_CLIENT_SECRET,
    )
    return build("drive","v3",credentials=creds, cache_discovery=False)

def gdrive_upload(path: pathlib.Path) -> str:
    from googleapiclient.http import MediaInMemoryUpload
    svc = gdrive_service()
    media = MediaInMemoryUpload(path.read_bytes(), mimetype="text/csv", resumable=False)
    meta = {"name": path.name, "parents":[GDRIVE_FOLDER_ID]}
    f = svc.files().create(body=meta, media_body=media, fields="id", supportsAllDrives=True).execute()
    return f["id"]

# -------------------- íŒŒì‹±/í´ë¦° --------------------
def _clean_name(txt: str) -> str:
    """'BEST |', 'BESTï½œ', 'BEST ã…£', 'BESTÂ·', 'BEST-' ë“± ì ‘ë‘ ì œê±° + ê³µë°± ì •ë¦¬"""
    t = (txt or "").strip()
    sep = r"[|\uFF5C\u2502\u3139lI:\u00B7\.\-\u2014\u2013\u30FB]"  # | ï½œ â”‚ ã…£ l I : Â· . - â€” â€“ ãƒ»
    pat = re.compile(rf"^\s*BEST(?:\s+|{sep})\s*", re.I)
    while True:
        new = pat.sub("", t).strip()
        if new == t: break
        t = new
    return " ".join(t.split())

def _pick_price(unit) -> Optional[int]:
    """
    ê°€ê²© ì…€ë ‰í„° ìœ ì—° íŒŒì‹±:
      - .goods-detail .goods-price .value
      - .price .value
      - [class*=price] .value
      - [class*=price] ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ì ì¶”ì¶œ
    """
    for sel in [".goods-detail .goods-price .value", ".price .value", '[class*="price"] .value']:
        el = unit.select_one(sel)
        if el:
            v = to_int(el.get_text(strip=True))
            if v:
                return v
    block = unit.select_one('[class*="price"]')
    if block:
        m = re.search(r"(\d[\d,]*)", block.get_text(" ", strip=True))
        if m:
            v = to_int(m.group(1))
            if v:
                return v
    return None

def parse_html_filtered(html: str) -> List[Dict]:
    """
    - ì œëª©(.goods-detail .tit / .tit / .name / [class*=tit])ê³¼ ê°€ê²©ì´ ëª¨ë‘ ìˆëŠ” ë…¸ë“œë§Œ ìƒí’ˆ.
    - BEST ë¼ë²¨ ì œê±°.
    - URL ì ˆëŒ€ê²½ë¡œí™” í›„ dictë¡œ ì¤‘ë³µ ì œê±°(í‚¤: url).
    - ìµœì¢…ì ìœ¼ë¡œ rank=1..N ì¬ë¶€ì—¬(ë’¤ì£½ë°•ì£½ ë°©ì§€).
    """
    soup = BeautifulSoup(html, "lxml")
    units = soup.select(".goods-list .goods-unit")
    by_url: Dict[str, Dict] = {}
    for unit in units:
        tit_el = (unit.select_one(".goods-detail .tit")
                  or unit.select_one(".tit")
                  or unit.select_one(".name")
                  or unit.select_one('[class*="tit"]'))
        if not tit_el:
            continue

        # ì œëª© ë‚´ë¶€ BEST ë±ƒì§€ ì œê±°
        for b in tit_el.select(".best"):
            b.extract()
        name = _clean_name(tit_el.get_text(" ", strip=True))

        price = _pick_price(unit)
        if not name or not price or price <= 0:
            continue

        # ë§í¬
        url = RANK_URL
        a = (unit.select_one(".goods-thumb a.goods-link")
             or unit.select_one(".goods-detail a.goods-link")
             or unit.select_one("a"))
        if a and a.has_attr("href"):
            href = a["href"].strip()
            if href.startswith("//"):
                url = "https:" + href
            elif href.startswith("/"):
                url = BASE_URL + href
            else:
                url = href

        # ì¤‘ë³µ ì œê±° (ìµœì‹  DOM ìˆœì„œëŒ€ë¡œ ë§ˆì§€ë§‰ ê°’ìœ¼ë¡œ ìœ ì§€)
        by_url[url] = {
            "name": name,
            "price": int(price),
            "url": url,
        }

    # DOM ìˆœì„œë¥¼ ëª» ë¯¿ì„ ë•Œë¥¼ ëŒ€ë¹„í•´, ê°€ê²©/ì´ë¦„ì„ ë³´ì¡°í‚¤ë¡œ ì•ˆì • ì •ë ¬
    items = list(by_url.values())
    items.sort(key=lambda x: (x["name"].lower(), x["price"], x["url"]))
    # ìµœì¢… rank ì¬ë¶€ì—¬(1..N)
    out = []
    for i, it in enumerate(items, start=1):
        out.append({"rank": i, **it})
    return out

# -------------------- Playwright ìˆ˜ì§‘ --------------------
def fetch_playwright() -> List[Dict]:
    from playwright.sync_api import sync_playwright

    def _is_day_active(page) -> bool:
        grp = page.locator(".el-radio-group.ipt-sorting")
        try:
            act = grp.locator("label.is-active, label.active")
            if act.count() and "ì¼ê°„" in (act.first.inner_text() or ""):
                return True
        except Exception:
            pass
        try:
            day = grp.locator("label:has-text('ì¼ê°„')")
            if day.count():
                pressed = (day.first.get_attribute("aria-pressed") or "") == "true"
                checked = (day.first.locator("input").get_attribute("aria-checked") or "") == "true"
                if pressed or checked:
                    return True
        except Exception:
            pass
        return False

    def _click_beauty(page):
        """
        ì¹´í…Œê³ ë¦¬: ë·°í‹°/ìœ„ìƒ ê°•ì œ í´ë¦­
        - value="CTGR_00014" ìš°ì„ 
        - í…ìŠ¤íŠ¸ 'ë·°í‹°/ìœ„ìƒ' ë³´ì¡°
        - ìŠ¤ì™€ì´í¼ next ë²„íŠ¼ìœ¼ë¡œ ê°€ì‹œí™” ì‹œë„
        """
        try:
            target = page.locator('button.cate-btn[value="CTGR_00014"]')
            if not target.count():
                target = page.locator("button.cate-btn:has-text('ë·°í‹°/ìœ„ìƒ')")
            if target.count():
                if not target.first.is_visible():
                    nxt = page.locator(".pdcate-swiper .swiper-button-next")
                    for _ in range(10):
                        if target.first.is_visible(): break
                        if nxt.count(): nxt.first.click()
                        page.wait_for_timeout(150)
                target.first.click()
                page.wait_for_timeout(500)
        except Exception:
            pass

    def _click_daily(page):
        """
        ì •ë ¬: ì¼ê°„ ê°•ì œ í´ë¦­(ì£¼ê°„â†’ì¼ê°„ í† ê¸€ í¬í•¨), í™œì„± ê²€ì¦ ë£¨í”„
        """
        for _ in range(8):
            if _is_day_active(page):
                return
            # ì£¼ê°„ í´ë¦­ í›„ ì¼ê°„ í´ë¦­(í† ê¸€ ìœ ë„)
            try:
                wk = page.locator(".el-radio-group.ipt-sorting label:has-text('ì£¼ê°„')")
                if wk.count(): wk.first.click()
            except Exception: pass
            try:
                dy = page.locator(".el-radio-group.ipt-sorting label:has-text('ì¼ê°„')")
                if dy.count(): dy.first.click()
            except Exception: pass
            # value=2 ë¼ë””ì˜¤ ì§ì ‘ íŠ¸ë¦¬ê±°(ë³´ì¡°)
            try:
                page.evaluate("""
                    () => {
                        const el = document.querySelector('.ipt-sorting input.el-radio-button__orig-radio[value="2"]');
                        if (el) el.click();
                    }
                """)
            except Exception: pass
            page.wait_for_timeout(700)
        print("[warn] ì¼ê°„ íƒ­ ê³ ì • í™•ì¸ ì‹¤íŒ¨(í˜„ì¬ íƒ­ìœ¼ë¡œ ì§„í–‰)")

    def _force_load(page, target: int) -> int:
        """ë¬´í•œ ìŠ¤í¬ë¡¤ + 'ë”ë³´ê¸°' ë³‘í–‰ ë¡œë“œ, ì •ì²´ ê°ì§€ë¡œ ì¢…ë£Œ"""
        def count_cards():
            return page.locator(".goods-list .goods-unit").count()

        last = -1; stall = 0
        for _ in range(150):
            if count_cards() >= target: break
            # ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­
            try:
                more = page.locator("button:has-text('ë”ë³´ê¸°')")
                if more.count() and more.first.is_enabled():
                    more.first.click()
                    page.wait_for_timeout(900)
            except Exception:
                pass
            # ìŠ¤í¬ë¡¤ ë‹¤ìš´
            page.mouse.wheel(0, 5000)
            page.wait_for_timeout(700)
            cur = count_cards()
            if cur == last:
                stall += 1
                if stall >= 8: break
            else:
                stall = 0; last = cur
        return count_cards()

    def _wait_stable(page, rounds=3, gap_ms=700):
        """ì¹´ë“œ ìˆ˜ ë³€ë™ì´ roundsíšŒ ì—°ì† ì—†ì„ ë•Œê¹Œì§€ ëŒ€ê¸° + ë§ˆì§€ë§‰ ì¹´ë“œ ë…¸ì¶œ 1.2s"""
        def cnt(): return page.locator(".goods-list .goods-unit").count()
        same = 0; last = -1
        for _ in range(50):
            c = cnt()
            if c == last:
                same += 1
                if same >= rounds: break
            else:
                same = 0; last = c
            page.wait_for_timeout(gap_ms)
        try:
            if c > 0:
                last_card = page.locator(".goods-list .goods-unit").nth(c-1)
                last_card.scroll_into_view_if_needed()
                page.wait_for_timeout(1200)
        except Exception:
            pass

    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=True,
            args=["--no-sandbox","--disable-dev-shm-usage","--disable-gpu",
                  "--disable-blink-features=AutomationControlled"]
        )
        ctx = browser.new_context(
            locale="ko-KR",
            user_agent=("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                        "AppleWebKit/537.36 (KHTML, like Gecko) "
                        "Chrome/126.0 Safari/537.36"),
            viewport={"width":1440,"height":2200}
        )
        ctx.add_init_script("Object.defineProperty(navigator,'webdriver',{get:()=>undefined});")
        page = ctx.new_page()
        page.goto(RANK_URL, wait_until="domcontentloaded", timeout=60_000)

        # ë·°í‹°/ìœ„ìƒ + ì¼ê°„ ê°•ì œ
        _click_beauty(page)
        _click_daily(page)
        try:
            page.wait_for_selector(".goods-list .goods-unit", state="visible", timeout=20_000)
        except Exception:
            pass

        _ = _force_load(page, TARGET_COUNT)
        _wait_stable(page)

        # ë””ë²„ê·¸ ì €ì¥
        (DEBUG_DIR / "page_rank.png").write_bytes(page.screenshot(full_page=True))
        (DEBUG_DIR / "page_rank.html").write_text(page.content(), encoding="utf-8")

        html = page.content()
        ctx.close(); browser.close()

    # íŒŒì‹± + ì¤‘ë³µ ì œê±° + ìˆœìœ„ ì¬ë¶€ì—¬
    items = parse_html_filtered(html)
    if len(items) < 10:
        raise RuntimeError(f"ìœ íš¨ ì¹´ë“œ ë¶€ì¡±(Playwright íŒŒì‹± ìˆ˜={len(items)})")
    # ëª©í‘œ ê°œìˆ˜ë§Œ ì˜ë¼ì„œ ë°˜í™˜
    return items[:max(TARGET_COUNT, 1)]

# -------------------- Requests í´ë°± --------------------
def fetch_requests() -> List[Dict]:
    r = requests.get(RANK_URL, timeout=25, headers={
        "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126 Safari/537.36"
    })
    r.raise_for_status()
    html = r.text
    (DEBUG_DIR / "page_rank.html").write_text(html, encoding="utf-8")
    items = parse_html_filtered(html)
    return items

# -------------------- ì „ì¼ ë¹„êµ/ë³€ë™ --------------------
def normalize_name(s: str) -> str:
    return re.sub(r"\s+", " ", s or "").strip().lower()

def prev_csv_path(today_csv: pathlib.Path) -> Optional[pathlib.Path]:
    stem = "ë‹¤ì´ì†Œëª°_ë·°í‹°ìœ„ìƒ_ì¼ê°„_"
    files = sorted(DATA_DIR.glob(f"{stem}*.csv"))
    prevs = [p for p in files if p.name < today_csv.name]
    return prevs[-1] if prevs else None

def analyze(today: List[Dict], prev: List[Dict]) -> Dict[str, List[Dict]]:
    N = 30
    tmap = {normalize_name(x["name"]): x for x in today}
    pmap = {normalize_name(x["name"]): x for x in prev}
    rising, falling, new_in, out = [], [], [], []

    for k, t in tmap.items():
        if t["rank"]<=N:
            p = pmap.get(k)
            if p:
                d = p["rank"] - t["rank"]
                if d>0:
                    rising.append({"name":t["name"],"prev":p["rank"],"curr":t["rank"],"delta":d,"url":t["url"]})
            else:
                new_in.append({"name":t["name"],"prev":None,"curr":t["rank"],"url":t["url"]})

    for k, p in pmap.items():
        if p["rank"]<=N:
            t = tmap.get(k)
            if not t or t["rank"]>N:
                out.append({"name":p["name"],"prev":p["rank"],"curr":None})
            else:
                d = t["rank"] - p["rank"]
                if d>0:
                    falling.append({"name":t["name"],"prev":p["rank"],"curr":t["rank"],"delta":d,"url":t["url"]})

    rising.sort(key=lambda x:(-x["delta"], x["curr"], x["prev"], normalize_name(x["name"])))
    falling.sort(key=lambda x:(-x["delta"], x["prev"], x["curr"], normalize_name(x["name"])))
    new_in.sort(key=lambda x:(x["curr"], normalize_name(x["name"])))
    out.sort(key=lambda x:(x["prev"], normalize_name(x["name"])))

    return {
        "rising": rising[:3],
        "new_in": new_in[:3],
        "falling": falling[:5],
        "out": out,
        "inout_count": len(new_in)+len(out),
    }

# -------------------- Slack ë©”ì‹œì§€ --------------------
def slack_message(today_rows: List[Dict], change: Dict) -> str:
    L = []
    L.append(f"*ë‹¤ì´ì†Œëª° ë·°í‹°/ìœ„ìƒ ì¼ê°„ ë­í‚¹ â€” {today_kst()}*")
    L.append("")
    L.append("*TOP 10*")
    for r in [x for x in today_rows if x["rank"]<=10]:
        name = r["name"].replace("&","&amp;").replace("<","ã€ˆ").replace(">","ã€‰")
        L.append(f"{r['rank']}. <{r['url']}|{name}> â€” {fmt_won(r['price'])}")
    L.append("")

    L.append("ğŸ”¥ *ê¸‰ìƒìŠ¹*")
    if change["rising"]:
        for r in change["rising"]:
            L.append(f"- {r['name']} {r['prev']}ìœ„ â†’ {r['curr']}ìœ„ (â†‘{r['delta']})")
    else:
        L.append("- í•´ë‹¹ ì—†ìŒ")
    L.append("")

    L.append("ğŸ†• *ë‰´ë­ì»¤*")
    if change["new_in"]:
        for r in change["new_in"]:
            L.append(f"- {r['name']} NEW â†’ {r['curr']}ìœ„")
    else:
        L.append("- í•´ë‹¹ ì—†ìŒ")
    L.append("")

    L.append("ğŸ“‰ *ê¸‰í•˜ë½*")
    had = False
    for r in change["falling"]:
        L.append(f"- {r['name']} {r['prev']}ìœ„ â†’ {r['curr']}ìœ„ (â†“{r['delta']})"); had = True
    outs = [o for o in change["out"] if o["prev"]<=30]
    for o in outs:
        L.append(f"- {o['name']} {o['prev']}ìœ„ â†’ OUT"); had = True
    if not had:
        L.append("- í•´ë‹¹ ì—†ìŒ")
    L.append("")

    L.append("ğŸ” *ë­í¬ ì¸&ì•„ì›ƒ*")
    L.append(f"{change['inout_count']}ê°œì˜ ì œí’ˆì´ ì¸&ì•„ì›ƒ ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return "\n".join(L)

# -------------------- ë©”ì¸ --------------------
def main():
    t0 = time.time()
    print("ìˆ˜ì§‘ ì‹œì‘:", RANK_URL)

    try:
        items = fetch_playwright()
        print("[Playwright] ì •ìƒ ìˆ˜ì§‘")
    except Exception as e:
        print("[Playwright ì‹¤íŒ¨ â†’ Requests í´ë°±]", e)
        items = fetch_requests()

    cnt = len(items)
    print("ìˆ˜ì§‘ ì™„ë£Œ:", cnt)
    if cnt < 20:
        raise RuntimeError("ìœ íš¨ ìƒí’ˆ ì¹´ë“œê°€ ë„ˆë¬´ ì ê²Œ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì…€ë ‰í„°/ë Œë”ë§ ì ê²€ í•„ìš”")

    csv_path = DATA_DIR / f"ë‹¤ì´ì†Œëª°_ë·°í‹°ìœ„ìƒ_ì¼ê°„_{today_kst()}.csv"
    rows = [{"date": today_kst(), "rank":i["rank"], "name":i["name"], "price":i["price"], "url":i["url"]} for i in items]
    save_csv(csv_path, rows)

    prev_path = prev_csv_path(csv_path)
    prev_rows = load_csv(prev_path)
    change = analyze(rows, prev_rows)

    try:
        _ = gdrive_upload(csv_path)
        print("Drive ì—…ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print("[Drive ì—…ë¡œë“œ ì‹¤íŒ¨]", e)

    msg = slack_message(rows, change)
    slack(msg)

    print(f"ì´ {cnt}ê±´, ê²½ê³¼: {time.time()-t0:.1f}s")

if __name__ == "__main__":
    try:
        main()
    except Exception:
        print(traceback.format_exc())
        sys.exit(1)
