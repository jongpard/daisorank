# -*- coding: utf-8 -*-
"""
DaisoMall C105 ë­í‚¹ í¬ë¡¤ëŸ¬
- 1ì°¨: HTTP ì •ì  ìˆ˜ì§‘ (ë¶€ì¡±/ì‹¤íŒ¨ ì‹œ Playwright í´ë°±)
- í´ë°±: ìŠ¤í¬ë¡¤/íƒ­í´ë¦­/ë‹¤ì–‘í•œ ì…€ë ‰í„° ì‹œë„ + XHR JSON ë³µì› + ë””ë²„ê·¸ ë¤í”„
- CSV: ë‹¤ì´ì†Œëª°_ë­í‚¹_YYYY-MM-DD.csv (KST)
- ë¹„êµ í‚¤: product_code ìš°ì„ , ì—†ìœ¼ë©´ url
- Slack: êµ­ë‚´ í¬ë§·(Top10 â†’ ê¸‰ìƒìŠ¹ â†’ ë‰´ë­ì»¤ â†’ ê¸‰í•˜ë½(5) â†’ ì¸&ì•„ì›ƒ)
- Google Drive: OAuth(ê°œì¸ ê³„ì • refresh token) ì—…ë¡œë“œ + ì „ì¼ CSV ë‹¤ìš´ë¡œë“œ
"""

import os
import re
import io
import math
import pytz
import json
import traceback
import datetime as dt
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass

import requests
import pandas as pd
from bs4 import BeautifulSoup

# ================== ê¸°ë³¸ ì„¤ì • ==================
KST = pytz.timezone("Asia/Seoul")
RANK_URL = "https://www.daisomall.co.kr/ds/rank/C105"
MAX_RANK = int(os.getenv("DAISO_MAX_RANK", "200"))

HEADERS = {
    "User-Agent": ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                   "AppleWebKit/537.36 (KHTML, like Gecko) "
                   "Chrome/124.0.0.0 Safari/537.36"),
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
    "Cache-Control": "no-cache",
    "Pragma": "no-cache",
}

# ================== ìœ í‹¸/ê³µí†µ ==================
def now_kst() -> dt.datetime:
    return dt.datetime.now(KST)

def today_kst_str() -> str:
    return now_kst().strftime("%Y-%m-%d")

def yesterday_kst_str() -> str:
    return (now_kst() - dt.timedelta(days=1)).strftime("%Y-%m-%d")

def build_filename(date_str: str) -> str:
    return f"ë‹¤ì´ì†Œëª°_ë­í‚¹_{date_str}.csv"

def clean(s: Optional[str]) -> str:
    return re.sub(r"\s+", " ", (s or "")).strip()

def slack_escape(s: str) -> str:
    return s.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")

def debug_dump_page(page, prefix="page"):
    """Playwright ë””ë²„ê·¸: ì „ì²´ ìŠ¤í¬ë¦°ìƒ· + HTML ì €ì¥"""
    try:
        os.makedirs("data/debug", exist_ok=True)
        png = f"data/debug/{prefix}.png"
        html = f"data/debug/{prefix}.html"
        page.screenshot(path=png, full_page=True)
        content = page.content()
        with open(html, "w", encoding="utf-8") as f:
            f.write(content)
        print(f"[debug] saved {png}, {html}")
    except Exception as e:
        print("[debug] dump failed:", e)

# ê¸ˆì•¡/í¼ì„¼íŠ¸/ìƒí’ˆì½”ë“œ
KRW_RE = re.compile(r"(?:â‚©|)\s*([\d,]+)\s*ì›")
PCT_RE = re.compile(r"(\d+)\s*%")
PC_PATTERNS = [
    re.compile(r"[?&](?:goodsNo|itemNo|prodNo|productNo|goods_id|no)=(\d+)", re.I),
    re.compile(r"/(?:product|goods)/(\d+)(?:[/?#]|$)", re.I),
    re.compile(r"/p/(\d+)(?:[/?#]|$)", re.I),
]

def extract_product_code(url: str, block_text: str = "") -> str:
    if not url:
        return ""
    for pat in PC_PATTERNS:
        m = pat.search(url)
        if m:
            return m.group(1)
    m2 = re.search(r"ìƒí’ˆë²ˆí˜¸\s*[:ï¼š]\s*(\d+)", block_text)
    return m2.group(1) if m2 else ""

def parse_prices(block_text: str) -> Tuple[Optional[int], Optional[int], Optional[int]]:
    """ìµœì†Ÿê°’=íŒë§¤ê°€, ìµœëŒ“ê°’=ì •ê°€, í¼ì„¼íŠ¸ëŠ” í…ìŠ¤íŠ¸ ìš°ì„ , ì—†ìœ¼ë©´ ê³„ì‚°(ë²„ë¦¼)"""
    amounts = [int(x.replace(",", "")) for x in KRW_RE.findall(block_text or "")]
    sale = orig = pct = None
    if amounts:
        sale = min(amounts)
        if len(amounts) >= 2:
            orig = max(amounts)
            if orig == sale:
                orig = None
    m = PCT_RE.search(block_text or "")
    if m:
        pct = int(m.group(1))
    elif orig and sale and orig > 0:
        pct = max(0, int(math.floor((1 - sale / orig) * 100)))
    return sale, orig, pct

# ================== ëª¨ë¸ ==================
@dataclass
class Product:
    rank: Optional[int]
    brand: str
    name: str
    price: Optional[int]
    orig_price: Optional[int]
    discount_percent: Optional[int]
    url: str
    product_code: str

# ================== íŒŒì„œ(HTTP) ==================
def parse_http(html: str) -> List[Product]:
    soup = BeautifulSoup(html, "lxml")
    items: List[Product] = []
    seen = set()

    anchors = soup.select(
        "a[href*='/product/'], a[href*='/goods/'], a[href*='goodsNo='], "
        "ul.goods_list a[href], div.goods_list a[href]"
    )
    for a in anchors:
        href = a.get("href") or ""
        if not href:
            continue
        if href.startswith("//"):
            href = "https:" + href
        elif href.startswith("/"):
            href = "https://www.daisomall.co.kr" + href

        card = a.find_parent("li") or a.find_parent("div")
        if not card:
            continue

        name = clean(a.get_text(" ", strip=True))
        brand = ""

        brand_el = None
        for sel in [".brand", ".brand-name", ".prd-brand", ".ds-brand", ".goods-brand", ".txt_brand", ".brand_name"]:
            brand_el = card.select_one(sel)
            if brand_el:
                break
        if brand_el:
            brand = clean(brand_el.get_text(" ", strip=True))
        else:
            for sub_a in card.select("a"):
                h = (sub_a.get("href") or "").lower()
                if ("/product/" in h) or ("/goods/" in h) or ("goodsno=" in h):
                    continue
                t = clean(sub_a.get_text(" ", strip=True))
                if 1 <= len(t) <= 40:
                    brand = t
                    break

        block_text = clean(card.get_text(" ", strip=True))
        code = extract_product_code(href, block_text)
        key = code or href
        if key in seen:
            continue
        seen.add(key)

        sale, orig, pct = parse_prices(block_text)
        items.append(Product(
            rank=len(items) + 1,
            brand=brand,
            name=name,
            price=sale,
            orig_price=orig,
            discount_percent=pct,
            url=href,
            product_code=code
        ))
        if len(items) >= MAX_RANK:
            break

    return items

# ================== Playwright í´ë°±(ê°•í™”) ==================
def fetch_by_playwright() -> List[Product]:
    """
    - C105 íƒ­ í´ë¦­ ì‹œë„
    - ì¶©ë¶„íˆ ìŠ¤í¬ë¡¤Â·ëŒ€ê¸°
    - ë‹¤ì–‘í•œ ì¹´ë“œ ì…€ë ‰í„° ì‹œë„
    - ê·¸ë˜ë„ ë¶€ì¡±í•˜ë©´ XHR JSON ì‘ë‹µì—ì„œ ë³µì›
    - ë””ë²„ê·¸ ë¤í”„ ì €ì¥
    """
    from playwright.sync_api import sync_playwright

    products: List[Product] = []
    seen = set()

    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=True,
            args=[
                "--no-sandbox",
                "--disable-dev-shm-usage",
                "--disable-blink-features=AutomationControlled",
            ],
        )
        context = browser.new_context(
            viewport={"width": 1440, "height": 1100},
            user_agent=HEADERS["User-Agent"],
            locale="ko-KR",
            timezone_id="Asia/Seoul",
            extra_http_headers={"Accept-Language": HEADERS["Accept-Language"]},
        )
        context.add_init_script(
            "Object.defineProperty(navigator, 'webdriver', {get: () => undefined});"
        )
        page = context.new_page()

        # XHR JSON ìˆ˜ì§‘
        xhr_rows: List[dict] = []
        def on_response(res):
            try:
                url = res.url
                ctype = (res.headers or {}).get("content-type", "")
                if ("rank" in url.lower() or "best" in url.lower()) and "json" in ctype.lower():
                    data = res.json()
                    if isinstance(data, (dict, list)):
                        xhr_rows.append({"url": url, "data": data})
            except Exception:
                pass
        page.on("response", on_response)

        page.goto(RANK_URL, wait_until="domcontentloaded", timeout=60_000)

        # íƒ­ í´ë¦­ ì‹œë„
        try:
            page.locator('a[href="/ds/rank/C105"]').first.click(timeout=3_000)
        except Exception:
            pass

        # ë„¤íŠ¸ì›Œí¬ ì•ˆì •í™”
        try:
            page.wait_for_load_state("networkidle", timeout=5_000)
        except Exception:
            pass

        # ì¶©ë¶„íˆ ìŠ¤í¬ë¡¤ (ì¦ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ)
        last = 0
        idle = 0
        for _ in range(20):
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            try:
                page.wait_for_load_state("networkidle", timeout=1_500)
            except Exception:
                pass
            cnt = page.eval_on_selector_all(
                "a[href*='/product/'], a[href*='/goods/'], a[href*='goodsNo=']",
                "els => els.length"
            )
            if cnt == last:
                idle += 1
            else:
                idle = 0
            last = cnt
            if cnt >= MAX_RANK or idle >= 3:
                break

        debug_dump_page(page, "page_rank")

        # 1ì°¨: ë‹¤ì–‘í•œ ì¹´ë“œ ì…€ë ‰í„°ì—ì„œ ìˆ˜ì§‘
        rows = page.evaluate("""
            () => {
              const SELS = [
                'ul li[data-goods-no]',
                'ul.goods_list li',
                'div.goods_list li',
                'li.goods-item',
                'li.rank_list_item',
                'li.prd_item',
                'div.prd_list li',
                'div.item, li.item'
              ];
              let cards = [];
              for (const s of SELS) {
                cards = Array.from(document.querySelectorAll(s));
                if (cards.length >= 10) break;
              }
              const res = [];
              const seen = new Set();
              for (const card of cards) {
                let a = card.querySelector("a[href*='/product/'], a[href*='/goods/'], a[href*='goodsNo=']");
                if (!a) continue;
                let href = a.getAttribute('href') || '';
                if (!href) continue;
                if (href.startsWith('//')) href = 'https:' + href;
                if (href.startsWith('/')) href = location.origin + href;

                let name = (a.textContent || '').replace(/\\s+/g,' ').trim();
                let brand = '';
                const brandSels = ['.brand', '.brand-name', '.prd-brand', '.ds-brand', '.goods-brand', '.txt_brand', '.brand_name'];
                for (const s of brandSels) {
                  const el = card.querySelector(s);
                  if (el) { brand = (el.textContent||'').replace(/\\s+/g,' ').trim(); break; }
                }
                if (!brand) {
                  const subAs = Array.from(card.querySelectorAll('a'));
                  for (const b of subAs) {
                    const h = (b.getAttribute('href') || '').toLowerCase();
                    if (h.includes('/product/') || h.includes('/goods/') || h.includes('goodsno=')) continue;
                    const t = (b.textContent || '').replace(/\\s+/g,' ').trim();
                    if (t.length >= 1 && t.length <= 40) { brand = t; break; }
                  }
                }
                const block = (card.innerText || '').replace(/\\s+/g,' ').trim();

                const key = href + '|' + name;
                if (seen.has(key)) continue;
                seen.add(key);
                res.push({href, name, brand, block});
              }
              return res;
            }
        """)

        # 1ì°¨ ì¡°ë¦½
        for r in rows:
            href = r.get("href") or ""
            name = clean(r.get("name"))
            brand = clean(r.get("brand"))
            block = clean(r.get("block"))
            code = extract_product_code(href, block)
            key = code or href
            if key in seen:
                continue
            seen.add(key)
            sale, orig, pct = parse_prices(block)
            products.append(Product(
                rank=len(products)+1, brand=brand, name=name,
                price=sale, orig_price=orig, discount_percent=pct,
                url=href, product_code=code
            ))
            if len(products) >= MAX_RANK:
                break

        # 2ì°¨: ì¹´ë“œë¡œ ë¶€ì¡±í•˜ë©´ XHR(JSON) ë³µì›
        if len(products) < 10 and xhr_rows:
            for pack in xhr_rows:
                data = pack.get("data")
                stack = [data]
                while stack:
                    cur = stack.pop()
                    if isinstance(cur, dict):
                        if {"goodsNo","goodsNm"} <= set(cur.keys()):
                            try:
                                href = cur.get("url") or f"https://www.daisomall.co.kr/goods/{cur.get('goodsNo')}"
                                name = clean(cur.get("goodsNm") or "")
                                brand = clean(cur.get("brandNm") or "")
                                block = f"{name} {brand} {cur}"
                                code = str(cur.get("goodsNo") or "") or extract_product_code(href, block)
                                key = code or href
                                if key in seen:
                                    continue
                                sale = None
                                if cur.get("sellPrice") is not None:
                                    try: sale = int(str(cur.get("sellPrice")).replace(",",""))
                                    except: pass
                                orig = None
                                if cur.get("originPrice") is not None:
                                    try: orig = int(str(cur.get("originPrice")).replace(",",""))
                                    except: pass
                                pct = None
                                if orig and sale:
                                    pct = max(0, int(math.floor((1 - sale/orig) * 100)))
                                products.append(Product(
                                    rank=len(products)+1, brand=brand, name=name,
                                    price=sale, orig_price=orig, discount_percent=pct,
                                    url=href, product_code=code
                                ))
                                if len(products) >= MAX_RANK:
                                    break
                            except Exception:
                                pass
                        else:
                            for v in cur.values():
                                stack.append(v)
                    elif isinstance(cur, list):
                        for v in cur:
                            stack.append(v)
                if len(products) >= MAX_RANK:
                    break

        context.close()
        browser.close()

    return products

# ================== ìˆ˜ì§‘ ==================
def fetch_products() -> List[Product]:
    print("ìˆ˜ì§‘ ì‹œì‘:", RANK_URL)
    # HTTP
    try:
        r = requests.get(RANK_URL, headers=HEADERS, timeout=20)
        r.raise_for_status()
        items = parse_http(r.text)
        print("[HTTP] ìˆ˜ì§‘:", len(items))
        if len(items) >= 30:
            return items[:MAX_RANK]
    except Exception as e:
        print("[HTTP ì˜¤ë¥˜]", e)

    # Playwright í´ë°±
    print("[Playwright í´ë°± ì§„ì…]")
    items = fetch_by_playwright()
    print("[Playwright] ìˆ˜ì§‘:", len(items))
    return items[:MAX_RANK]

# ================== ë°ì´í„°í”„ë ˆì„/ì €ì¥ ==================
def to_df(products: List[Product], date_str: str) -> pd.DataFrame:
    rows = []
    for p in products:
        rows.append({
            "date": date_str,
            "rank": p.rank,
            "brand": p.brand,
            "product_name": p.name,
            "price": p.price,
            "orig_price": p.orig_price,
            "discount_percent": p.discount_percent,
            "url": p.url,
            "product_code": p.product_code,
        })
    return pd.DataFrame(rows)

# ================== Google Drive ==================
def normalize_folder_id(raw: str) -> str:
    if not raw:
        return ""
    s = raw.strip()
    m = re.search(r"/folders/([a-zA-Z0-9_-]{8,})", s) or re.search(r"[?&]id=([a-zA-Z0-9_-]{8,})", s)
    return (m.group(1) if m else s)

def build_drive_service():
    from googleapiclient.discovery import build
    from google.oauth2.credentials import Credentials

    cid = os.getenv("GOOGLE_CLIENT_ID")
    csc = os.getenv("GOOGLE_CLIENT_SECRET")
    rtk = os.getenv("GOOGLE_REFRESH_TOKEN")
    if not (cid and csc and rtk):
        raise RuntimeError("Google Drive í´ë” ì ‘ê·¼ ë¶ˆê°€: Client/Secret/Refresh í† í° í™•ì¸ í•„ìš”")

    creds = Credentials(
        None,
        refresh_token=rtk,
        token_uri="https://oauth2.googleapis.com/token",
        client_id=cid,
        client_secret=csc,
        scopes=["https://www.googleapis.com/auth/drive"]
    )
    svc = build("drive", "v3", credentials=creds, cache_discovery=False)
    try:
        about = svc.about().get(fields="user(displayName,emailAddress)").execute()
        u = about.get("user", {})
        print(f"[Drive] user={u.get('displayName')} <{u.get('emailAddress')}>")
    except Exception as e:
        print("[Drive] whoami ì‹¤íŒ¨:", e)
    return svc

def drive_upload_csv(service, folder_id: str, filename: str, df: pd.DataFrame) -> str:
    from googleapiclient.http import MediaIoBaseUpload

    q = f"name = '{filename}' and '{folder_id}' in parents and trashed = false"
    res = service.files().list(
        q=q,
        fields="files(id,name)",
        supportsAllDrives=True,
        includeItemsFromAllDrives=True
    ).execute()
    file_id = res.get("files", [{}])[0].get("id") if res.get("files") else None

    buf = io.BytesIO()
    df.to_csv(buf, index=False, encoding="utf-8-sig")
    buf.seek(0)
    media = MediaIoBaseUpload(buf, mimetype="text/csv", resumable=False)

    if file_id:
        service.files().update(
            fileId=file_id,
            media_body=media,
            supportsAllDrives=True
        ).execute()
        return file_id

    meta = {"name": filename, "parents": [folder_id], "mimeType": "text/csv"}
    created = service.files().create(
        body=meta,
        media_body=media,
        fields="id",
        supportsAllDrives=True
    ).execute()
    return created["id"]

def drive_download_csv(service, folder_id: str, filename: str) -> Optional[pd.DataFrame]:
    from googleapiclient.http import MediaIoBaseDownload

    res = service.files().list(
        q=f"name = '{filename}' and '{folder_id}' in parents and trashed = false",
        fields="files(id,name)",
        supportsAllDrives=True,
        includeItemsFromAllDrives=True
    ).execute()
    files = res.get("files", [])
    if not files:
        return None

    fid = files[0]["id"]
    req = service.files().get_media(fileId=fid, supportsAllDrives=True)
    fh = io.BytesIO()
    downloader = MediaIoBaseDownload(fh, req)
    done = False
    while not done:
        _, done = downloader.next_chunk()
    fh.seek(0)
    return pd.read_csv(fh)

# ================== Slack ==================
def fmt_krw(v) -> str:
    try:
        return f"â‚©{int(round(float(v))):,}"
    except:
        return "â‚©0"

def slack_post(text: str):
    url = os.getenv("SLACK_WEBHOOK_URL")
    if not url:
        print("[ê²½ê³ ] SLACK_WEBHOOK_URL ë¯¸ì„¤ì • â†’ ì½˜ì†” ì¶œë ¥\n", text)
        return
    try:
        r = requests.post(url, json={"text": text}, timeout=20)
        if r.status_code >= 300:
            print("[Slack ì‹¤íŒ¨]", r.status_code, r.text)
    except Exception as e:
        print("[Slack ì˜¤ë¥˜]", e)

# ================== ë¹„êµ/ì„¹ì…˜ êµ¬ì„± ==================
def build_sections(df_today: pd.DataFrame, df_prev: Optional[pd.DataFrame]) -> Dict[str, List[str]]:
    S = {"top10": [], "rising": [], "newcomers": [], "falling": [], "outs": [], "inout_count": 0}

    def display_name(row) -> str:
        br = clean(row.get("brand", ""))
        nm = clean(row.get("product_name", ""))
        return f"{br} {nm}" if br else nm

    # Top10
    top10 = df_today.dropna(subset=["rank"]).sort_values("rank").head(10)
    lines = []
    for _, r in top10.iterrows():
        disp = slack_escape(display_name(r))
        link = f"<{r['url']}|{disp}>"
        tail = f" (â†“{int(r['discount_percent'])}%)" if pd.notnull(r.get("discount_percent")) else ""
        lines.append(f"{int(r['rank'])}. {link} â€” {fmt_krw(r['price'])}{tail}")
    S["top10"] = lines

    if df_prev is None or not len(df_prev):
        return S

    # Key ì •ì˜
    def keyify(df):
        df = df.copy()
        df["key"] = df.apply(
            lambda x: x["product_code"] if (pd.notnull(x.get("product_code")) and str(x.get("product_code")).strip())
            else x["url"], axis=1)
        df.set_index("key", inplace=True)
        return df

    df_t = keyify(df_today)
    df_p = keyify(df_prev)

    t30 = df_t[(df_t["rank"].notna()) & (df_t["rank"] <= 30)].copy()
    p30 = df_p[(df_p["rank"].notna()) & (df_p["rank"] <= 30)].copy()

    common = set(t30.index) & set(p30.index)
    new = set(t30.index) - set(p30.index)
    out = set(p30.index) - set(t30.index)

    def link_of(row):
        return f"<{row['url']}|{slack_escape(display_name(row))}>"

    # ê¸‰ìƒìŠ¹
    rising = []
    for k in common:
        pr = int(p30.loc[k, "rank"])
        cr = int(t30.loc[k, "rank"])
        imp = pr - cr
        if imp > 0:
            rising.append((imp, cr, pr, link_of(t30.loc[k])))
    rising.sort(key=lambda x: (-x[0], x[1], x[2]))
    S["rising"] = [f"- {lnk} {pr}ìœ„ â†’ {cr}ìœ„ (â†‘{imp})" for imp, cr, pr, lnk in rising[:3]] or ["- í•´ë‹¹ ì—†ìŒ"]

    # ë‰´ë­ì»¤
    newcomers = [(int(t30.loc[k, "rank"]), link_of(t30.loc[k])) for k in new]
    newcomers.sort(key=lambda x: x[0])
    S["newcomers"] = [f"- {lnk} NEW â†’ {rk}ìœ„" for rk, lnk in newcomers[:3]] or ["- í•´ë‹¹ ì—†ìŒ"]

    # ê¸‰í•˜ë½
    falling = []
    for k in common:
        pr = int(p30.loc[k, "rank"])
        cr = int(t30.loc[k, "rank"])
        drop = cr - pr
        if drop > 0:
            falling.append((drop, cr, pr, link_of(t30.loc[k])))
    falling.sort(key=lambda x: (-x[0], x[1], x[2]))
    S["falling"] = [f"- {lnk} {pr}ìœ„ â†’ {cr}ìœ„ (â†“{drop})" for drop, cr, pr, lnk in falling[:5]] or ["- í•´ë‹¹ ì—†ìŒ"]

    # OUT
    outs = [(int(p30.loc[k, "rank"]), link_of(p30.loc[k])) for k in out]
    outs.sort(key=lambda x: x[0])
    S["outs"] = [f"- {lnk} {rk}ìœ„ â†’ OUT" for rk, lnk in outs]

    S["inout_count"] = len(new) + len(out)
    return S

def build_slack_message(date_str: str, S: Dict[str, List[str]]) -> str:
    parts: List[str] = []
    parts.append(f"*ë‹¤ì´ì†Œëª° ë­í‚¹ â€” {date_str}*")
    parts.append("")
    parts.append("*TOP 10*")
    parts.extend(S.get("top10") or ["- ë°ì´í„° ì—†ìŒ"])
    parts.append("")
    parts.append("*ğŸ”¥ ê¸‰ìƒìŠ¹*")
    parts.extend(S.get("rising") or ["- í•´ë‹¹ ì—†ìŒ"])
    parts.append("")
    parts.append("*ğŸ†• ë‰´ë­ì»¤*")
    parts.extend(S.get("newcomers") or ["- í•´ë‹¹ ì—†ìŒ"])
    parts.append("")
    parts.append("*ğŸ“‰ ê¸‰í•˜ë½*")
    parts.extend(S.get("falling") or ["- í•´ë‹¹ ì—†ìŒ"])
    if S.get("outs"):
        parts.extend(S["outs"])
    parts.append("")
    parts.append("*ğŸ”„ ë­í¬ ì¸&ì•„ì›ƒ*")
    parts.append(f"{S.get('inout_count', 0)}ê°œì˜ ì œí’ˆì´ ì¸&ì•„ì›ƒ ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return "\n".join(parts)

# ================== ë©”ì¸ ==================
def main():
    date_str = today_kst_str()
    yday_str = yesterday_kst_str()
    file_today = build_filename(date_str)
    file_prev = build_filename(yday_str)

    products = fetch_products()
    print("ìˆ˜ì§‘ ì™„ë£Œ:", len(products))
    if len(products) < 10:
        raise RuntimeError("ì œí’ˆ ì¹´ë“œê°€ ë„ˆë¬´ ì ê²Œ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì…€ë ‰í„°/ë Œë”ë§ ì ê²€ í•„ìš”")

    df_today = to_df(products, date_str)

    os.makedirs("data", exist_ok=True)
    local_path = os.path.join("data", file_today)
    df_today.to_csv(local_path, index=False, encoding="utf-8-sig")
    print("ë¡œì»¬ ì €ì¥:", local_path)

    # Google Drive ì—…ë¡œë“œ + ì „ì¼ CSV ë°›ê¸°
    df_prev = None
    try:
        folder = normalize_folder_id(os.getenv("GDRIVE_FOLDER_ID", ""))
        if not folder:
            raise RuntimeError("GDRIVE_FOLDER_ID ë¯¸ì„¤ì •")
        svc = build_drive_service()
        drive_upload_csv(svc, folder, file_today, df_today)
        print("Google Drive ì—…ë¡œë“œ ì™„ë£Œ:", file_today)
        df_prev = drive_download_csv(svc, folder, file_prev)
        print("ì „ì¼ CSV", "ë¯¸ë°œê²¬" if df_prev is None else "ë‹¤ìš´ë¡œë“œ ì„±ê³µ")
    except Exception as e:
        print("Google Drive ì²˜ë¦¬ ì˜¤ë¥˜:", e)

    S = build_sections(df_today, df_prev)
    msg = build_slack_message(date_str, S)
    slack_post(msg)
    print("Slack ì „ì†¡ ì™„ë£Œ")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print("[ì˜¤ë¥˜ ë°œìƒ]", e)
        traceback.print_exc()
        try:
            slack_post(f"*ë‹¤ì´ì†Œëª° ë­í‚¹ ìë™í™” ì‹¤íŒ¨*\n```\n{e}\n```")
        except:
            pass
        raise
